{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Resale Value of Knives from a Texas Government Surplus Store\n",
    "\n",
    "## Using Machine Learning to Support an Ebay Store's Financial Success\n",
    "\n",
    "\n",
    "### Data Exploration and Modeling\n",
    "\n",
    "\n",
    "**Author:** Dylan Dey\n",
    "\n",
    "\n",
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd \n",
    "import  json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_iqr_filter(df):\n",
    "    \n",
    "    price_Q1 = df['converted_price'].quantile(0.25)\n",
    "    price_Q3 = df['converted_price'].quantile(0.75)\n",
    "    price_iqr = price_Q3 - price_Q1\n",
    "\n",
    "    profit_Q1 = df['profit'].quantile(0.25)\n",
    "    profit_Q3 = df['profit'].quantile(0.75)\n",
    "    profit_iqr = profit_Q3 - profit_Q1\n",
    "\n",
    "    ROI_Q1 = df['ROI'].quantile(0.25)\n",
    "    ROI_Q3 = df['ROI'].quantile(0.75)\n",
    "    ROI_iqr = ROI_Q3 - ROI_Q1\n",
    "\n",
    "    price_upper_limit = price_Q3 + (1.5 * price_iqr)\n",
    "    price_lower_limit = price_Q1 - (1.5 * price_iqr)\n",
    "\n",
    "    profit_upper_limit = profit_Q3 + (1.5 * profit_iqr)\n",
    "    profit_lower_limit = profit_Q1 - (1.5 * profit_iqr)\n",
    "\n",
    "    ROI_upper_limit = ROI_Q3 + (1.5 * ROI_iqr)\n",
    "    ROI_lower_limit = ROI_Q1 - (1.5 * ROI_iqr)\n",
    "    \n",
    "#     print(f'Brand: {df.brand[0]}')\n",
    "#     print(f'price upper limit: ${np.round(price_upper_limit,2)}')\n",
    "#     print(f'price lower limit: ${np.round(price_lower_limit,2)}')\n",
    "#     print('-----------------------------------')\n",
    "#     print(f'profit upper limit: ${np.round(profit_upper_limit,2)}')\n",
    "#     print(f'profit lower limit: ${np.round(profit_lower_limit,2)}')\n",
    "#     print('-----------------------------------')\n",
    "#     print(f'ROI upper limit: {np.round(ROI_upper_limit,2)}%')\n",
    "#     print(f'ROI lower limit: {np.round(ROI_lower_limit,2)}%')\n",
    "#     print('-----------------------------------')\n",
    "\n",
    "    \n",
    "    new_df = df[(df['converted_price'] <= price_upper_limit) &\n",
    "                (df['converted_price'] >= price_lower_limit) &\n",
    "                (df['profit'] <= profit_upper_limit) &\n",
    "                (df['ROI'] <= ROI_upper_limit) &\n",
    "                (df['profit'] <= profit_upper_limit) &\n",
    "                (df['ROI'] >= ROI_lower_limit)]\n",
    "    \n",
    "    return new_df\n",
    "#download jpg urls from dataFrame\n",
    "def download(row):\n",
    "    filename = os.path.join(root_folder, str(row.name) + im_extension)\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.Image\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    except:\n",
    "        print(f'{filename} error')\n",
    "\n",
    "\n",
    "def cardinality_threshold(column,threshold=0.75,return_categories_list=True):\n",
    "    #calculate the threshold value using\n",
    "    #the frequency of instances in column\n",
    "    threshold_value=int(threshold*len(column))\n",
    "    #initialize a new list for lower cardinality column\n",
    "    categories_list=[]\n",
    "    #initialize a variable to calculate sum of frequencies\n",
    "    s=0\n",
    "    #Create a dictionary (unique_category: frequency)\n",
    "    counts=Counter(column)\n",
    "\n",
    "    #Iterate through category names and corresponding frequencies after sorting the categories\n",
    "    #by descending order of frequency\n",
    "    for i,j in counts.most_common():\n",
    "        #Add the frequency to the total sum\n",
    "        s += dict(counts)[i]\n",
    "        #append the category name to the categories list\n",
    "        categories_list.append(i)\n",
    "        #Check if the global sum has reached the threshold value, if so break the loop\n",
    "        if s >= threshold_value:\n",
    "            break\n",
    "      #append the new 'Other' category to list\n",
    "    categories_list.append('Other')\n",
    "\n",
    "    #Take all instances not in categories below threshold  \n",
    "    #that were kept and lump them into the\n",
    "    #new 'Other' category.\n",
    "    new_column = column.apply(lambda x: x if x in categories_list else 'Other')\n",
    "\n",
    "    #Return the transformed column and\n",
    "    #unique categories if return_categories = True\n",
    "    if(return_categories_list):\n",
    "        return new_column,categories_list\n",
    "    #Return only the transformed column if return_categories=False\n",
    "    else:\n",
    "        return new_column\n",
    "    \n",
    "def fix(col):\n",
    "    dd = dict()\n",
    "    for d in col:\n",
    "        values = list(d.values())\n",
    "        if len(values) == 2:\n",
    "            dd[values[0]] = values[1]\n",
    "    return dd\n",
    "\n",
    "#function for extracted item Specifics from Shopping API data\n",
    "def transform_item_specifics(df, perc=90.0):\n",
    "\n",
    "    df.dropna(subset=['ItemSpecifics'], inplace=True)\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(lambda x: ast.literal_eval(x))\n",
    "    df['item_list'] = df['ItemSpecifics'].apply(lambda x: x['NameValueList'])\n",
    "\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(lambda x: [x['NameValueList']] if isinstance(x['NameValueList'], dict) else x['NameValueList'])\n",
    "\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(fix)\n",
    "\n",
    "    df = pd.json_normalize(df['ItemSpecifics'])\n",
    "\n",
    "    min_count =  int(((100-perc)/100)*df.shape[0] + 1)\n",
    "    mod_df = df.dropna(axis=1, \n",
    "                       thresh=min_count)\n",
    "\n",
    "    return mod_df\n",
    "\n",
    "# This function removes noisy data\n",
    "#lots/sets/groups of knives can\n",
    "#confuse the model from predicting\n",
    "#the appropriate value of individual knives\n",
    "def data_cleaner(df):\n",
    "    lot = re.compile('(?<!-\\S)lot(?![^\\s.,:?!])')\n",
    "    group = re.compile('(group)')\n",
    "    is_set = re.compile('(?<!-\\S)set(?![^\\s.,?!])')\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    trim_list = [lot,group,is_set]\n",
    "    for item in trim_list:\n",
    "        df.loc[df['title'].apply(lambda x: re.search(item, x)).notnull(), 'trim'] = 1 \n",
    "    to_drop = df.loc[df['trim'] == 1].index\n",
    "    df.drop(to_drop, inplace=True)\n",
    "    df.drop('trim', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prepare_listed(listed_data_df, Ids_df):\n",
    "    listed_data_df.drop('galleryPlusPictureURL', axis=1, inplace=True)\n",
    "   \n",
    "    Ids_df.rename({'Title': 'title',\n",
    "                   'ItemID': 'itemId'},\n",
    "                    axis=1,inplace=True)\n",
    "    \n",
    "    Ids_df.drop(['ConditionID','ConvertedCurrentPrice'], \n",
    "                         axis=1, inplace=True)\n",
    "    Ids_df['title'] = Ids_df['title'].str.lower()\n",
    "    \n",
    "    df_merged = listed_data_df.merge(Ids_df)\n",
    "\n",
    "    df_spec = transform_item_specifics(df_merged, perc=65.0)\n",
    "    \n",
    "    df_spec.drop('Brand', axis=1, inplace=True)\n",
    "    \n",
    "    tot_listed_df = df_merged.join(df_spec)\n",
    "\n",
    "    listed_knives = data_cleaner(tot_listed_df).copy()\n",
    "    listed_knives.drop(['sellingStatus', 'shippingInfo', \n",
    "                        'GalleryURL', 'ItemSpecifics', \n",
    "                        'item_list', 'listingInfo'], \n",
    "                        axis=1, inplace=True)\n",
    "    listed_used_knives = listed_knives.loc[listed_knives['condition'] != 1000.0]\n",
    "    listed_used_knives.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return listed_used_knives\n",
    "\n",
    "\n",
    "def prepare_tera_df(df, x, overhead_cost=3):\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\"$\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\",\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].apply(float)\n",
    "    \n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\"$\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\",\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].apply(float)\n",
    "    \n",
    "    df['converted_price'] = (df['price_in_US'] + df['shipping_cost'])\n",
    "    \n",
    "    df['profit'] = ((df['converted_price']*.87) - list(bucket_dict.values())[x] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/(list(bucket_dict.values())[x]))*100.0\n",
    "    \n",
    "    df['brand'] = list(bucket_dict.keys())[x]\n",
    "    df['cost'] = list(bucket_dict.values())[x]\n",
    "\n",
    "    \n",
    "    return df    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Finding API data\n",
    "df_bench = pd.read_csv(\"listed_data/df_bench.csv\")\n",
    "df_buck = pd.read_csv(\"listed_data/df_buck.csv\")\n",
    "df_case = pd.read_csv(\"listed_data/df_case.csv\")\n",
    "df_caseXX = pd.read_csv(\"listed_data/df_CaseXX.csv\")\n",
    "df_crkt = pd.read_csv(\"listed_data/df_crkt.csv\")\n",
    "df_kersh = pd.read_csv(\"listed_data/df_kershaw.csv\")\n",
    "df_sog = pd.read_csv(\"listed_data/df_sog.csv\")\n",
    "df_spyd = pd.read_csv(\"listed_data/df_spyderco.csv\")\n",
    "df_vict = pd.read_csv(\"listed_data/df_victorinox.csv\")\n",
    "\n",
    "#load Shopping API data\n",
    "bench = pd.read_csv(\"listed_data/benchIds.csv\")\n",
    "buck = pd.read_csv(\"listed_data/buckIds.csv\")\n",
    "case = pd.read_csv(\"listed_data/caseIds.csv\")\n",
    "caseXX = pd.read_csv(\"listed_data/caseXXIds.csv\")\n",
    "crkt = pd.read_csv(\"listed_data/crktIds.csv\")\n",
    "kershaw = pd.read_csv(\"listed_data/kershawIds.csv\")\n",
    "sog = pd.read_csv(\"listed_data/sogIds.csv\")\n",
    "spyd = pd.read_csv(\"listed_data/spydIds.csv\")\n",
    "vict = pd.read_csv(\"listed_data/victIds.csv\")\n",
    "\n",
    "#Load scraped terapeak sold data\n",
    "sold_bench = pd.read_csv(\"terapeak_data/bench_scraped2.csv\")\n",
    "sold_buck1 = pd.read_csv(\"terapeak_data/buck_scraped2.csv\")\n",
    "sold_buck2 = pd.read_csv(\"terapeak_data/buck_scraped2_reversed.csv\")\n",
    "sold_case = pd.read_csv(\"terapeak_data/case_scraped2.csv\")\n",
    "sold_caseXX1 = pd.read_csv(\"terapeak_data/caseXX_scraped2.csv\")\n",
    "sold_caseXX2 = pd.read_csv(\"terapeak_data/caseXX2_reversed.csv\")\n",
    "sold_crkt = pd.read_csv(\"terapeak_data/crkt_scraped.csv\")\n",
    "sold_kershaw1 = pd.read_csv(\"terapeak_data/kershaw_scraped2.csv\")\n",
    "sold_kershaw2 = pd.read_csv(\"terapeak_data/kershaw_scraped2_reversed.csv\")\n",
    "sold_sog = pd.read_csv(\"terapeak_data/SOG_scraped2.csv\")\n",
    "sold_spyd = pd.read_csv(\"terapeak_data/spyd_scraped2.csv\")\n",
    "sold_vict1 = pd.read_csv(\"terapeak_data/vict_scraped.csv\")\n",
    "sold_vict2 = pd.read_csv(\"terapeak_data/vict_reversed.csv\")\n",
    "\n",
    "sold_list = [sold_bench,sold_buck1,\n",
    "             sold_buck2,sold_case,\n",
    "             sold_caseXX1,sold_caseXX2,\n",
    "             sold_crkt,sold_kershaw1,\n",
    "             sold_kershaw2,sold_sog, \n",
    "             sold_spyd, sold_vict1,\n",
    "             sold_vict2]\n",
    "\n",
    "\n",
    "listed_df = pd.concat([df_bench,df_buck,\n",
    "                       df_case,df_caseXX,\n",
    "                       df_crkt,df_kersh,\n",
    "                       df_sog,df_spyd,\n",
    "                       df_vict])\n",
    "\n",
    "\n",
    "Ids_df = pd.concat([bench,buck,\n",
    "                   case,caseXX,\n",
    "                   crkt,kershaw,\n",
    "                   sog,spyd,vict])\n",
    "\n",
    "used_listed_df = prepare_listed(listed_df, Ids_df)\n",
    "\n",
    "\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in sold_list:\n",
    "    dataframe.rename({'Text': 'title',\n",
    "                      'shipping_': 'shipping_cost'},\n",
    "                     axis=1, inplace=True)\n",
    "\n",
    "    dataframe['date_sold'] = pd.to_datetime(dataframe['date_sold'])\n",
    "\n",
    "sold_buck = pd.concat([sold_buck1,sold_buck2])\n",
    "sold_caseXX = pd.concat([sold_caseXX1,sold_caseXX2])\n",
    "sold_kershaw = pd.concat([sold_kershaw1,sold_kershaw2])\n",
    "sold_vict = pd.concat([sold_vict1,sold_vict2])\n",
    "\n",
    "sold_bench = prepare_tera_df(sold_bench, 0)\n",
    "sold_buck = prepare_tera_df(sold_buck, 1)\n",
    "sold_case = prepare_tera_df(sold_case, 2)\n",
    "sold_caseXX = prepare_tera_df(sold_caseXX, 2)\n",
    "sold_crkt = prepare_tera_df(sold_crkt, 3)\n",
    "sold_kershaw = prepare_tera_df(sold_kershaw, 4)\n",
    "sold_sog = prepare_tera_df(sold_sog, 5)\n",
    "sold_spyd = prepare_tera_df(sold_spyd, 6)\n",
    "sold_vict = prepare_tera_df(sold_vict, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in sold_list:\n",
    "    dataframe['title'] = dataframe['title'].str.lower()\n",
    "    dataframe['title'] = dataframe['title'].str.strip()\n",
    "    dataframe.drop_duplicates(\n",
    "        subset = ['date_sold','price_in_US', \n",
    "                  'shipping_cost'],\n",
    "        keep = 'last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_df = pd.concat([sold_bench, sold_buck,\n",
    "                     sold_case, sold_caseXX, \n",
    "                     sold_crkt, sold_kershaw,\n",
    "                     sold_sog, sold_spyd,\n",
    "                     sold_vict]) \n",
    "\n",
    "sold_knives = data_cleaner(sold_df).copy()\n",
    "\n",
    "\n",
    "df = pd.concat([sold_knives,used_listed_df]).copy()\n",
    "df['Image'].fillna(df['pictureURLLarge'], inplace=True)\n",
    "\n",
    "df = apply_iqr_filter(df).copy()\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network with \"title\" column as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = df.loc[:, ['title', 'converted_price']]\n",
    "\n",
    "df['title'] = df['title'].str.replace(\"'\", \"\")\n",
    "\n",
    "df_title.rename({'title': 'data',\n",
    "                 'converted_price': 'labels'},\n",
    "                axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76400 entries, 0 to 76399\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   data    76400 non-null  object \n",
      " 1   labels  76400 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_title.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_title['labels'] = (df_title['labels']/mean_price)\n",
    "Y = df_title['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, Ytrain, Ytest = train_test_split(df_title['data'], Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(22920, 1), dtype=float32, numpy=\n",
       "array([[ 2.044234  ],\n",
       "       [-1.0531895 ],\n",
       "       [ 0.12339567],\n",
       "       ...,\n",
       "       [ 0.481761  ],\n",
       "       [ 0.5520005 ],\n",
       "       [-0.93306535]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer = tf.keras.layers.experimental.preprocessing.Normalization(axis=-1)\n",
    "# layer.adapt(Ytrain)\n",
    "# layer(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, Y_val, Y_test = train_test_split(df_test, Ytest, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to sequences\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(df_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "V = len(word2idx)\n",
    "print('Found %s unique tokens.' % V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences so that we get a N x T matrix\n",
    "data_train = pad_sequences(sequences_train)\n",
    "print('Shape of data train tensor:', data_train.shape)\n",
    "\n",
    "# get sequence length\n",
    "T = data_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = pad_sequences(sequences_val, maxlen=T)\n",
    "print('Shape of data test tensor:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pad_sequences(sequences_test, maxlen=T)\n",
    "print('Shape of data test tensor:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RNN model\n",
    "\n",
    "# We get to choose embedding dimensionality\n",
    "D = 20\n",
    "\n",
    "# Hidden state dimensionality\n",
    "M = 15\n",
    "\n",
    "\n",
    "i = Input(shape=(T,))\n",
    "x = Embedding(V + 1, D)(i)\n",
    "x = LSTM(M, return_sequences=True)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "\n",
    "model = Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "model.compile(\n",
    "  loss='MSE',\n",
    "  optimizer='adam',\n",
    "  metrics=['mae']\n",
    ")\n",
    "\n",
    "\n",
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  data_train,\n",
    "  Ytrain,\n",
    "  epochs=5,\n",
    "  validation_data=(data_val, Y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.evaluate(data_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12,8))\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.title(\"Loss vs val Loss for RNN model on titles (MSE)\", fontsize=15)\n",
    "plt.xlabel(\"epochs\", fontsize=15)\n",
    "plt.ylabel(\"loss (mean squared error)\", fontsize=15)\n",
    "plt.legend();\n",
    "plt.savefig('images/RNN_titles_MSE1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12,8))\n",
    "plt.plot(r.history['mae'], label='mae')\n",
    "plt.plot(r.history['val_mae'], label='val_mae')\n",
    "plt.title(\"Loss vs val Loss for RNN model on titles (MAE)\", fontsize=15)\n",
    "plt.xlabel(\"epochs\", fontsize=15)\n",
    "plt.ylabel(\"loss (mean absolute error)\", fontsize=15)\n",
    "plt.legend();\n",
    "plt.savefig('images/RNN_titles_MAE1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_scaled = Y_test*mean_price\n",
    "pred_scaled = pred*mean_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae = mean_absolute_error(Y_test_scaled, pred_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_score = f'\\nMAE on training set: ${test_mae:.2f}'\n",
    "string_score += f'\\nMean resale value of knives: ${mean_price:.2f}'\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plt.scatter(Y_test_scaled, pred_scaled)\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\"red\")\n",
    "plt.text(0, 140, string_score)\n",
    "plt.title('Regression Model for Predicting Resale Value')\n",
    "plt.ylabel('Model predictions for Resale Value($US)')\n",
    "plt.xlabel('True Values for Resale Value($US)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.276 * mean_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CNN model\n",
    "\n",
    "# We get to choose embedding dimensionality\n",
    "D = 20\n",
    "\n",
    "\n",
    "\n",
    "i = Input(shape=(T,))\n",
    "x = Embedding(V + 1, D)(i)\n",
    "x = Conv1D(32, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(64, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "\n",
    "model = Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "model.compile(\n",
    "  loss='MSE',\n",
    "  optimizer='adam',\n",
    "  metrics=['MSE']\n",
    ")\n",
    "\n",
    "\n",
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  data_train,\n",
    "  Ytrain,\n",
    "  epochs=5,\n",
    "  validation_data=(data_test, Ytest)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss per iteration\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy per iteration\n",
    "plt.plot(r.history['MSE'], label='MSE')\n",
    "plt.plot(r.history['val_MSE'], label='val_MSE')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN using images as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs = df.drop(['title', 'url', \n",
    "                   'date_sold', 'profit',\n",
    "                   'ROI', 'brand', 'cost',\n",
    "                   'pictureURLLarge'],\n",
    "                     axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs.dropna(subset=['Image'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs['file_index'] = df_imgs.index.values\n",
    "df_imgs['file_index'] = df_imgs['file_index'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs['filename'] = df_imgs['file_index'] + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Image Resolutions\n",
    "\n",
    "# # Import Packages\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot  as plt\n",
    "# from PIL import Image\n",
    "# from pathlib import Path\n",
    "# import imagesize\n",
    "# import numpy as np\n",
    "\n",
    "# # Get the Image Resolutions\n",
    "# imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "# img_meta = {}\n",
    "# for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# # Convert it to Dataframe and compute aspect ratio\n",
    "# img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "# img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "# img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "# print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "# img_meta_df.head()\n",
    "\n",
    "\n",
    "\n",
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(row):\n",
    "    filename = row.filepath\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "#     os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.Image\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    except:\n",
    "        print(f'{filename} error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images/'\n",
    "df_imgs['filepath'] = root_folder + df_imgs['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs['filepath'].sample(2).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_imgs.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_files = []\n",
    "pathway = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images/'\n",
    "for filename in os.listdir(pathway):\n",
    "    if filename.endswith('.jpg'):\n",
    "        try:\n",
    "            img = Image.open(pathway + filename)  # open the image file\n",
    "            img.verify()  # verify that it is, in fact an image\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(filename)\n",
    "            removed_files.append(filename)\n",
    "            os.remove(pathway + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = df_imgs.loc[df_imgs['filename'].isin(removed_files)].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " img_list = os.listdir('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df = df_imgs.loc[df_imgs['filename'].isin(img_list)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df.rename({'Image': 'data',\n",
    "               'converted_price': 'labels'},\n",
    "                axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_price = img_df['labels'].median()\n",
    "median_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df['labels'] = (img_df['labels']/median_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = img_df['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, Ytrain, Ytest = train_test_split(img_df, Y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train,\n",
    "directory= None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"labels\",\n",
    "subset=\"training\",\n",
    "batch_size=100,\n",
    "seed=55,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\")\n",
    "    \n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"labels\",\n",
    "subset=\"validation\",\n",
    "batch_size=100,\n",
    "seed=55,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\")\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=df_test,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"labels\",\n",
    "batch_size=100,\n",
    "seed=55,\n",
    "shuffle=False,\n",
    "class_mode=\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(16, (3, 3), padding='same', activation='relu',\n",
    "                        input_shape=(256 ,256,  3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "                        input_shape=(256 ,256,  3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='Adam',\n",
    "               metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model.fit(train_generator, epochs=3, validation_data=valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict(test_generator,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(summary.history['loss'])\n",
    "plt.plot(summary.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss(mean absolute error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define two sets of inputs\n",
    "# inputA = Input(shape=(32,))\n",
    "# inputB = Input(shape=(128,))\n",
    "# # the first branch operates on the first input\n",
    "# x = Dense(8, activation=\"relu\")(inputA)\n",
    "# x = Dense(4, activation=\"relu\")(x)\n",
    "# x = Model(inputs=inputA, outputs=x)\n",
    "# # the second branch opreates on the second input\n",
    "# y = Dense(64, activation=\"relu\")(inputB)\n",
    "# y = Dense(32, activation=\"relu\")(y)\n",
    "# y = Dense(4, activation=\"relu\")(y)\n",
    "# y = Model(inputs=inputB, outputs=y)\n",
    "# # combine the output of the two branches\n",
    "# combined = concatenate([x.output, y.output])\n",
    "# # apply a FC layer and then a regression prediction on the\n",
    "# # combined outputs\n",
    "# z = Dense(2, activation=\"relu\")(combined)\n",
    "# z = Dense(1, activation=\"linear\")(z)\n",
    "# # our model will accept the inputs of the two branches and\n",
    "# # then output a single value\n",
    "# model = Model(inputs=[x.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
