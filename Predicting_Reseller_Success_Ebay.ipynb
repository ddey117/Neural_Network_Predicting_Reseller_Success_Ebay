{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly0Flp1GSKsM"
   },
   "source": [
    "\n",
    "# Constructing A Neural Network To Classify Knives from a Texas Government Surplus Store\n",
    "\n",
    "## Categorizing Nine Different Knives as Profitable or Not Profitable using a CNN for Knife Images and a RNN for Titles from Ebay Listings \n",
    "\n",
    "**Author:** Dylan Dey\n",
    "***\n",
    "\n",
    "# Overview\n",
    "[Texas State Surplus Store](https://www.tfc.texas.gov/divisions/supportserv/prog/statesurplus/)\n",
    "\n",
    "[What happens to all those items that get confiscated by the TSA? Some end up in a Texas store.](https://www.wfaa.com/article/news/local/what-happens-to-all-those-items-that-get-confiscated-by-the-tsa-some-end-up-in-a-texas-store/287-ba80dac3-d91a-4b28-952a-0aaf4f69ff95)\n",
    "\n",
    "[Texas Surplus Store PDF](https://www.tfc.texas.gov/divisions/supportserv/prog/statesurplus/State%20Surplus%20Brochure-one%20bar_rev%201-10-2022.pdf)\n",
    "\n",
    "![Texas State Surplus Store](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRYkwyu20VBuQ52PrXdVRaGRIIg9OPXJg86lA&usqp=CAU)\n",
    "\n",
    "![Texas Knives In Stores](https://arc-anglerfish-arc2-prod-bostonglobe.s3.amazonaws.com/public/MWJCCFBSR4I6FCSNKONTFJIRAI.jpg)\n",
    "\n",
    "[Everything that doesn't make it through Texas airports can be found at one Austin store](https://cbsaustin.com/news/local/everything-that-doesnt-make-it-through-texas-airports-can-be-found-at-one-austin-store)\n",
    "\n",
    "\n",
    "> The Texas Facilities Commission collects left behind possessions, salvage, and surplus from Texas state agencies such as DPS, TXDOT, TCEQ, and Texas Parks & Wildlife. Examples of commonly available items include vehicles, furniture, office equipment and supplies, small electronics, and heavy equipment. The goal of this project is to create a Neural Network Classification Model in order to categorize knives available at this store as either profitable or not on ebay. \n",
    "\n",
    "# Business Problem\n",
    "\n",
    "[Family Ebay Store Front](https://www.ebay.com/str/texasdave3?mkcid=16&mkevt=1&mkrid=711-127632-2357-0&ssspo=ZW3G27tGR_m&sssrc=3418065&ssuid=&widget_ver=artemis&media=COPY)\n",
    "\n",
    "![Father's Ebay Account Since 1999](texas_dave.jpg)\n",
    "\n",
    "[Texas Dave's Knives](https://www.ebay.com/str/texasdave3/Knives/_i.html?store_cat=3393246519)\n",
    "\n",
    "\n",
    "> While taking online courses to transition careers during a difficult time of my life, I was also helping my family during a turbulent time for everyone. I have been employed at their retail store in San Antonio for the past several months and have been contributing significantly to their online reselling business on eBay. I would help source newer, cheaper products from Austin to try and resell at the retail store in San Antonio or online to earn some money, support our family business and keep us all afloat. This is how I discovered the Texas Facillities Retail Store. \n",
    "\n",
    "\n",
    "> My family has been running a resale shop and selling on Ebay and other sites for years and lately the business has picked up.  Consumer behavior is shifting:  getting a deal on eBay, or Goodwill, or hitting up a vintage boutique shop to find a unique treasure is now brag worthy.  Plus, people like the idea of sustainability - sending items to landfills is becoming very socially unacceptable – why not repurpose a used item?  With the pandemic related disruption of “normal” business and supply chains and the economic uncertainty of these times there is definitely an upswing in interest in the resale market. \n",
    "\n",
    "> Online sales sites like Ebay offer a worldwide robust buyer base for just about every product regardless of condition. Ebay  allows the reseller to find both  bargain hunters for common items and  enthusiasts searching for rare  collectible items. \n",
    "\n",
    "> An Ebay business has some pain points, however.  Selection of an item to sell is the main pain point. The item should be readily available in decent condition for the seller to purchase at a low price but not so widely available that the market is saturated with that item.  Then there needs to be a demand for the item – it should be something collectible that with appeal to hobbyists that would pay premium prices for hard-to-get items. Alternatively, it would be something useful to a large number of people even in a used condition. The item should be small enough to be easily shipped. It should not be difficult to ship either—that is it should not have hazardous chemicals, batteries etc. that would add costs to the shipping. Additionally, Ebay has strict rules about authentication and certification in many item categories- so obvious “high value” items like jewelry or designer purses are so restricted that it is not  feasible  for the average Ebay seller  to offer them . \n",
    "\n",
    "> This project recommends an item that would answer these concerns – pocket knives, These can be rare and collectible and also practical and useful. There are knife collector forums and subReddits, showing there is an interest among collectors.  A look at eBay listings shows rare knives selling for thousands of \n",
    "dollars each.  Knives are also a handy every day tool –  and based on the number  showing up in the Texas Surplus shop they are easy to lose and so need replacing often. This means there is a market for more common ones as well.  The great thing about single blade, modern, factory manufactured pocketknives is that they all weigh roughly 0.5 lbs making them cheap to ship. For my modeling purposes, it is safe to assume a flat shipping rate of 4.95(US Dollars) including the cost of wholesale purchased padded envelopes. And there are no restrictions on mailing these items and they are not frsgile so no special packaging is needed. \n",
    "\n",
    "> The second pain point is buying at a cost low enough to make a profit. It is not enough to just buy low and sell at a higher price as expenses need to be considered.  Ebay collects insertion fees and final value fees on all sales.  The fees vary with seller level (rating)  and some portions  are a percent of final sale. I have been selling knives from the lower priced bins and the mean seller fee for my sales so far is about 13.5% of the sold price.  So that is a cost to consider right up front. \n",
    "\n",
    "> A third pain point is the cost of excess inventory. A seller can obtain quality items at a reasonable cost and then the inventory may sit with no sales, meaning the capital expended is sitting tied up in unwanted items. This inventory carry cost is a drain on profitability.  This project is meant to help avoid purchasing the wrong items for resale. \n",
    "\n",
    "\n",
    "> As already mentioned, I have been experimenting with low cost used knives for resale but have not risked a large capital investment in the higher end items. The goal of this project is to attempt to address the pain points to determine if a larger investment would pay off. Can I identify which knives are worth investing in so that I can turn a decent profit and hopefully avoid excess inventory? A data driven approach would help avoid costly mistakes from the \"system\" resellers currently employ, which seems to be mainly a gambler’s approach. By managing resources upfront through a model, I can effectively increase my return on investment with messy data such as pictures and titles. The magic of Neural Networks!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> There are nine buckets of presorted brand knives that I was interested in, specifically. The other buckets are full of unbranded knives that usually are crowded with way too many people. These other bins, however, are behind glass, presorted, branded(and therefore have specific characteristics and logos for my model to identify), and priced higher. \n",
    "\n",
    "\n",
    "\n",
    "** knife bucket image ** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Ebay Developer Website](https://developer.ebay.com/)\n",
    "> Ebay has a seperate website for develoers in order to create an account and register an application keyset in order to make API call requests to their live website. By making a findItemsAdvanced call to the eBay Finding APIVersion 1.13.0, I was able to get a large dataset of[category_id=<48818>](https://www.ebay.com/sch/48818/i.html?_from=R40&_nkw=knife) knives.\n",
    "\n",
    "> When you log into Ebay as a buyer and search knife in the search bar, the response that loads outputs  Knives, Swords & Blades. Nested one category furtheris Collectible Folding Knives with an id of 182981. Nested one further is Modern Folding Knives(43333), and then finally, the category_id of most interest, 48818, Factory Manufactured Modern Collectible Folding Knives.\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What are the business's pain points related to this project?\n",
    "* How did you pick the data analysis question(s) that you did?\n",
    "* Why are these questions important from a business perspective?\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Data Understanding\n",
    "\n",
    "Describe the data being used for this project.\n",
    "***\n",
    "Questions to consider:\n",
    "* Where did the data come from, and how do they relate to the data analysis questions?\n",
    "* What do the data represent? Who is in the sample and what variables are included?\n",
    "* What is the target variable?\n",
    "* What are the properties of the variables you intend to use?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NJHL022ptlVx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import xmltodict, json\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 26D5-9745\n",
      "\n",
      " Directory of C:\\Users\\12108\\Desktop\\ebay_knife_data\\dsc-5-capstone-project\n",
      "\n",
      "07/05/2022  08:02 PM    <DIR>          .\n",
      "07/05/2022  08:02 PM    <DIR>          ..\n",
      "06/24/2022  04:24 PM                29 .gitignore\n",
      "07/05/2022  02:43 PM    <DIR>          .ipynb_checkpoints\n",
      "06/24/2022  04:24 PM                92 .learn\n",
      "07/05/2022  04:39 PM             1,118 aspect_filter.xml\n",
      "07/02/2022  07:01 PM            23,440 Call that a Knoife.ipynb\n",
      "06/24/2022  04:24 PM             1,849 CONTRIBUTING.md\n",
      "06/30/2022  02:39 PM    <DIR>          data\n",
      "07/05/2022  05:22 PM             9,568 Ebay Finding API.ipynb\n",
      "06/24/2022  04:24 PM                57 ebay_api_key.env\n",
      "06/24/2022  04:24 PM        49,147,416 full_dataset.csv\n",
      "07/05/2022  05:18 PM         4,896,847 full_dataset3.csv\n",
      "06/29/2022  09:18 PM    <DIR>          images\n",
      "06/24/2022  04:24 PM             1,371 LICENSE.md\n",
      "07/05/2022  07:52 PM            41,243 Predicting_Reseller_Success_Ebay.ipynb\n",
      "06/27/2022  10:25 AM    <DIR>          preparedData\n",
      "06/24/2022  04:24 PM            12,259 README.md\n",
      "06/30/2022  03:52 PM    <DIR>          surplusStore\n",
      "07/05/2022  08:02 PM            55,670 texas_dave.jpg\n",
      "06/30/2022  02:39 PM    <DIR>          val_counts\n",
      "              13 File(s)     54,190,959 bytes\n",
      "               8 Dir(s)  1,852,089,552,896 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWyJEtLjtAqo"
   },
   "source": [
    "# Data Obtainment: eBay Finding APIVersion 1.13.0  \"http://www.ebay.com/marketplace/search/v1/services\"\n",
    "\n",
    "```\n",
    "import xmltodict, json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"ebay_api_key.env\") #hide production key in environment file\n",
    "\n",
    "\n",
    "\n",
    "APP_ID = os.getenv(\"EBAY_PROD_ID\")\n",
    "```\n",
    "### URL\n",
    "HTTP call url to send the Production Client API. The Payload will modify the url below with a ? followed by an % and key value pairs. This is the most common and recommended way that ebay suggests users to send a request to the Finding API.\n",
    "\n",
    "### HTTP Headers\n",
    "Required headers to return a 200 response code. The top header is AuthnAuth security. As there is no need for a user token and the only interface is with my app, the only security required is for the app. The second row is the call be sent to the Finding API, findItemsAdvanced. ## THIS SHOULD BE A LINK DOG. The third formats the respons as JSON.\n",
    "\n",
    "### Payload\n",
    "Modifies HTTP call. Search for knife in the Collectible-Modern-Factory-Manufactured-Folding-Knives that is of the Used Condition. This request returns 100 pages on page 1.\n",
    "\n",
    "\n",
    "```\n",
    "url = \"https://svcs.ebay.com/services/search/FindingService/v1\" #url fir Finding API\n",
    "\n",
    "\n",
    "headers = {\"X-EBAY-SOA-SECURITY-APPNAME\": APP_ID, #production client_id\n",
    "           \"X-EBAY-SOA-OPERATION-NAME\": \"findItemsAdvanced\", #call_id\n",
    "           \"X-EBAY-API-RESPONSE-ENCODING\": \"JSON\" #format respons as JSON\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "payload = {\"keywords\":\"knife\", #knife in search bar\n",
    "           \"categoryId\":\"48818\", #Collectible-Modern-Factory-Manufactured-Folding-Knives\n",
    "          \"itemFilter.name\":\"Condition\", #item filter, used condition\n",
    "          \"itemFilter.value\":\"Used\",\n",
    "          \"paginationInput.entriesPerPage\":100, #pagination \n",
    "          \"paginationInput.pageNumber\":1\n",
    "          }\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### request section\n",
    "```\n",
    "r = requests.get(url, headers=headers, params=payload)\n",
    "json_dict_100knivesp1 = xmltodict.parse(r.text)\n",
    "knives_data_list_of_dicts = json_dict_100knivesp1['findItemsAdvancedResponse']['searchResult']['item']\n",
    "total_pages = int(json_dict_100knivesp1['findItemsAdvancedResponse']['paginationOutput']['totalPages'])\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xJ2lkpov0BT"
   },
   "source": [
    "### Function For Organizing Call Response\n",
    "```\n",
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "        'viewItemURL', 'postalCode', 'location', \n",
    "        'country', 'shippingInfo', 'sellingStatus', \n",
    "        'listingInfo', 'returnsAccepted', 'condition'\n",
    "       ]\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "        if all(prepared_data.values()):\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GJVsHGWv86E"
   },
   "source": [
    "### Organize Call with Function\n",
    "\n",
    "```\n",
    "prepared_knives = prepare_data(knives_data_list_of_dicts)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hrxkIw-wIuk"
   },
   "source": [
    "### Paginate and append responses to empty list to get full list of data\n",
    "\n",
    "\n",
    "```\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "for page in range(total_pages):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    payload[\"paginationInput.pageNumber\"] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "    response = requests.get(url, headers=headers, params=payload)\n",
    "    \n",
    "    # Get the response body in JSON format\n",
    "    response_json = xmltodict.parse(r.text)\n",
    "    \n",
    "    # Get the list of businesses from the response_json\n",
    "    knives_data_list_of_dicts = response_json['findItemsAdvancedResponse']['searchResult']['item']\n",
    "    \n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(knives_data_list_of_dicts)\n",
    "    \n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "df = pd.DataFrame(full_dataset)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EteiyNxgwIxE"
   },
   "source": [
    "## Pull converted price from nested dictionary\n",
    "\n",
    "```\n",
    "\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    price_list.append(row['sellingStatus']['convertedCurrentPrice']['#text'])\n",
    "    \n",
    "    \n",
    "df[\"price_in_US\"] = price_list\n",
    "\n",
    "\n",
    "df.to_csv(\"full_datase.csv\", index=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwLbGNr0TbIj"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* Were there variables you dropped or created?\n",
    "* How did you address missing values or outliers?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "\n",
    "\n",
    "# here you run your code to clean the data\n",
    "\n",
    "```\n",
    "import code.data_cleaning as dc\n",
    "\n",
    "full_dataset = dc.full_clean()\n",
    "```\n",
    "\n",
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "# here you run your code to model the data\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do in the future to improve this project?\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('full_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict = {'benchmade': 45,\n",
    "               'buck': 20,\n",
    "               'case': 20,\n",
    "               'crkt': 15,\n",
    "               'kershaw': 15,\n",
    "               'leatherman': 30, \n",
    "               'sog': 15,\n",
    "               'spyderco': 30,\n",
    "               'victorinox': 20\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DI_PDFVzOuX_"
   },
   "outputs": [],
   "source": [
    "#lowercase titles\n",
    "df.title = df.title.apply(str.lower)\n",
    "\n",
    "#remove special characters\n",
    "df.title.apply(pp.remove_special_chars)\n",
    "#remove special characters\n",
    "df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "test = ' '.join(df['title'].sample(500))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQTkWMsUOua0"
   },
   "outputs": [],
   "source": [
    "# #REGEX BRAND PATTERNS AFTERN LOWERCASING TITLES AND REMOVING SPEVIAL CHARACTERS\n",
    "\n",
    "\n",
    "#turn this into a dict bro\n",
    "\n",
    "# benchmade_pattern = \"benchmade\"\n",
    "# buck_pattern = \"buck\"\n",
    "# case_pattern = \"case\"\n",
    "# crkt_pattern = \"crkt\"\n",
    "# kershaw_pattern = \"kershaw\"\n",
    "# leatherman_pattern = \"leatherman\"\n",
    "# sog_pattern = \"sog\"\n",
    "# spyderco_pattern = \"spyderco\"\n",
    "# victorinox_pattern = \"victorinox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['is_profitable'] = (df['price_in_US'] - df[list(costs.keys())].sum(axis=1))>0\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvwvPz68W4mU"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "df = pd.read_csv(\"surplusStore/workingDataFrame2.csv\")\n",
    "\n",
    "df.columns\n",
    "\n",
    "costs = {'benchmade': 45,\n",
    "         'buck': 20,\n",
    "         'case': 20,\n",
    "         'crkt': 15,\n",
    "         'kershaw': 15,\n",
    "         'leatherman': 30,\n",
    "         'sog': 15,\n",
    "         'spyderco': 30,\n",
    "         'victorinox': 20,\n",
    "        }\n",
    "\n",
    "df['is_profitable'] = (df['price_in_US'] - df[list(costs.keys())].sum(axis=1))>0\n",
    "\n",
    "df.info()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_brand(pattern):\n",
    "    df[pattern] = df.title.apply(lambda x: re.findall(pattern, x))\n",
    "    df[pattern] = df[pattern].apply(lambda x: np.nan if len(x)==0 else 1)\n",
    "    df[pattern].fillna(0, inplace=True)\n",
    "    df['is_profitable'] = (df['price_in_US'] - df[list(bucket_dict .keys())].sum(axis=1))>0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(bucket_dict.keys()):\n",
    "    find_brand(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3-_pAWYVbtA"
   },
   "outputs": [],
   "source": [
    "print('Benchmade Value Counts')\n",
    "display(df.benchmade.value_counts(normalize=False))\n",
    "print('-------------------------------------------')\n",
    "\n",
    "print('Buck Value Counts')\n",
    "display(df.buck.value_counts(normalize=False))\n",
    "print('-------------------------------------------')\n",
    "\n",
    "print('Case Value Counts')\n",
    "display(df.case.value_counts(normalize=False))\n",
    "print('-------------------------------------------')\n",
    "\n",
    "print('CRKT Value Counts (Normalized)')\n",
    "display(df.crkt.value_counts(normalize=True))\n",
    "print('-------------------------------------------')\n",
    "\n",
    "print('Kershaw Value Counts')\n",
    "display(df.kershaw.value_counts(normalize=False))\n",
    "print('-------------------------------------------')\n",
    "\n",
    "print('Leatherman Value Counts')\n",
    "display(df.leatherman.value_counts(normalize=False))\n",
    "print('-------------------------------------------')\n",
    "\n",
    "print('Sog Value Counts (Normalized)')\n",
    "display(df.sog.value_counts(normalize=True))\n",
    "print('-------------------------------------------')\n",
    "\n",
    "print('Spyderco Value Counts')\n",
    "display(df.spyderco.value_counts(normalize=False))\n",
    "print('-------------------------------------------')\n",
    "\n",
    "print('Victorinox Value Counts')\n",
    "df.victorinox.value_counts(normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop unnecessary columns\n",
    "# to_drop = list(df.columns[5:10])\n",
    "# to_drop.append(df.columns[0])\n",
    "# zero_count_brands = ['crkt', 'sog']\n",
    "# df.drop(columns=to_drop + zero_count_brands, index=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIS CALL TO THE WEBSITE RETURNED NO SOG KNIVES OR CRKT KNIVES\n",
    "## MUST MAKE SEPERATE CALLS\n",
    "## NOT SHOWN IN THIS NOTEBOOK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTpEYt6FV261"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_CRKT = pd.read_csv('data/full_dataset_CRKT.csv')\n",
    "df_SOG = pd.read_csv('data/full_dataset_SOG.csv')\n",
    "\n",
    "\n",
    "\n",
    "# df_SOG.info()\n",
    "\n",
    "# df_SOG['sog'] = 1.0\n",
    "# df_SOG['sog'] = 1.0\n",
    "\n",
    "# mkdir surplusStore\n",
    "\n",
    "# df_surplus.to_csv('surplusStore/workingDataFrame.csv', index=False)\n",
    "# df_CRKT.to_csv('surplusStore/df_CRKT.csv', index=False)\n",
    "# df_SOG.to_csv('surplusStore/df_SOG.csv', index=False)\n",
    "\n",
    "# df_surplus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUiDAmQAWck1"
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "\n",
    "\n",
    "# path = r'C:\\Users\\12108\\Desktop\\ebay_knife_data\\dsc-5-capstone-project\\surplusStore'                     # use your path\n",
    "# all_files = glob.glob(os.path.join(path, \"*.csv\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "# df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "# concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "# concatenated_df.head()\n",
    "\n",
    "# concatenated_df.fillna(0, inplace=True)\n",
    "\n",
    "# concatenated_df.info()\n",
    "\n",
    "# concatenated_df.to_csv('surplusStore/workingDataFrame2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XptkIA-htqb4"
   },
   "source": [
    "# Data Science Processes\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As discussed, this section is all about synthesizing your skills in order to work through a full Data Science workflow. In this lesson, you'll take a look at some general outlines for how Data Scientists organize their workflow and conceptualize their process.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- List the different data science process frameworks\n",
    "- Compare and contrast popular data science process frameworks such as CRISP-DM, KDD, OSEMN\n",
    "\n",
    "\n",
    "## What is a Data Science Process?\n",
    "\n",
    "Data Science projects are often complex, with many stakeholders, data sources, and goals. Due to this, the Data Science community has created several methodologies for helping organize and structure Data Science Projects.  In this lesson, you'll explore three of the most popular methodologies -- **_CRISP-DM_**, **_KDD_**, and **_OSEMN_**, and explore how you can make use of them to keep your projects well-structured and organized. \n",
    "\n",
    "## CRoss-Industry Standard Process for Data Mining (CRISP-DM)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_crisp-dm.png\" width=\"500\">\n",
    "\n",
    "**_CRISP-DM_** is probably the most popular Data Science process in the Data Science world right now. Take a look at the visualization above to get a feel for CRISP-DM. Notice that CRISP-DM is an iterative process!\n",
    "\n",
    "Let's take a look at the individual steps involved in CRISP-DM.\n",
    "\n",
    "**_Business Understanding:_**  This stage is all about gathering facts and requirements. Who will be using the model you build? How will they be using it? How will this help the goals of the business or organization overall? Data Science projects are complex, with many moving parts and stakeholders. They're also time intensive to complete or modify. Because of this, it is very important that the Data Science team working on the project has a deep understanding of what the problem is, and how the solution will be used. Consider the fact that many stakeholders involved in the project may not have technical backgrounds, and may not even be from the same organization.  Stakeholders from one part of the organization may have wildly different expectations about the project than stakeholders from a different part of the organization -- for instance, the sales team may be under the impression that a recommendation system project is meant to increase sales by recommending upsells to current customers, while the marketing team may be under the impression that the project is meant to help generate new leads by personalizing product recommendations in a marketing email. These are two very different interpretations of a recommendation system project, and it's understandable that both departments would immediately assume that the primary goal of the project is one that helps their organization. As a Data Scientist, it's up to you to clarify the requirements and make sure that everyone involved understands what the project is and isn't. \n",
    "\n",
    "During this stage, the goal is to get everyone on the same page and to provide clarity on the scope of the project for everyone involved, not just the Data Science team. Generate and answer as many contextual questions as you can about the project. \n",
    "\n",
    "Good questions for this stage include:\n",
    "\n",
    "- Who are the stakeholders in this project? Who will be directly affected by the creation of this project?\n",
    "- What business problem(s) will this Data Science project solve for the organization?  \n",
    "- What problems are inside the scope of this project?\n",
    "- What problems are outside the scope of this project?\n",
    "- What data sources are available to us?\n",
    "- What is the expected timeline for this project? Are there hard deadlines (e.g. \"must be live before holiday season shopping\") or is this an ongoing project?\n",
    "- Do stakeholders from different parts of the company or organization all have the exact same understanding about what this project is and isn't?\n",
    "\n",
    "**_Data Understanding:_**\n",
    "\n",
    "Once we have a solid understanding of the business implications for this project, we move on to understanding our data. During this stage, we'll aim to get a solid understanding of the data needed to complete the project.  This step includes both understanding where our data is coming from, as well as the information contained within the data. \n",
    "\n",
    "Consider the following questions when working through this stage:\n",
    "\n",
    "- What data is available to us? Where does it live? Do we have the data, or can we scrape/buy/source the data from somewhere else?\n",
    "- Who controls the data sources, and what steps are needed to get access to the data?\n",
    "- What is our target?\n",
    "- What predictors are available to us?\n",
    "- What data types are the predictors we'll be working with?\n",
    "- What is the distribution of our data?\n",
    "- How many observations does our dataset contain? Do we have a lot of data? Only a little? \n",
    "- Do we have enough data to build a model? Will we need to use resampling methods?\n",
    "- How do we know the data is correct? How is the data collected? Is there a chance the data could be wrong?\n",
    "\n",
    "**_Data Preparation:_**\n",
    "\n",
    "Once we have a strong understanding of our data, we can move onto preparing the data for our modeling steps. \n",
    "\n",
    "During this stage, we'll want to handle the following issues:\n",
    "\n",
    "- Detecting and dealing with missing values\n",
    "- Data type conversions (e.g. numeric data mistakenly encoded as strings)\n",
    "- Checking for and removing multicollinearity (correlated predictors)\n",
    "- Normalizing our numeric data\n",
    "- Converting categorical data to numeric format through one-hot encoding\n",
    "\n",
    "**_Modeling:_**\n",
    "\n",
    "Once we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task. \n",
    "\n",
    "Consider the following questions during the modeling step:\n",
    "\n",
    "- Is this a classification task? A regression task? Something else?\n",
    "- What models will we try?\n",
    "- How do we deal with overfitting?\n",
    "- Do we need to use regularization or not?\n",
    "- What sort of validation strategy will we be using to check that our model works well on unseen data?\n",
    "- What loss functions will we use?\n",
    "- What threshold of performance do we consider as successful?\n",
    "\n",
    "**_Evaluation:_**\n",
    "\n",
    "During this step, we'll evaluate the results of our modeling efforts. Does our model solve the problems that we outlined all the way back during step 1? Why or why not? Often times, evaluating the results of our modeling step will raise new questions, or will cause us to consider changing our approach to the problem.  Notice from the CRISP-DM diagram above, that the \"Evaluation\" step is unique in that it points to both _Business Understanding_ and _Deployment_.  As we mentioned before, Data Science is an iterative process -- that means that given the new information our model has provided, we'll often want to start over with another iteration, armed with our newfound knowledge! Perhaps the results of our model showed us something important that we had originally failed to consider the goal of the project or the scope.  Perhaps we learned that the model can't be successful without more data, or different data. Perhaps our evaluation shows us that we should reconsider our approach to cleaning and structuring the data, or how we frame the project as a whole (e.g. realizing we should treat the problem as a classification rather than a regression task). In any of these cases, it is totally encouraged to revisit the earlier steps.  \n",
    "\n",
    "Of course, if the results are satisfactory, then we instead move onto deployment!\n",
    "\n",
    "**_Deployment:_**\n",
    "\n",
    "During this stage, we'll focus on moving our model into production and automating as much as possible. Everything before this serves as a proof-of-concept or an investigation.  If the project has proved successful, then you'll work with stakeholders to determine the best way to implement models and insights.  For example, you might set up an automated ETL (Extract-Transform-Load) pipelines of raw data in order to feed into a database and reformat it so that it is ready for modeling. During the deployment step, you'll actively work to determine the best course of action for getting the results of your project into the wild, and you'll often be involved with building everything needed to put the software into production. \n",
    "\n",
    "This is one of the most rewarding steps of the entire Data Science process -- getting to see your work go live!\n",
    "\n",
    "## Knowledge Discovery in Databases\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_kdd.png\" width=\"800\">\n",
    "\n",
    "**_Knowledge Discovery in Databases_**, or **_KDD_** is considered the oldest Data Science process. The creation of this process is credited to Gregory Piatetsky-Shapiro, who also runs the ever-popular Data Science blog, [kdnuggets](https://www.kdnuggets.com/). If you're interested, read the original white paper on KDD, which can be found [here](https://www.kdnuggets.com/gpspubs/aimag-kdd-overview-1992.pdf)!\n",
    "\n",
    "The KDD process is quite similar to the CRISP-DM process. The diagram above illustrates every step of the KDD process, as well as the expected output at each stage. \n",
    "\n",
    "**_Selection_**:\n",
    "\n",
    "During this stage, you'll focus on selecting your problem, and the data that will help you answer it. This stage works much like the first stage of CRISP-DM -- you begin by focusing on developing an understanding of the domain the problem resides in (e.g. marketing, finance, increasing customer sales, etc), the previous work done in this domain, and the goals of the stakeholders involved with the process.  \n",
    "\n",
    "Once you've developed a strong understanding of the goals and the domain, you'll work to establish where your data is coming from, and which data will be useful to you.  Organizations and companies usually have a ton of data, and only some of it will be relevant to the problem you're trying to solve.  During this stage, you'll focus on examining the data sources available to you and gathering the data that you deem useful for the project.  \n",
    "\n",
    "The output of this stage is the dataset you'll be using for the Data Science project. \n",
    "\n",
    "**_Preprocessing_**:\n",
    "\n",
    "The preprocessing stage is pretty straightforward -- the goal of this stage is to \"clean\" the data by preprocessing it.  For text data, this may include things like tokenization.  You'll also identify and deal with issues like outliers and/or missing data in this stage.  \n",
    "\n",
    "In practice, this stage often blurs with the _Transformation_ stage. \n",
    "\n",
    "The output of this stage is preprocessed data that is more \"clean\" than it was at the start of this stage -- although the dataset is not quite ready for modeling yet. \n",
    "\n",
    "**_Transformation_**:\n",
    "\n",
    "During this stage, you'll take your preprocessed data and transform it in a way that makes it more ideal for modeling.  This may include steps like feature engineering and dimensionality reduction.  At this stage, you'll also deal with things like checking for and removing multicollinearity from the dataset. Categorical data should also be converted to numeric format through one-hot encoding during this step.\n",
    "\n",
    "The output of this stage is a dataset that is now ready for modeling. All null values and outliers are removed, categorical data has been converted to a format that a model can work with, and the dataset is generally ready for experimentation with modeling.  \n",
    "\n",
    "**_Data Mining_**:\n",
    "\n",
    "The Data Mining stage refers to using different modeling techniques to try and build a model that solves the problem we're after -- often, this is a classification or regression task. During this stage, you'll also define your parameters for given models, as well as your overall criteria for measuring the performance of a model.  \n",
    "\n",
    "You may be wondering what Data Mining is, and how it relates to Data Science. In practice, it's just an older term that essentially means the same thing as Data Science. Dr. Piatetsky-Shapiro defines Data Mining as \"the non-trivial extraction of implicit, previously unknown and potentially useful information from data.\"  Making of things such as Machine Learning algorithms to find insights in large datasets that aren't immediately obvious without these algorithms is at the heart of the concept of Data Mining, just as it is in Data Science. In a pragmatic sense, this is why the terms Data Mining and Data Science are typically used interchangeably, although the term Data Mining is considered an older term that isn't used as often nowadays. \n",
    "\n",
    "The output of this stage results from a fit to the data for the problem we're trying to solve.  \n",
    "\n",
    "**_Interpretation/Evaluation_**:\n",
    "\n",
    "During this final stage of KDD, we focus on interpreting the \"patterns\" discovered in the previous step to help us make generalizations or predictions that help us answer our original question. During this stage, you'll consolidate everything you've learned to present it to stakeholders for guiding future actions. Your output may be a presentation that you use to communicate to non-technical managers or executives (never discount the importance of knowing PowerPoint as a Data Scientist!).  Your conclusions for a project may range from \"this approach didn't work\" or \"we need more data about {X}\" to \"this is ready for production, let's build it!\".  \n",
    "\n",
    "## OSEMN\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_osemn.png\" width=\"800\">\n",
    "<a href=\"https://www.kdnuggets.com/2018/02/data-science-command-line-book-exploring-data.html\" target=\"_blank\">Adapted from: KDNuggets</a>\n",
    "\n",
    "This brings us to the Data Science process we'll be using during this section -- OSEMN (sometimes referred as OSEMiN, and pronounced \"OH-sum\", rhymes with \"possum\"). This is the most straightforward of the Data Science processes discussed so far. Note that during this process, just like the others, the stages often blur together. It is completely acceptable (and often a best practice!) to float back and forth between stages as you learn new things about your problem, dataset, requirements, etc.  It's quite common to get to the modeling step and realize that you need to scrub your data a bit more or engineer a different feature and jump back to the \"Scrub\" stage, or go all the way back to the \"Obtain\" stage when you realize your current data isn't sufficient to solve this problem. As with any of these frameworks, OSEMN is meant to be treated more like a set of guidelines for structuring your project than set-in-stone steps that cannot be violated.  \n",
    "\n",
    "**_Obtain_**:\n",
    "\n",
    "As with CRISP-DM and KDD, this step involves understanding stakeholder requirements, gathering information on the problem, and finally, sourcing data that we think will be necessary for solving this problem. \n",
    "\n",
    "**_Scrub_**:\n",
    "\n",
    "During this stage, we'll focus on preprocessing our data.  Important steps such as identifying and removing null values, dealing with outliers, normalizing data, and feature engineering/feature selection are handled around this stage.  The line with this stage really blurs with the _Explore_ stage, as it is common to only realize that certain columns require cleaning or preprocessing as a result of the visualizations and explorations done during Step 3.  \n",
    "\n",
    "Note that although technically, categorical data should be one-hot encoded during this step, in practice, it's usually done after data exploration.  This is because it is much less time-consuming to visualize and explore a few columns containing categorical data than it is to explore many different dummy columns that have been one-hot encoded. \n",
    "\n",
    "**_Explore_**:\n",
    "\n",
    "This step focuses on getting to know the dataset you're working with. As mentioned above, this step tends to blend with the _Scrub_ step mentioned above.  During this step, you'll create visualizations to really get a feel for your dataset.  You'll focus on things such as understanding the distribution of different columns, checking for multicollinearity, and other tasks like that.  If your project is a classification task, you may check the balance of the different classes in your dataset.  If your problem is a regression task, you may check that the dataset meets the assumptions necessary for a regression task.  \n",
    "\n",
    "At the end of this step, you should have a dataset ready for modeling that you've thoroughly explored and are extremely familiar with.  \n",
    "\n",
    "**_Model_**:\n",
    "\n",
    "This step, as with the last two frameworks, is also pretty self-explanatory. It consists of building and tuning models using all the tools you have in your data science toolbox.  In practice, this often means defining a threshold for success, selecting machine learning algorithms to test on the project, and tuning the ones that show promise to try and increase your results.  As with the other stages, it is both common and accepted to realize something, jump back to a previous stage like _Scrub_ or _Explore_, and make some changes to see how it affects the model.  \n",
    "\n",
    "**_Interpret_**:\n",
    "\n",
    "During this step, you'll interpret the results of your model(s), and communicate results to stakeholders.  As with the other frameworks, communication is incredibly important! During this stage, you may come to realize that further investigation is needed, or more data.  That's totally fine -- figure out what's needed, go get it, and start the process over! If your results are satisfactory to all stakeholders involved, you may also go from this stage right into putting your model into production and automating processes necessary to support it.  \n",
    "\n",
    "\n",
    "## A Note On Communicating Results\n",
    "\n",
    "Regardless of the quality of your results, it's very important that you be aware of the business requirements and stakeholder expectations at all times! Generally, no matter which of the above processes you use, you'll communicate your results in a two-pronged manner: \n",
    "\n",
    "- A short, high-level presentation covering your question, process, and results meant for non-technical audiences\n",
    "- A detailed Jupyter Notebook demonstrating your entire process meant for technical audiences\n",
    "\n",
    "In general, you can see why Data Scientists love Jupyter Notebooks! It is very easy to format results in a reproducible, easy-to-understand way.  Although a detailed Jupyter Notebook may seem like the more involved of the two deliverables listed above, the high-level presentation is often the hardest! Just remember -- even if the project took you/your team over a year and utilized the most cutting-edge machine learning techniques available, you still need to be able to communicate your results in about 5 slides (using graphics, not words, whenever possible!), in a 5 minute presentation in a way that someone that can't write code can still understand and be convinced by!\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lesson, you learned about the different data science process frameworks including CRISP-DM, KDD, and OSEMN. You also learned that the data science process is iterative and that a typical data science project involves many different stakeholders who may not have a technical background. As such, it's important to recognize that data scientists must be able to communicate their findings in a non-technical way."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Capstone_RoughDraft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
