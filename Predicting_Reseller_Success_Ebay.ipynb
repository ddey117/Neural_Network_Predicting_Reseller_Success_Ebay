{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NJHL022ptlVx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import xmltodict, json\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "# import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly0Flp1GSKsM"
   },
   "source": [
    "# Constructing A Neural Network To Classify Knives from a Texas Government Surplus Store\n",
    "\n",
    "## Categorizing Nine Different Knives as Profitable or Not Profitable using a CNN for Knife Images and a RNN for Titles from Ebay Listings \n",
    "\n",
    "**Author:** Dylan Dey\n",
    "***\n",
    "\n",
    "## Overview\n",
    "[Texas State Surplus Store](https://www.tfc.texas.gov/divisions/supportserv/prog/statesurplus/)\n",
    "\n",
    "[What happens to all those items that get confiscated by the TSA? Some end up in a Texas store.](https://www.wfaa.com/article/news/local/what-happens-to-all-those-items-that-get-confiscated-by-the-tsa-some-end-up-in-a-texas-store/287-ba80dac3-d91a-4b28-952a-0aaf4f69ff95)\n",
    "\n",
    "![Benchmade Bucket Price Example](images/benchmadePriceBucket2.jpg)\n",
    "\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What are the business's pain points related to this project?\n",
    "* How did you pick the data analysis question(s) that you did?\n",
    "* Why are these questions important from a business perspective?\n",
    "***\n",
    "\n",
    "\n",
    "## Data Understanding\n",
    "\n",
    "Describe the data being used for this project.\n",
    "***\n",
    "Questions to consider:\n",
    "* Where did the data come from, and how do they relate to the data analysis questions?\n",
    "* What do the data represent? Who is in the sample and what variables are included?\n",
    "* What is the target variable?\n",
    "* What are the properties of the variables you intend to use?\n",
    "***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWyJEtLjtAqo"
   },
   "source": [
    "# Data Obtainment: eBay Finding APIVersion 1.13.0  \"http://www.ebay.com/marketplace/search/v1/services\"\n",
    "\n",
    "```\n",
    "import xmltodict, json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"ebay_api_key.env\") #hide production key in environment file\n",
    "\n",
    "\n",
    "\n",
    "APP_ID = os.getenv(\"EBAY_PROD_ID\")\n",
    "```\n",
    "### URL\n",
    "HTTP call url to send the Production Client API. The Payload will modify the url below with a ? followed by an % and key value pairs. This is the most common and recommended way that ebay suggests users to send a request to the Finding API.\n",
    "\n",
    "### HTTP Headers\n",
    "Required headers to return a 200 response code. The top header is AuthnAuth security. As there is no need for a user token and the only interface is with my app, the only security required is for the app. The second row is the call be sent to the Finding API, findItemsAdvanced. ## THIS SHOULD BE A LINK DOG. The third formats the respons as JSON.\n",
    "\n",
    "### Payload\n",
    "Modifies HTTP call. Search for knife in the Collectible-Modern-Factory-Manufactured-Folding-Knives that is of the Used Condition. This request returns 100 pages on page 1.\n",
    "\n",
    "\n",
    "```\n",
    "url = \"https://svcs.ebay.com/services/search/FindingService/v1\" #url fir Finding API\n",
    "\n",
    "\n",
    "headers = {\"X-EBAY-SOA-SECURITY-APPNAME\": APP_ID, #production client_id\n",
    "           \"X-EBAY-SOA-OPERATION-NAME\": \"findItemsAdvanced\", #call_id\n",
    "           \"X-EBAY-API-RESPONSE-ENCODING\": \"JSON\" #format respons as JSON\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "payload = {\"keywords\":\"knife\", #knife in search bar\n",
    "           \"categoryId\":\"48818\", #Collectible-Modern-Factory-Manufactured-Folding-Knives\n",
    "          \"itemFilter.name\":\"Condition\", #item filter, used condition\n",
    "          \"itemFilter.value\":\"Used\",\n",
    "          \"paginationInput.entriesPerPage\":100, #pagination \n",
    "          \"paginationInput.pageNumber\":1\n",
    "          }\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### request section\n",
    "```\n",
    "r = requests.get(url, headers=headers, params=payload)\n",
    "json_dict_100knivesp1 = xmltodict.parse(r.text)\n",
    "knives_data_list_of_dicts = json_dict_100knivesp1['findItemsAdvancedResponse']['searchResult']['item']\n",
    "total_pages = int(json_dict_100knivesp1['findItemsAdvancedResponse']['paginationOutput']['totalPages'])\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xJ2lkpov0BT"
   },
   "source": [
    "### Function For Organizing Call Response\n",
    "```\n",
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "        'viewItemURL', 'postalCode', 'location', \n",
    "        'country', 'shippingInfo', 'sellingStatus', \n",
    "        'listingInfo', 'returnsAccepted', 'condition'\n",
    "       ]\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "        if all(prepared_data.values()):\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GJVsHGWv86E"
   },
   "source": [
    "### Organize Call with Function\n",
    "\n",
    "```\n",
    "prepared_knives = prepare_data(knives_data_list_of_dicts)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hrxkIw-wIuk"
   },
   "source": [
    "### Paginate and append responses to empty list to get full list of data\n",
    "\n",
    "\n",
    "```\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "for page in range(total_pages):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    payload[\"paginationInput.pageNumber\"] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "    response = requests.get(url, headers=headers, params=payload)\n",
    "    \n",
    "    # Get the response body in JSON format\n",
    "    response_json = xmltodict.parse(r.text)\n",
    "    \n",
    "    # Get the list of businesses from the response_json\n",
    "    knives_data_list_of_dicts = response_json['findItemsAdvancedResponse']['searchResult']['item']\n",
    "    \n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(knives_data_list_of_dicts)\n",
    "    \n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "df = pd.DataFrame(full_dataset)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EteiyNxgwIxE"
   },
   "source": [
    "## Pull converted price from nested dictionary\n",
    "\n",
    "```\n",
    "\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    price_list.append(row['sellingStatus']['convertedCurrentPrice']['#text'])\n",
    "    \n",
    "    \n",
    "df[\"price_in_US\"] = price_list\n",
    "\n",
    "\n",
    "df.to_csv(\"full_datase.csv\", index=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwLbGNr0TbIj"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* Were there variables you dropped or created?\n",
    "* How did you address missing values or outliers?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "\n",
    "\n",
    "# here you run your code to clean the data\n",
    "\n",
    "```\n",
    "import code.data_cleaning as dc\n",
    "\n",
    "full_dataset = dc.full_clean()\n",
    "```\n",
    "\n",
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "# here you run your code to model the data\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do in the future to improve this project?\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 26D5-9745\n",
      "\n",
      " Directory of C:\\Users\\12108\\Desktop\\ebay_knife_data\\dsc-5-capstone-project\n",
      "\n",
      "07/05/2022  01:34 PM    <DIR>          .\n",
      "07/05/2022  01:34 PM    <DIR>          ..\n",
      "06/24/2022  04:24 PM                29 .gitignore\n",
      "07/04/2022  12:35 PM    <DIR>          .ipynb_checkpoints\n",
      "06/24/2022  04:24 PM                92 .learn\n",
      "07/02/2022  07:01 PM            23,440 Call that a Knoife.ipynb\n",
      "06/24/2022  04:24 PM             1,849 CONTRIBUTING.md\n",
      "07/05/2022  01:34 PM            44,166 Copy_of_Capstone_RoughDraft.ipynb\n",
      "06/30/2022  02:39 PM    <DIR>          data\n",
      "07/03/2022  03:33 PM             7,535 Ebay Finding API.ipynb\n",
      "06/24/2022  04:24 PM                57 ebay_api_key.env\n",
      "06/24/2022  04:24 PM        49,147,416 full_dataset.csv\n",
      "06/24/2022  04:24 PM         8,873,178 full_dataset.csv2\n",
      "06/24/2022  04:24 PM         9,324,754 full_dataset2.csv\n",
      "06/29/2022  09:18 PM    <DIR>          images\n",
      "06/24/2022  04:24 PM             1,371 LICENSE.md\n",
      "06/27/2022  10:25 AM    <DIR>          preparedData\n",
      "06/24/2022  04:24 PM            12,259 README.md\n",
      "06/30/2022  03:52 PM    <DIR>          surplusStore\n",
      "06/30/2022  02:39 PM    <DIR>          val_counts\n",
      "06/24/2022  04:24 PM                 2 x\n",
      "              13 File(s)     67,436,148 bytes\n",
      "               8 Dir(s)  1,851,424,280,576 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('full_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemId</th>\n",
       "      <th>title</th>\n",
       "      <th>galleryURL</th>\n",
       "      <th>viewItemURL</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>location</th>\n",
       "      <th>country</th>\n",
       "      <th>shippingInfo</th>\n",
       "      <th>sellingStatus</th>\n",
       "      <th>listingInfo</th>\n",
       "      <th>returnsAccepted</th>\n",
       "      <th>condition</th>\n",
       "      <th>price_in_US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>324946521772</td>\n",
       "      <td>Victorinox Classic SD Mini Swiss Army Pocket K...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/smEAAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/Victorinox-Classic-SD...</td>\n",
       "      <td>228**</td>\n",
       "      <td>Harrisonburg,VA,USA</td>\n",
       "      <td>US</td>\n",
       "      <td>OrderedDict([('shippingServiceCost', OrderedDi...</td>\n",
       "      <td>OrderedDict([('currentPrice', OrderedDict([('@...</td>\n",
       "      <td>OrderedDict([('bestOfferEnabled', 'false'), ('...</td>\n",
       "      <td>True</td>\n",
       "      <td>Used</td>\n",
       "      <td>6.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>334474786773</td>\n",
       "      <td>Benchmade TiPika Rare Discontinued Red Class K...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/nbkAAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/Benchmade-TiPika-Rare...</td>\n",
       "      <td>597**</td>\n",
       "      <td>Bozeman,MT,USA</td>\n",
       "      <td>US</td>\n",
       "      <td>OrderedDict([('shippingServiceCost', OrderedDi...</td>\n",
       "      <td>OrderedDict([('currentPrice', OrderedDict([('@...</td>\n",
       "      <td>OrderedDict([('bestOfferEnabled', 'false'), ('...</td>\n",
       "      <td>False</td>\n",
       "      <td>Used</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>233805693414</td>\n",
       "      <td>Victorinox Classic SD Swiss Army Knife 3 Tool ...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/3f4AAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/Victorinox-Classic-SD...</td>\n",
       "      <td>303**</td>\n",
       "      <td>Atlanta,GA,USA</td>\n",
       "      <td>US</td>\n",
       "      <td>OrderedDict([('shippingServiceCost', OrderedDi...</td>\n",
       "      <td>OrderedDict([('currentPrice', OrderedDict([('@...</td>\n",
       "      <td>OrderedDict([('bestOfferEnabled', 'false'), ('...</td>\n",
       "      <td>True</td>\n",
       "      <td>Used</td>\n",
       "      <td>8.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>334475242083</td>\n",
       "      <td>pocket knife</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/ZecAAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/pocket-knife-/3344752...</td>\n",
       "      <td>320**</td>\n",
       "      <td>Fernandina Beach,FL,USA</td>\n",
       "      <td>US</td>\n",
       "      <td>OrderedDict([('shippingServiceCost', OrderedDi...</td>\n",
       "      <td>OrderedDict([('currentPrice', OrderedDict([('@...</td>\n",
       "      <td>OrderedDict([('bestOfferEnabled', 'false'), ('...</td>\n",
       "      <td>False</td>\n",
       "      <td>Used</td>\n",
       "      <td>99.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>275349806592</td>\n",
       "      <td>LOT VINTAGE KNIVES POCKET KNIFE CUSTOM WHITTLE...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/GdIAAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/LOT-VINTAGE-KNIVES-PO...</td>\n",
       "      <td>748**</td>\n",
       "      <td>Tecumseh,OK,USA</td>\n",
       "      <td>US</td>\n",
       "      <td>OrderedDict([('shippingType', 'Calculated'), (...</td>\n",
       "      <td>OrderedDict([('currentPrice', OrderedDict([('@...</td>\n",
       "      <td>OrderedDict([('bestOfferEnabled', 'false'), ('...</td>\n",
       "      <td>True</td>\n",
       "      <td>Used</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         itemId                                              title  \\\n",
       "0  324946521772  Victorinox Classic SD Mini Swiss Army Pocket K...   \n",
       "1  334474786773  Benchmade TiPika Rare Discontinued Red Class K...   \n",
       "2  233805693414  Victorinox Classic SD Swiss Army Knife 3 Tool ...   \n",
       "3  334475242083                                       pocket knife   \n",
       "4  275349806592  LOT VINTAGE KNIVES POCKET KNIFE CUSTOM WHITTLE...   \n",
       "\n",
       "                                          galleryURL  \\\n",
       "0  https://i.ebayimg.com/thumbs/images/g/smEAAOSw...   \n",
       "1  https://i.ebayimg.com/thumbs/images/g/nbkAAOSw...   \n",
       "2  https://i.ebayimg.com/thumbs/images/g/3f4AAOSw...   \n",
       "3  https://i.ebayimg.com/thumbs/images/g/ZecAAOSw...   \n",
       "4  https://i.ebayimg.com/thumbs/images/g/GdIAAOSw...   \n",
       "\n",
       "                                         viewItemURL postalCode  \\\n",
       "0  https://www.ebay.com/itm/Victorinox-Classic-SD...      228**   \n",
       "1  https://www.ebay.com/itm/Benchmade-TiPika-Rare...      597**   \n",
       "2  https://www.ebay.com/itm/Victorinox-Classic-SD...      303**   \n",
       "3  https://www.ebay.com/itm/pocket-knife-/3344752...      320**   \n",
       "4  https://www.ebay.com/itm/LOT-VINTAGE-KNIVES-PO...      748**   \n",
       "\n",
       "                  location country  \\\n",
       "0      Harrisonburg,VA,USA      US   \n",
       "1           Bozeman,MT,USA      US   \n",
       "2           Atlanta,GA,USA      US   \n",
       "3  Fernandina Beach,FL,USA      US   \n",
       "4          Tecumseh,OK,USA      US   \n",
       "\n",
       "                                        shippingInfo  \\\n",
       "0  OrderedDict([('shippingServiceCost', OrderedDi...   \n",
       "1  OrderedDict([('shippingServiceCost', OrderedDi...   \n",
       "2  OrderedDict([('shippingServiceCost', OrderedDi...   \n",
       "3  OrderedDict([('shippingServiceCost', OrderedDi...   \n",
       "4  OrderedDict([('shippingType', 'Calculated'), (...   \n",
       "\n",
       "                                       sellingStatus  \\\n",
       "0  OrderedDict([('currentPrice', OrderedDict([('@...   \n",
       "1  OrderedDict([('currentPrice', OrderedDict([('@...   \n",
       "2  OrderedDict([('currentPrice', OrderedDict([('@...   \n",
       "3  OrderedDict([('currentPrice', OrderedDict([('@...   \n",
       "4  OrderedDict([('currentPrice', OrderedDict([('@...   \n",
       "\n",
       "                                         listingInfo  returnsAccepted  \\\n",
       "0  OrderedDict([('bestOfferEnabled', 'false'), ('...             True   \n",
       "1  OrderedDict([('bestOfferEnabled', 'false'), ('...            False   \n",
       "2  OrderedDict([('bestOfferEnabled', 'false'), ('...             True   \n",
       "3  OrderedDict([('bestOfferEnabled', 'false'), ('...            False   \n",
       "4  OrderedDict([('bestOfferEnabled', 'false'), ('...             True   \n",
       "\n",
       "  condition  price_in_US  \n",
       "0      Used         6.95  \n",
       "1      Used         0.99  \n",
       "2      Used         8.53  \n",
       "3      Used        99.99  \n",
       "4      Used         0.99  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DI_PDFVzOuX_"
   },
   "outputs": [],
   "source": [
    "# #lowercase titles\n",
    "# df1.title = df1.title.apply(str.lower)\n",
    "\n",
    "# #remove special characters\n",
    "# df1.title.apply(pp.remove_special_chars)\n",
    "# #remove special characters\n",
    "# df1.title.apply(pp.remove_special_chars)\n",
    "\n",
    "# test = ' '.join(df1['title'].sample(500))\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQTkWMsUOua0"
   },
   "outputs": [],
   "source": [
    "# #REGEX BRAND PATTERNS AFTERN LOWERCASING TITLES AND REMOVING SPEVIAL CHARACTERS\n",
    "\n",
    "\n",
    "#turn this into a dict bro\n",
    "\n",
    "# benchmade_pattern = \"benchmade\"\n",
    "# buck_pattern = \"buck\"\n",
    "# case_pattern = \"case\"\n",
    "# crkt_pattern = \"crkt\"\n",
    "# kershaw_pattern = \"kershaw\"\n",
    "# leatherman_pattern = \"leatherman\"\n",
    "# sog_pattern = \"sog\"\n",
    "# spyderco_pattern = \"spyderco\"\n",
    "# victorinox_pattern = \"victorinox\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN5Z7ZZ3VfUW"
   },
   "source": [
    "# FUNCTIONIZE THIS BRUH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3-_pAWYVbtA"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# re.findall(kershaw_pattern, test)\n",
    "\n",
    "# test\n",
    "\n",
    "# df1['benchmade'] = df1.title.apply(lambda x: re.findall(benchmade_pattern, x))\n",
    "# df1['buck'] = df1.title.apply(lambda x: re.findall(buck_pattern, x))\n",
    "# df1['case'] = df1.title.apply(lambda x: re.findall(case_pattern, x))\n",
    "# df1['crkt'] = df1.title.apply(lambda x: re.findall(crkt_pattern, x))\n",
    "# df1['kershaw'] = df1.title.apply(lambda x: re.findall(kershaw_pattern, x))\n",
    "# df1['leatherman'] = df1.title.apply(lambda x: re.findall(leatherman_pattern, x))\n",
    "# df1['sog'] = df1.title.apply(lambda x: re.findall(sog_pattern, x))\n",
    "# df1['spyderco'] = df1.title.apply(lambda x: re.findall(spyderco_pattern, x))\n",
    "# df1['victorinox'] = df1.title.apply(lambda x: re.findall(victorinox_pattern, x))\n",
    "\n",
    "# patterns = ['benchmade', 'buck', 'case', 'crkt', 'kershaw', 'leatherman', 'sog', 'spyderco', 'victorinox']\n",
    "\n",
    "# def find_brands(pattern):\n",
    "#     df1[pattern] = df1.title.apply(lambda x: re.findall(pattern, x))\n",
    "#     df1[pattern] = df1[pattern].apply(lambda x: np.nan if len(x)==0 else 1)\n",
    "#     df1[pattern].fillna(0, inplace=True)\n",
    "#     return df1\n",
    "\n",
    "# for pattern in patterns:\n",
    "#     find_brands(pattern)\n",
    "\n",
    "# to_drop = ['country', 'shippingInfo', 'sellingStatus','listingInfo','returnsAccepted', 'location']\n",
    "# df1.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "# display(df1.condition.value_counts())\n",
    "# df1.drop(columns='condition', inplace=True)\n",
    "# df1.info()\n",
    "\n",
    "# print('Benchmade Value Counts')\n",
    "# display(df1.benchmade.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Buck Value Counts')\n",
    "# display(df1.buck.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Case Value Counts')\n",
    "# display(df1.case.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('CRKT Value Counts (Normalized)')\n",
    "# display(df1.crkt.value_counts(normalize=True))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Kershaw Value Counts')\n",
    "# display(df1.kershaw.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Leatherman Value Counts')\n",
    "# display(df1.leatherman.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Sog Value Counts (Normalized)')\n",
    "# display(df1.sog.value_counts(normalize=True))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Spyderco Value Counts')\n",
    "# display(df1.spyderco.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Victorinox Value Counts')\n",
    "# df1.victorinox.value_counts(normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTpEYt6FV261"
   },
   "outputs": [],
   "source": [
    "# df1.drop(columns=['sog', 'crkt'], inplace=True)\n",
    "\n",
    "# df_surplus = df1.loc[(df1['benchmade'] != 0) |\n",
    "#                      (df1['buck'] != 0) | \n",
    "#                      (df1['case'] != 0) | \n",
    "#                      (df1['kershaw'] != 0) | \n",
    "#                      (df1['leatherman'] != 0) | \n",
    "#                      (df1['spyderco'] != 0) | \n",
    "#                      (df1['victorinox'] != 0)].copy()\n",
    "\n",
    "# df_surplus.info()\n",
    "\n",
    "# df_CRKT = pd.read_csv('data/full_dataset_CRKT.csv')\n",
    "# df_SOG = pd.read_csv('data/full_dataset_SOG.csv')\n",
    "\n",
    "# to_drop_CRKT= ['country', 'shippingInfo', 'sellingStatus','listingInfo','returnsAccepted', 'location', 'crkt_cost', 'crkt_bias', 'condition']\n",
    "# to_drop_SOG= ['country', 'shippingInfo', 'sellingStatus','listingInfo','returnsAccepted', 'location', 'sog_cost', 'sog_bias', 'condition']\n",
    "\n",
    "# df_CRKT.drop(columns=to_drop_CRKT, inplace=True)\n",
    "\n",
    "# df_SOG.drop(columns=to_drop_SOG, inplace=True)\n",
    "\n",
    "# df_SOG.info()\n",
    "\n",
    "# df_SOG['sog'] = 1.0\n",
    "# df_SOG['sog'] = 1.0\n",
    "\n",
    "# mkdir surplusStore\n",
    "\n",
    "# df_surplus.to_csv('surplusStore/workingDataFrame.csv', index=False)\n",
    "# df_CRKT.to_csv('surplusStore/df_CRKT.csv', index=False)\n",
    "# df_SOG.to_csv('surplusStore/df_SOG.csv', index=False)\n",
    "\n",
    "# df_surplus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUiDAmQAWck1"
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "\n",
    "\n",
    "# path = r'C:\\Users\\12108\\Desktop\\ebay_knife_data\\dsc-5-capstone-project\\surplusStore'                     # use your path\n",
    "# all_files = glob.glob(os.path.join(path, \"*.csv\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "# df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "# concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "# concatenated_df.head()\n",
    "\n",
    "# concatenated_df.fillna(0, inplace=True)\n",
    "\n",
    "# concatenated_df.info()\n",
    "\n",
    "# concatenated_df.to_csv('surplusStore/workingDataFrame2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvwvPz68W4mU"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "df = pd.read_csv(\"surplusStore/workingDataFrame2.csv\")\n",
    "\n",
    "df.columns\n",
    "\n",
    "costs = {'benchmade': 45,\n",
    "         'buck': 20,\n",
    "         'case': 20,\n",
    "         'crkt': 15,\n",
    "         'kershaw': 15,\n",
    "         'leatherman': 30,\n",
    "         'sog': 15,\n",
    "         'spyderco': 30,\n",
    "         'victorinox': 20,\n",
    "        }\n",
    "\n",
    "df['is_profitable'] = (df['price_in_US'] - df[list(costs.keys())].sum(axis=1))>0\n",
    "\n",
    "df.info()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XptkIA-htqb4"
   },
   "source": [
    "# Data Science Processes\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As discussed, this section is all about synthesizing your skills in order to work through a full Data Science workflow. In this lesson, you'll take a look at some general outlines for how Data Scientists organize their workflow and conceptualize their process.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- List the different data science process frameworks\n",
    "- Compare and contrast popular data science process frameworks such as CRISP-DM, KDD, OSEMN\n",
    "\n",
    "\n",
    "## What is a Data Science Process?\n",
    "\n",
    "Data Science projects are often complex, with many stakeholders, data sources, and goals. Due to this, the Data Science community has created several methodologies for helping organize and structure Data Science Projects.  In this lesson, you'll explore three of the most popular methodologies -- **_CRISP-DM_**, **_KDD_**, and **_OSEMN_**, and explore how you can make use of them to keep your projects well-structured and organized. \n",
    "\n",
    "## CRoss-Industry Standard Process for Data Mining (CRISP-DM)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_crisp-dm.png\" width=\"500\">\n",
    "\n",
    "**_CRISP-DM_** is probably the most popular Data Science process in the Data Science world right now. Take a look at the visualization above to get a feel for CRISP-DM. Notice that CRISP-DM is an iterative process!\n",
    "\n",
    "Let's take a look at the individual steps involved in CRISP-DM.\n",
    "\n",
    "**_Business Understanding:_**  This stage is all about gathering facts and requirements. Who will be using the model you build? How will they be using it? How will this help the goals of the business or organization overall? Data Science projects are complex, with many moving parts and stakeholders. They're also time intensive to complete or modify. Because of this, it is very important that the Data Science team working on the project has a deep understanding of what the problem is, and how the solution will be used. Consider the fact that many stakeholders involved in the project may not have technical backgrounds, and may not even be from the same organization.  Stakeholders from one part of the organization may have wildly different expectations about the project than stakeholders from a different part of the organization -- for instance, the sales team may be under the impression that a recommendation system project is meant to increase sales by recommending upsells to current customers, while the marketing team may be under the impression that the project is meant to help generate new leads by personalizing product recommendations in a marketing email. These are two very different interpretations of a recommendation system project, and it's understandable that both departments would immediately assume that the primary goal of the project is one that helps their organization. As a Data Scientist, it's up to you to clarify the requirements and make sure that everyone involved understands what the project is and isn't. \n",
    "\n",
    "During this stage, the goal is to get everyone on the same page and to provide clarity on the scope of the project for everyone involved, not just the Data Science team. Generate and answer as many contextual questions as you can about the project. \n",
    "\n",
    "Good questions for this stage include:\n",
    "\n",
    "- Who are the stakeholders in this project? Who will be directly affected by the creation of this project?\n",
    "- What business problem(s) will this Data Science project solve for the organization?  \n",
    "- What problems are inside the scope of this project?\n",
    "- What problems are outside the scope of this project?\n",
    "- What data sources are available to us?\n",
    "- What is the expected timeline for this project? Are there hard deadlines (e.g. \"must be live before holiday season shopping\") or is this an ongoing project?\n",
    "- Do stakeholders from different parts of the company or organization all have the exact same understanding about what this project is and isn't?\n",
    "\n",
    "**_Data Understanding:_**\n",
    "\n",
    "Once we have a solid understanding of the business implications for this project, we move on to understanding our data. During this stage, we'll aim to get a solid understanding of the data needed to complete the project.  This step includes both understanding where our data is coming from, as well as the information contained within the data. \n",
    "\n",
    "Consider the following questions when working through this stage:\n",
    "\n",
    "- What data is available to us? Where does it live? Do we have the data, or can we scrape/buy/source the data from somewhere else?\n",
    "- Who controls the data sources, and what steps are needed to get access to the data?\n",
    "- What is our target?\n",
    "- What predictors are available to us?\n",
    "- What data types are the predictors we'll be working with?\n",
    "- What is the distribution of our data?\n",
    "- How many observations does our dataset contain? Do we have a lot of data? Only a little? \n",
    "- Do we have enough data to build a model? Will we need to use resampling methods?\n",
    "- How do we know the data is correct? How is the data collected? Is there a chance the data could be wrong?\n",
    "\n",
    "**_Data Preparation:_**\n",
    "\n",
    "Once we have a strong understanding of our data, we can move onto preparing the data for our modeling steps. \n",
    "\n",
    "During this stage, we'll want to handle the following issues:\n",
    "\n",
    "- Detecting and dealing with missing values\n",
    "- Data type conversions (e.g. numeric data mistakenly encoded as strings)\n",
    "- Checking for and removing multicollinearity (correlated predictors)\n",
    "- Normalizing our numeric data\n",
    "- Converting categorical data to numeric format through one-hot encoding\n",
    "\n",
    "**_Modeling:_**\n",
    "\n",
    "Once we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task. \n",
    "\n",
    "Consider the following questions during the modeling step:\n",
    "\n",
    "- Is this a classification task? A regression task? Something else?\n",
    "- What models will we try?\n",
    "- How do we deal with overfitting?\n",
    "- Do we need to use regularization or not?\n",
    "- What sort of validation strategy will we be using to check that our model works well on unseen data?\n",
    "- What loss functions will we use?\n",
    "- What threshold of performance do we consider as successful?\n",
    "\n",
    "**_Evaluation:_**\n",
    "\n",
    "During this step, we'll evaluate the results of our modeling efforts. Does our model solve the problems that we outlined all the way back during step 1? Why or why not? Often times, evaluating the results of our modeling step will raise new questions, or will cause us to consider changing our approach to the problem.  Notice from the CRISP-DM diagram above, that the \"Evaluation\" step is unique in that it points to both _Business Understanding_ and _Deployment_.  As we mentioned before, Data Science is an iterative process -- that means that given the new information our model has provided, we'll often want to start over with another iteration, armed with our newfound knowledge! Perhaps the results of our model showed us something important that we had originally failed to consider the goal of the project or the scope.  Perhaps we learned that the model can't be successful without more data, or different data. Perhaps our evaluation shows us that we should reconsider our approach to cleaning and structuring the data, or how we frame the project as a whole (e.g. realizing we should treat the problem as a classification rather than a regression task). In any of these cases, it is totally encouraged to revisit the earlier steps.  \n",
    "\n",
    "Of course, if the results are satisfactory, then we instead move onto deployment!\n",
    "\n",
    "**_Deployment:_**\n",
    "\n",
    "During this stage, we'll focus on moving our model into production and automating as much as possible. Everything before this serves as a proof-of-concept or an investigation.  If the project has proved successful, then you'll work with stakeholders to determine the best way to implement models and insights.  For example, you might set up an automated ETL (Extract-Transform-Load) pipelines of raw data in order to feed into a database and reformat it so that it is ready for modeling. During the deployment step, you'll actively work to determine the best course of action for getting the results of your project into the wild, and you'll often be involved with building everything needed to put the software into production. \n",
    "\n",
    "This is one of the most rewarding steps of the entire Data Science process -- getting to see your work go live!\n",
    "\n",
    "## Knowledge Discovery in Databases\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_kdd.png\" width=\"800\">\n",
    "\n",
    "**_Knowledge Discovery in Databases_**, or **_KDD_** is considered the oldest Data Science process. The creation of this process is credited to Gregory Piatetsky-Shapiro, who also runs the ever-popular Data Science blog, [kdnuggets](https://www.kdnuggets.com/). If you're interested, read the original white paper on KDD, which can be found [here](https://www.kdnuggets.com/gpspubs/aimag-kdd-overview-1992.pdf)!\n",
    "\n",
    "The KDD process is quite similar to the CRISP-DM process. The diagram above illustrates every step of the KDD process, as well as the expected output at each stage. \n",
    "\n",
    "**_Selection_**:\n",
    "\n",
    "During this stage, you'll focus on selecting your problem, and the data that will help you answer it. This stage works much like the first stage of CRISP-DM -- you begin by focusing on developing an understanding of the domain the problem resides in (e.g. marketing, finance, increasing customer sales, etc), the previous work done in this domain, and the goals of the stakeholders involved with the process.  \n",
    "\n",
    "Once you've developed a strong understanding of the goals and the domain, you'll work to establish where your data is coming from, and which data will be useful to you.  Organizations and companies usually have a ton of data, and only some of it will be relevant to the problem you're trying to solve.  During this stage, you'll focus on examining the data sources available to you and gathering the data that you deem useful for the project.  \n",
    "\n",
    "The output of this stage is the dataset you'll be using for the Data Science project. \n",
    "\n",
    "**_Preprocessing_**:\n",
    "\n",
    "The preprocessing stage is pretty straightforward -- the goal of this stage is to \"clean\" the data by preprocessing it.  For text data, this may include things like tokenization.  You'll also identify and deal with issues like outliers and/or missing data in this stage.  \n",
    "\n",
    "In practice, this stage often blurs with the _Transformation_ stage. \n",
    "\n",
    "The output of this stage is preprocessed data that is more \"clean\" than it was at the start of this stage -- although the dataset is not quite ready for modeling yet. \n",
    "\n",
    "**_Transformation_**:\n",
    "\n",
    "During this stage, you'll take your preprocessed data and transform it in a way that makes it more ideal for modeling.  This may include steps like feature engineering and dimensionality reduction.  At this stage, you'll also deal with things like checking for and removing multicollinearity from the dataset. Categorical data should also be converted to numeric format through one-hot encoding during this step.\n",
    "\n",
    "The output of this stage is a dataset that is now ready for modeling. All null values and outliers are removed, categorical data has been converted to a format that a model can work with, and the dataset is generally ready for experimentation with modeling.  \n",
    "\n",
    "**_Data Mining_**:\n",
    "\n",
    "The Data Mining stage refers to using different modeling techniques to try and build a model that solves the problem we're after -- often, this is a classification or regression task. During this stage, you'll also define your parameters for given models, as well as your overall criteria for measuring the performance of a model.  \n",
    "\n",
    "You may be wondering what Data Mining is, and how it relates to Data Science. In practice, it's just an older term that essentially means the same thing as Data Science. Dr. Piatetsky-Shapiro defines Data Mining as \"the non-trivial extraction of implicit, previously unknown and potentially useful information from data.\"  Making of things such as Machine Learning algorithms to find insights in large datasets that aren't immediately obvious without these algorithms is at the heart of the concept of Data Mining, just as it is in Data Science. In a pragmatic sense, this is why the terms Data Mining and Data Science are typically used interchangeably, although the term Data Mining is considered an older term that isn't used as often nowadays. \n",
    "\n",
    "The output of this stage results from a fit to the data for the problem we're trying to solve.  \n",
    "\n",
    "**_Interpretation/Evaluation_**:\n",
    "\n",
    "During this final stage of KDD, we focus on interpreting the \"patterns\" discovered in the previous step to help us make generalizations or predictions that help us answer our original question. During this stage, you'll consolidate everything you've learned to present it to stakeholders for guiding future actions. Your output may be a presentation that you use to communicate to non-technical managers or executives (never discount the importance of knowing PowerPoint as a Data Scientist!).  Your conclusions for a project may range from \"this approach didn't work\" or \"we need more data about {X}\" to \"this is ready for production, let's build it!\".  \n",
    "\n",
    "## OSEMN\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_osemn.png\" width=\"800\">\n",
    "<a href=\"https://www.kdnuggets.com/2018/02/data-science-command-line-book-exploring-data.html\" target=\"_blank\">Adapted from: KDNuggets</a>\n",
    "\n",
    "This brings us to the Data Science process we'll be using during this section -- OSEMN (sometimes referred as OSEMiN, and pronounced \"OH-sum\", rhymes with \"possum\"). This is the most straightforward of the Data Science processes discussed so far. Note that during this process, just like the others, the stages often blur together. It is completely acceptable (and often a best practice!) to float back and forth between stages as you learn new things about your problem, dataset, requirements, etc.  It's quite common to get to the modeling step and realize that you need to scrub your data a bit more or engineer a different feature and jump back to the \"Scrub\" stage, or go all the way back to the \"Obtain\" stage when you realize your current data isn't sufficient to solve this problem. As with any of these frameworks, OSEMN is meant to be treated more like a set of guidelines for structuring your project than set-in-stone steps that cannot be violated.  \n",
    "\n",
    "**_Obtain_**:\n",
    "\n",
    "As with CRISP-DM and KDD, this step involves understanding stakeholder requirements, gathering information on the problem, and finally, sourcing data that we think will be necessary for solving this problem. \n",
    "\n",
    "**_Scrub_**:\n",
    "\n",
    "During this stage, we'll focus on preprocessing our data.  Important steps such as identifying and removing null values, dealing with outliers, normalizing data, and feature engineering/feature selection are handled around this stage.  The line with this stage really blurs with the _Explore_ stage, as it is common to only realize that certain columns require cleaning or preprocessing as a result of the visualizations and explorations done during Step 3.  \n",
    "\n",
    "Note that although technically, categorical data should be one-hot encoded during this step, in practice, it's usually done after data exploration.  This is because it is much less time-consuming to visualize and explore a few columns containing categorical data than it is to explore many different dummy columns that have been one-hot encoded. \n",
    "\n",
    "**_Explore_**:\n",
    "\n",
    "This step focuses on getting to know the dataset you're working with. As mentioned above, this step tends to blend with the _Scrub_ step mentioned above.  During this step, you'll create visualizations to really get a feel for your dataset.  You'll focus on things such as understanding the distribution of different columns, checking for multicollinearity, and other tasks like that.  If your project is a classification task, you may check the balance of the different classes in your dataset.  If your problem is a regression task, you may check that the dataset meets the assumptions necessary for a regression task.  \n",
    "\n",
    "At the end of this step, you should have a dataset ready for modeling that you've thoroughly explored and are extremely familiar with.  \n",
    "\n",
    "**_Model_**:\n",
    "\n",
    "This step, as with the last two frameworks, is also pretty self-explanatory. It consists of building and tuning models using all the tools you have in your data science toolbox.  In practice, this often means defining a threshold for success, selecting machine learning algorithms to test on the project, and tuning the ones that show promise to try and increase your results.  As with the other stages, it is both common and accepted to realize something, jump back to a previous stage like _Scrub_ or _Explore_, and make some changes to see how it affects the model.  \n",
    "\n",
    "**_Interpret_**:\n",
    "\n",
    "During this step, you'll interpret the results of your model(s), and communicate results to stakeholders.  As with the other frameworks, communication is incredibly important! During this stage, you may come to realize that further investigation is needed, or more data.  That's totally fine -- figure out what's needed, go get it, and start the process over! If your results are satisfactory to all stakeholders involved, you may also go from this stage right into putting your model into production and automating processes necessary to support it.  \n",
    "\n",
    "\n",
    "## A Note On Communicating Results\n",
    "\n",
    "Regardless of the quality of your results, it's very important that you be aware of the business requirements and stakeholder expectations at all times! Generally, no matter which of the above processes you use, you'll communicate your results in a two-pronged manner: \n",
    "\n",
    "- A short, high-level presentation covering your question, process, and results meant for non-technical audiences\n",
    "- A detailed Jupyter Notebook demonstrating your entire process meant for technical audiences\n",
    "\n",
    "In general, you can see why Data Scientists love Jupyter Notebooks! It is very easy to format results in a reproducible, easy-to-understand way.  Although a detailed Jupyter Notebook may seem like the more involved of the two deliverables listed above, the high-level presentation is often the hardest! Just remember -- even if the project took you/your team over a year and utilized the most cutting-edge machine learning techniques available, you still need to be able to communicate your results in about 5 slides (using graphics, not words, whenever possible!), in a 5 minute presentation in a way that someone that can't write code can still understand and be convinced by!\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lesson, you learned about the different data science process frameworks including CRISP-DM, KDD, and OSEMN. You also learned that the data science process is iterative and that a typical data science project involves many different stakeholders who may not have a technical background. As such, it's important to recognize that data scientists must be able to communicate their findings in a non-technical way."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Capstone_RoughDraft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
