{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly0Flp1GSKsM"
   },
   "source": [
    "\n",
    "# Constructing A Neural Network To Classify Knives from a Texas Government Surplus Store\n",
    "\n",
    "## Categorizing Nine Different Knives as Profitable or Not Profitable using a CNN for Knife Images and a RNN for Titles from Ebay Listings \n",
    "\n",
    "**Author:** Dylan Dey\n",
    "***\n",
    "\n",
    "# Overview\n",
    "[Texas State Surplus Store](https://www.tfc.texas.gov/divisions/supportserv/prog/statesurplus/)\n",
    "\n",
    "[What happens to all those items that get confiscated by the TSA? Some end up in a Texas store.](https://www.wfaa.com/article/news/local/what-happens-to-all-those-items-that-get-confiscated-by-the-tsa-some-end-up-in-a-texas-store/287-ba80dac3-d91a-4b28-952a-0aaf4f69ff95)\n",
    "\n",
    "[Texas Surplus Store PDF](https://www.tfc.texas.gov/divisions/supportserv/prog/statesurplus/State%20Surplus%20Brochure-one%20bar_rev%201-10-2022.pdf)\n",
    "\n",
    "![Texas State Surplus Store](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRYkwyu20VBuQ52PrXdVRaGRIIg9OPXJg86lA&usqp=CAU)\n",
    "\n",
    "![Texas Knives In Stores](https://arc-anglerfish-arc2-prod-bostonglobe.s3.amazonaws.com/public/MWJCCFBSR4I6FCSNKONTFJIRAI.jpg)\n",
    "\n",
    "[Everything that doesn't make it through Texas airports can be found at one Austin store](https://cbsaustin.com/news/local/everything-that-doesnt-make-it-through-texas-airports-can-be-found-at-one-austin-store)\n",
    "\n",
    "\n",
    "> The Texas Facilities Commission collects left behind possessions, salvage, and surplus from Texas state agencies such as DPS, TXDOT, TCEQ, and Texas Parks & Wildlife. Examples of commonly available items include vehicles, furniture, office equipment and supplies, small electronics, and heavy equipment. The goal of this project is to create a Neural Network Classification Model in order to categorize knives available at this store as either profitable or not on ebay. \n",
    "\n",
    "# Business Problem\n",
    "\n",
    "[Family Ebay Store Front](https://www.ebay.com/str/texasdave3?mkcid=16&mkevt=1&mkrid=711-127632-2357-0&ssspo=ZW3G27tGR_m&sssrc=3418065&ssuid=&widget_ver=artemis&media=COPY)\n",
    "\n",
    "![Father's Ebay Account Since 1999](texas_dave.jpg)\n",
    "\n",
    "[Texas Dave's Knives](https://www.ebay.com/str/texasdave3/Knives/_i.html?store_cat=3393246519)\n",
    "\n",
    "\n",
    "> While taking online courses to transition careers during a difficult time of my life, I was also helping my family during a turbulent time for everyone. I have been employed at their retail store in San Antonio for the past several months and have been contributing significantly to their online reselling business on eBay. I would help source newer, cheaper products from Austin to try and resell at the retail store in San Antonio or online to earn some money, support our family business and keep us all afloat. This is how I discovered the Texas Facillities Retail Store. \n",
    "\n",
    "\n",
    "> My family has been running a resale shop and selling on Ebay and other sites for years and lately the business has picked up.  Consumer behavior is shifting:  getting a deal on eBay, or Goodwill, or hitting up a vintage boutique shop to find a unique treasure is now brag worthy.  Plus, people like the idea of sustainability - sending items to landfills is becoming very socially unacceptable – why not repurpose a used item?  With the pandemic related disruption of “normal” business and supply chains and the economic uncertainty of these times there is definitely an upswing in interest in the resale market. \n",
    "\n",
    "> Online sales sites like Ebay offer a worldwide robust buyer base for just about every product regardless of condition. Ebay  allows the reseller to find both  bargain hunters for common items and  enthusiasts searching for rare  collectible items. \n",
    "\n",
    "> An Ebay business has some pain points, however.  Selection of an item to sell is the main pain point. The item should be readily available in decent condition for the seller to purchase at a low price but not so widely available that the market is saturated with that item.  Then there needs to be a demand for the item – it should be something collectible that with appeal to hobbyists that would pay premium prices for hard-to-get items. Alternatively, it would be something useful to a large number of people even in a used condition. The item should be small enough to be easily shipped. It should not be difficult to ship either—that is it should not have hazardous chemicals, batteries etc. that would add costs to the shipping. Additionally, Ebay has strict rules about authentication and certification in many item categories- so obvious “high value” items like jewelry or designer purses are so restricted that it is not  feasible  for the average Ebay seller  to offer them . \n",
    "\n",
    "> This project recommends an item that would answer these concerns – pocket knives, These can be rare and collectible and also practical and useful. There are knife collector forums and subReddits, showing there is an interest among collectors.  A look at eBay listings shows rare knives selling for thousands of \n",
    "dollars each.  Knives are also a handy every day tool –  and based on the number  showing up in the Texas Surplus shop they are easy to lose and so need replacing often. This means there is a market for more common ones as well.  The great thing about single blade, modern, factory manufactured pocketknives is that they all weigh roughly 0.5 lbs making them cheap to ship. For my modeling purposes, it is safe to assume a flat shipping rate of 4.95(US Dollars) including the cost of wholesale purchased padded envelopes. And there are no restrictions on mailing these items and they are not frsgile so no special packaging is needed. \n",
    "\n",
    "> The second pain point is buying at a cost low enough to make a profit. It is not enough to just buy low and sell at a higher price as expenses need to be considered.  Ebay collects insertion fees and final value fees on all sales.  The fees vary with seller level (rating)  and some portions  are a percent of final sale. I have been selling knives from the lower priced bins and the mean seller fee for my sales so far is about 13.5% of the sold price.  So that is a cost to consider right up front. \n",
    "\n",
    "> A third pain point is the cost of excess inventory. A seller can obtain quality items at a reasonable cost and then the inventory may sit with no sales, meaning the capital expended is sitting tied up in unwanted items. This inventory carry cost is a drain on profitability.  This project is meant to help avoid purchasing the wrong items for resale. \n",
    "\n",
    "\n",
    "> As already mentioned, I have been experimenting with low cost used knives for resale but have not risked a large capital investment in the higher end items. The goal of this project is to attempt to address the pain points to determine if a larger investment would pay off. Can I identify which knives are worth investing in so that I can turn a decent profit and hopefully avoid excess inventory? A data driven approach would help avoid costly mistakes from the \"system\" resellers currently employ, which seems to be mainly a gambler’s approach. By managing resources upfront through a model, I can effectively increase my return on investment with messy data such as pictures and titles. The magic of Neural Networks!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> There are nine buckets of presorted brand knives that I was interested in, specifically. The other buckets are full of unbranded knives that usually are crowded with way too many people. These other bins, however, are behind glass, presorted, branded(and therefore have specific characteristics and logos for my model to identify), and priced higher. \n",
    "\n",
    "\n",
    "\n",
    "** knife bucket image ** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Ebay Developer Website](https://developer.ebay.com/)\n",
    "> Ebay has a seperate website for develoers in order to create an account and register an application keyset in order to make API call requests to their live website. By making a findItemsAdvanced call to the eBay Finding APIVersion 1.13.0, I was able to get a large dataset of[category_id=<48818>](https://www.ebay.com/sch/48818/i.html?_from=R40&_nkw=knife) knives.\n",
    "\n",
    "> When you log into Ebay as a buyer and search knife in the search bar, the response that loads outputs  Knives, Swords & Blades. Nested one category furtheris Collectible Folding Knives with an id of 182981. Nested one further is Modern Folding Knives(43333), and then finally, the category_id of most interest, 48818, Factory Manufactured Modern Collectible Folding Knives.\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What are the business's pain points related to this project?\n",
    "* How did you pick the data analysis question(s) that you did?\n",
    "* Why are these questions important from a business perspective?\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Data Understanding\n",
    "\n",
    "Describe the data being used for this project.\n",
    "***\n",
    "Questions to consider:\n",
    "* Where did the data come from, and how do they relate to the data analysis questions?\n",
    "* What do the data represent? Who is in the sample and what variables are included?\n",
    "* What is the target variable?\n",
    "* What are the properties of the variables you intend to use?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Obtainment \n",
    "## 'Ebay FindingService',  '1.12.0', 'findItemsAdvanced', 'eBaySDK/2.2.0 Python/3.8.5 Windows/10'\n",
    "\n",
    "[Ebay suggested  SDKs on ebay developer website](https://developer.ebay.com/develop/ebay-sdks)\n",
    "\n",
    "[Python SDK to simplify making calls](https://github.com/timotheus/ebaysdk-python/wiki/Trading-API-Class)\n",
    "\n",
    "[eBay Finding APIVersion 1.13.0 call index](https://developer.ebay.com/devzone/finding/CallRef/index.html)\n",
    "\n",
    "[finditemsAdvanced Call Reference](https://developer.ebay.com/devzone/finding/CallRef/findItemsAdvanced.html)\n",
    "\n",
    "\n",
    "\n",
    "The Ebay developer website suggests using an SDK in order to make a call to their APIs. I decided to git clone [the Python SDK to simplify making calls](https://github.com/timotheus/ebaysdk-python/wiki/Trading-API-Class) and used the .yaml file from the github repository in order to store all of my necessary developer/security keys. Please feel free to read through the documentation in the github and the documentation in the API reference to see what all is available using this SDK and API.\n",
    "\n",
    "Unfortunately, the API limits you to 100 pages and 100 entries MAX. So even if I tried to loop to page 101 to grab just one more entry, ebay will throw an error to the connection. However, 10,000 items seems like a reasonable amount of data for this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test cell Below Sends a findItemsAdvancedRequest call to the ebay traditional (non-RESTful) finding api. If I were browsing on Ebay's website, I would be doing the following on the webpage:\n",
    "- Typing the keywords 'knife' in the search bar\n",
    "- filter for only used knives using the navigation box \n",
    "- filter for only fixed price buy type (no auctions) using the navigation box\n",
    "- filter for only a select few brands that I care about by placing check marks in appropriate boxes\n",
    "- set the max items on my page to 10 and scroll all the way down and gawk at all the pretty knives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRUB/EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ebaysdk.finding import Connection\n",
    "import pandas as pd \n",
    "import  json\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "# import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_iqr_filter(df):\n",
    "    \n",
    "    price_Q1 = df['converted_price'].quantile(0.25)\n",
    "    price_Q3 = df['converted_price'].quantile(0.75)\n",
    "    price_iqr = price_Q3 - price_Q1\n",
    "\n",
    "    profit_Q1 = df['profit'].quantile(0.25)\n",
    "    profit_Q3 = df['profit'].quantile(0.75)\n",
    "    profit_iqr = profit_Q3 - profit_Q1\n",
    "\n",
    "    ROI_Q1 = df['ROI'].quantile(0.25)\n",
    "    ROI_Q3 = df['ROI'].quantile(0.75)\n",
    "    ROI_iqr = ROI_Q3 - ROI_Q1\n",
    "\n",
    "    price_upper_limit = price_Q3 + (1.5 * price_iqr)\n",
    "    price_lower_limit = price_Q1 - (1.5 * price_iqr)\n",
    "\n",
    "    profit_upper_limit = profit_Q3 + (1.5 * profit_iqr)\n",
    "    profit_lower_limit = profit_Q1 - (1.5 * profit_iqr)\n",
    "\n",
    "    ROI_upper_limit = ROI_Q3 + (1.5 * ROI_iqr)\n",
    "    ROI_lower_limit = ROI_Q1 - (1.5 * ROI_iqr)\n",
    "    \n",
    "#     print(f'Brand: {df.brand[0]}')\n",
    "#     print(f'price upper limit: ${np.round(price_upper_limit,2)}')\n",
    "#     print(f'price lower limit: ${np.round(price_lower_limit,2)}')\n",
    "#     print('-----------------------------------')\n",
    "#     print(f'profit upper limit: ${np.round(profit_upper_limit,2)}')\n",
    "#     print(f'profit lower limit: ${np.round(profit_lower_limit,2)}')\n",
    "#     print('-----------------------------------')\n",
    "#     print(f'ROI upper limit: {np.round(ROI_upper_limit,2)}%')\n",
    "#     print(f'ROI lower limit: {np.round(ROI_lower_limit,2)}%')\n",
    "#     print('-----------------------------------')\n",
    "\n",
    "    \n",
    "    new_df = df[(df['converted_price'] < price_upper_limit) &\n",
    "                (df['converted_price'] > price_lower_limit) &\n",
    "                (df['profit'] <profit_upper_limit) &\n",
    "                (df['ROI'] < ROI_upper_limit) &\n",
    "                (df['profit'] < profit_upper_limit) &\n",
    "                (df['ROI'] > ROI_lower_limit)]\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    condition = None\n",
    "    for row in full_dataset:\n",
    "        listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "        price_list.append(listed_price)\n",
    "     \n",
    "        try:\n",
    "            listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "\n",
    "        try:\n",
    "            condition = float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0,\n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "overhead_cost = 3\n",
    "def prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    " \n",
    "    #remove special characters\n",
    "#     df.title.apply(pp.remove_special_chars)\n",
    "    df['brand'] = str(list(bucket_dict.keys())[bucket_dict_position])\n",
    "    df['cost'] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    df['profit'] = (df['converted_price'] -  df['cost'] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/( df['cost'] + overhead_cost))*100.0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teradf_bench.to_csv(\"data/tera_bench_prepared.csv\", index=False)\n",
    "# teradf_buck.to_csv(\"data/tera_buck_prepared.csv\", index=False)\n",
    "# teradf_case.to_csv(\"data/tera_case_prepared.csv\", index=False)\n",
    "# teradf_crkt.to_csv(\"data/tera_crkt_prepared.csv\", index=False)\n",
    "# teradf_kershaw.to_csv(\"data/tera_kershaw_prepared.csv\", index=False)\n",
    "# teradf_leath.to_csv(\"data/tera_leatherman_prepared.csv\", index=False)\n",
    "# teradf_sog.to_csv(\"data/tera_sog_prepared.csv\", index=False)\n",
    "# teradf_spyd.to_csv(\"data/tera_bench2.csv\", index=False)\n",
    "# teradf_vict.to_csv(\"data/tera_victorinox_prepared.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teradf_buck['title'].fillna(teradf_buck['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teradf_buck.drop('title2', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teradf_case['title'].fillna(teradf_case['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teradf_case.drop(['title2', \n",
    "#                   'Price', \n",
    "#                   'units_sold'], \n",
    "#                   axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench2.to_csv('data/tera_benchmade_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.rename({'Field2':'date_sold', \n",
    "#                  'title1': 'title', \n",
    "#                  'avg_cost':'avg_price'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2['title'].fillna(df_crkt2['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.drop(['Field', 'Price', 'title2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.to_csv('data/tera_crkt_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kershaw2.rename({'Field':'url', 'Field3':'date_sold', 'Field1': 'title'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kershaw2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kershaw2.drop(['Field2', 'Price', 'url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kershaw2.to_csv('data/tera_kershaw_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2['title'].fillna(df_leatherman2['title1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.rename({'Field2':'date_sold'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.drop(['Field', 'Price', 'title1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.to_csv('data/tera_leatherman_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog.rename({'title1': 'title'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog['title'].fillna(df_sog['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog.drop(['Field', 'Price', 'title2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog.to_csv('data/tera_sog_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.rename({'Field3': 'title', 'Field2': 'date_sold'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2['title'].fillna(df_spyderco2['name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.drop(['units_sold', 'Price', 'name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.to_csv('data/tera_spyderco_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.rename({'FfImage':'Image', 'Field3':'date_sold'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2['title'].fillna(df_victorinox2['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.drop(['url','Field2', 'Price', 'title2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.to_csv('data/tera_victorinox_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench['title'] = teradf_bench['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"(, preview full size imagebenchmade)\")\n",
    "teradf_bench['title'] = teradf_bench['title'].apply(lambda x: re.sub(pattern, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = pd.read_csv(\"data/df_bench1.csv\")\n",
    "df_buck = pd.read_csv(\"data/df_buck.csv\")\n",
    "df_case = pd.read_csv(\"data/df_case.csv\")\n",
    "df_caseXX = pd.read_csv(\"data/df_CaseXX.csv\")\n",
    "df_crkt = pd.read_csv(\"data/df_crkt.csv\")\n",
    "df_kershaw = pd.read_csv(\"data/df_kershaw.csv\")\n",
    "df_leath = pd.read_csv(\"data/df_leatherman.csv\")\n",
    "df_sog = pd.read_csv(\"data/df_sog.csv\")\n",
    "df_spyd = pd.read_csv(\"data/df_spyderco.csv\")\n",
    "df_vict = pd.read_csv(\"data/df_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df = pd.concat([df_case,df_caseXX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://i.ebayimg.com/thumbs/images/g/DDoAAOSw~41jhp-z/s-l140.jpg\n",
      "https://i.ebayimg.com/thumbs/images/g/jrIAAOSwA75fIHXW/s-l140.jpg\n",
      "https://i.ebayimg.com/thumbs/images/g/5XsAAOSws9JinPU7/s-l140.jpg\n",
      "https://i.ebayimg.com/thumbs/images/g/SdEAAOSw7lBi2QdR/s-l140.jpg\n",
      "https://i.ebayimg.com/thumbs/images/g/xWYAAOSwLnNjJYFD/s-l140.jpg\n",
      "https://i.ebayimg.com/thumbs/images/g/n3wAAOSwf6thpNm7/s-l140.jpg\n",
      "https://i.ebayimg.com/thumbs/images/g/exEAAOSwlZNjhV~U/s-l140.jpg\n",
      "https://i.ebayimg.com/thumbs/images/g/iwEAAOSwh05je~fY/s-l140.jpg\n",
      "https://i.ebayimg.com/thumbs/images/g/oJQAAOSwNLFi0bqp/s-l140.jpg\n",
      "https://i.ebayimg.com/thumbs/images/g/ClcAAOSwTN9hQR92/s-l140.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "779     None\n",
       "688     None\n",
       "1217    None\n",
       "793     None\n",
       "2516    None\n",
       "2308    None\n",
       "964     None\n",
       "645     None\n",
       "321     None\n",
       "2317    None\n",
       "Name: galleryURL, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_buck['galleryURL'].sample(10).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.concat([teradf_bench, df_bench])\n",
    "buck = pd.concat([teradf_buck, df_buck])\n",
    "case = pd.concat([teradf_case, case_df])\n",
    "crkt = pd.concat([teradf_crkt, df_crkt])\n",
    "kershaw = pd.concat([teradf_kershaw, df_kershaw])\n",
    "leath = pd.concat([teradf_leath, df_leath])\n",
    "sog = pd.concat([teradf_sog, df_sog])\n",
    "spyd = pd.concat([teradf_spyd, df_spyd])\n",
    "vict = pd.concat([teradf_vict, df_vict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kershaw['title'] = kershaw['title'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaner(df):\n",
    "    lot = re.compile('(?<!-\\S)lot(?![^\\s.,:?!])')\n",
    "    disp = re.compile('(display)')\n",
    "    box = re.compile('(box)')\n",
    "    group = re.compile('(group)')\n",
    "    is_set = re.compile('(?<!-\\S)set(?![^\\s.,?!])')\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    trim_list = [lot,disp,box,group,is_set]\n",
    "    for item in trim_list:\n",
    "        df.loc[df['title'].apply(lambda x: re.search(item, x)).isna() != True, 'trim'] = 1 \n",
    "    to_drop = df.loc[df['trim'] == 1].index\n",
    "    df.drop(to_drop, inplace=True)\n",
    "    df.drop('trim', axis=1, inplace=True)\n",
    "    \n",
    "#     df = apply_iqr_filter(df)\n",
    "    return df\n",
    "    \n",
    "bench = data_cleaner(bench)\n",
    "buck = data_cleaner(buck)\n",
    "case = data_cleaner(case)\n",
    "crkt = data_cleaner(crkt)\n",
    "kershaw = data_cleaner(kershaw)\n",
    "leath = data_cleaner(leath)\n",
    "sog = data_cleaner(sog)\n",
    "spyd = data_cleaner(spyd)\n",
    "vict = data_cleaner(vict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bench_used = df_bench.loc[df_bench['condition'] != 1000.0].copy()\n",
    "# buck_used = df_buck.loc[df_buck['condition'] != 1000.0].copy()\n",
    "# case_used = df_case.loc[df_case['condition'] != 1000.0].copy()\n",
    "# crkt_used = df_crkt.loc[df_crkt['condition'] != 1000.0].copy()\n",
    "# kershaw_used = df_kershaw.loc[df_kershaw['condition'] != 1000.0].copy()\n",
    "# leath_used = df_leath.loc[df_leath['condition'] != 1000.0].copy()\n",
    "# sog_used = df_sog.loc[df_sog['condition'] != 1000.0].copy()\n",
    "# spyd_used = df_spyd.loc[df_spyd['condition'] != 1000.0].copy()\n",
    "# vict_used = df_vict.loc[df_vict['condition'] != 1000.0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buck_used = data_cleaner(buck_used)\n",
    "# bench_used = data_cleaner(bench_used)\n",
    "# case_used = data_cleaner(case_used)\n",
    "# crkt_used = data_cleaner(crkt_used)\n",
    "# kershaw_used = data_cleaner(kershaw_used)\n",
    "# leath_used = data_cleaner(leath_used)\n",
    "# sog_used = data_cleaner(sog_used)\n",
    "# spyd_used = data_cleaner(spyd_used)\n",
    "# vict_used = data_cleaner(vict_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_buck = data_cleaner(df_buck)\n",
    "# df_bench = data_cleaner(df_bench)\n",
    "# df_case = data_cleaner(df_case)\n",
    "# df_crkt = data_cleaner(df_crkt)\n",
    "# df_kershaw = data_cleaner(df_kershaw)\n",
    "# df_leath = data_cleaner(df_leath)\n",
    "# df_sog = data_cleaner(df_sog)\n",
    "# df_spyd = data_cleaner(df_spyd)\n",
    "# df_vict = data_cleaner(df_vict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.reset_index(drop=True, inplace=True)\n",
    "buck.reset_index(drop=True, inplace=True)\n",
    "case.reset_index(drop=True, inplace=True)\n",
    "crkt.reset_index(drop=True, inplace=True)\n",
    "kershaw.reset_index(drop=True, inplace=True)\n",
    "leath.reset_index(drop=True, inplace=True)\n",
    "sog.reset_index(drop=True, inplace=True)\n",
    "spyd.reset_index(drop=True, inplace=True)\n",
    "vict.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date_sold</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>shipping_cost</th>\n",
       "      <th>converted_price</th>\n",
       "      <th>profit</th>\n",
       "      <th>ROI</th>\n",
       "      <th>brand</th>\n",
       "      <th>...</th>\n",
       "      <th>viewItemURL</th>\n",
       "      <th>autoPay</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>sellingStatus</th>\n",
       "      <th>shippingInfo</th>\n",
       "      <th>listingInfo</th>\n",
       "      <th>returnsAccepted</th>\n",
       "      <th>condition</th>\n",
       "      <th>topRatedListing</th>\n",
       "      <th>galleryPlusPictureURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8248</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mike adamich’s knife. update in description if...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.01</td>\n",
       "      <td>9977.01</td>\n",
       "      <td>43378.304348</td>\n",
       "      <td>case</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/Mike-Adamich-s-Knife-...</td>\n",
       "      <td>False</td>\n",
       "      <td>958**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingType': 'Calculated', 'shipToLocation...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>True</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9718</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>johnny cash antique bone gold record commemora...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1499.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1499.99</td>\n",
       "      <td>1476.99</td>\n",
       "      <td>6421.695652</td>\n",
       "      <td>case</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/Johnny-Cash-Antique-B...</td>\n",
       "      <td>False</td>\n",
       "      <td>784**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'true', 'buyItNowAvailabl...</td>\n",
       "      <td>False</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>https://galleryplus.ebayimg.com/ws/web/1321638...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10033</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case cherokee heritage 6 piece knife collectio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1228.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1228.20</td>\n",
       "      <td>1205.20</td>\n",
       "      <td>5240.000000</td>\n",
       "      <td>case</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/Case-Cherokee-Heritag...</td>\n",
       "      <td>True</td>\n",
       "      <td>373**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'true', 'buyItNowAvailabl...</td>\n",
       "      <td>True</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>https://galleryplus.ebayimg.com/ws/web/2028999...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Image  url                                              title date_sold  \\\n",
       "8248    NaN  NaN  mike adamich’s knife. update in description if...       NaN   \n",
       "9718    NaN  NaN  johnny cash antique bone gold record commemora...       NaN   \n",
       "10033   NaN  NaN  case cherokee heritage 6 piece knife collectio...       NaN   \n",
       "\n",
       "       price_in_US  shipping_cost  converted_price   profit           ROI  \\\n",
       "8248      10000.01            0.0         10000.01  9977.01  43378.304348   \n",
       "9718       1499.99            0.0          1499.99  1476.99   6421.695652   \n",
       "10033      1228.20            0.0          1228.20  1205.20   5240.000000   \n",
       "\n",
       "      brand  ...                                        viewItemURL  autoPay  \\\n",
       "8248   case  ...  https://www.ebay.com/itm/Mike-Adamich-s-Knife-...    False   \n",
       "9718   case  ...  https://www.ebay.com/itm/Johnny-Cash-Antique-B...    False   \n",
       "10033  case  ...  https://www.ebay.com/itm/Case-Cherokee-Heritag...     True   \n",
       "\n",
       "      postalCode                                      sellingStatus  \\\n",
       "8248       958**  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "9718       784**  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "10033      373**  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "\n",
       "                                            shippingInfo  \\\n",
       "8248   {'shippingType': 'Calculated', 'shipToLocation...   \n",
       "9718   {'shippingServiceCost': {'_currencyId': 'USD',...   \n",
       "10033  {'shippingServiceCost': {'_currencyId': 'USD',...   \n",
       "\n",
       "                                             listingInfo returnsAccepted  \\\n",
       "8248   {'bestOfferEnabled': 'false', 'buyItNowAvailab...            True   \n",
       "9718   {'bestOfferEnabled': 'true', 'buyItNowAvailabl...           False   \n",
       "10033  {'bestOfferEnabled': 'true', 'buyItNowAvailabl...            True   \n",
       "\n",
       "      condition topRatedListing  \\\n",
       "8248     3000.0           False   \n",
       "9718     1000.0           False   \n",
       "10033    1000.0           False   \n",
       "\n",
       "                                   galleryPlusPictureURL  \n",
       "8248                                                 NaN  \n",
       "9718   https://galleryplus.ebayimg.com/ws/web/1321638...  \n",
       "10033  https://galleryplus.ebayimg.com/ws/web/2028999...  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(case.loc[case['converted_price'] > 1000][3:6])\n",
    "to_drop = case.loc[case['converted_price'] > 1000][3:6].index\n",
    "case.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>title</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>shipping_cost</th>\n",
       "      <th>url</th>\n",
       "      <th>converted_price</th>\n",
       "      <th>profit</th>\n",
       "      <th>ROI</th>\n",
       "      <th>brand</th>\n",
       "      <th>cost</th>\n",
       "      <th>...</th>\n",
       "      <th>viewItemURL</th>\n",
       "      <th>autoPay</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>sellingStatus</th>\n",
       "      <th>shippingInfo</th>\n",
       "      <th>listingInfo</th>\n",
       "      <th>returnsAccepted</th>\n",
       "      <th>condition</th>\n",
       "      <th>topRatedListing</th>\n",
       "      <th>galleryPlusPictureURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>https://thumbs.ebaystatic.com/pict/20379295275...</td>\n",
       "      <td>buck knife 110 1st version 1965 w/buck etched ...</td>\n",
       "      <td>1254.10</td>\n",
       "      <td>9.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1263.10</td>\n",
       "      <td>1243.10</td>\n",
       "      <td>6215.500000</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9467</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rare custom buck knife 186 titanium dual blade...</td>\n",
       "      <td>1795.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1795.00</td>\n",
       "      <td>1772.00</td>\n",
       "      <td>7704.347826</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/RARE-CUSTOM-BUCK-KNIF...</td>\n",
       "      <td>False</td>\n",
       "      <td>064**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>True</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>https://galleryplus.ebayimg.com/ws/web/1851518...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9771</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rare custom buck knife 186 titanium dual blade...</td>\n",
       "      <td>1195.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1195.00</td>\n",
       "      <td>1172.00</td>\n",
       "      <td>5095.652174</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/RARE-CUSTOM-BUCK-KNIF...</td>\n",
       "      <td>False</td>\n",
       "      <td>064**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>True</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>https://galleryplus.ebayimg.com/ws/web/1851519...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>NaN</td>\n",
       "      <td>buck custom usa folding hunter knife special s...</td>\n",
       "      <td>999.99</td>\n",
       "      <td>6.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1006.98</td>\n",
       "      <td>983.98</td>\n",
       "      <td>4278.173913</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/Buck-Custom-USA-Foldi...</td>\n",
       "      <td>False</td>\n",
       "      <td>077**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'true', 'buyItNowAvailabl...</td>\n",
       "      <td>True</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10936</th>\n",
       "      <td>NaN</td>\n",
       "      <td>buck knife collectible.</td>\n",
       "      <td>1350.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1350.00</td>\n",
       "      <td>1327.00</td>\n",
       "      <td>5769.565217</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/Buck-knife-Collectibl...</td>\n",
       "      <td>True</td>\n",
       "      <td>838**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingType': 'Calculated', 'shipToLocation...</td>\n",
       "      <td>{'bestOfferEnabled': 'true', 'buyItNowAvailabl...</td>\n",
       "      <td>False</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11054</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23 collecter knives some camillus , buck, gerb...</td>\n",
       "      <td>975.00</td>\n",
       "      <td>118.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1093.07</td>\n",
       "      <td>1070.07</td>\n",
       "      <td>4652.478261</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/23-collecter-knives-s...</td>\n",
       "      <td>True</td>\n",
       "      <td>149**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>False</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11098</th>\n",
       "      <td>NaN</td>\n",
       "      <td>23 collector knives some camillus buck gerber ...</td>\n",
       "      <td>975.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1050.00</td>\n",
       "      <td>1027.00</td>\n",
       "      <td>4465.217391</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/23-collector-knives-s...</td>\n",
       "      <td>True</td>\n",
       "      <td>149**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>False</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Image  \\\n",
       "4997   https://thumbs.ebaystatic.com/pict/20379295275...   \n",
       "9467                                                 NaN   \n",
       "9771                                                 NaN   \n",
       "10009                                                NaN   \n",
       "10936                                                NaN   \n",
       "11054                                                NaN   \n",
       "11098                                                NaN   \n",
       "\n",
       "                                                   title  price_in_US  \\\n",
       "4997   buck knife 110 1st version 1965 w/buck etched ...      1254.10   \n",
       "9467   rare custom buck knife 186 titanium dual blade...      1795.00   \n",
       "9771   rare custom buck knife 186 titanium dual blade...      1195.00   \n",
       "10009  buck custom usa folding hunter knife special s...       999.99   \n",
       "10936                            buck knife collectible.      1350.00   \n",
       "11054  23 collecter knives some camillus , buck, gerb...       975.00   \n",
       "11098  23 collector knives some camillus buck gerber ...       975.00   \n",
       "\n",
       "       shipping_cost  url  converted_price   profit          ROI brand  cost  \\\n",
       "4997            9.00  NaN          1263.10  1243.10  6215.500000  buck  20.0   \n",
       "9467            0.00  NaN          1795.00  1772.00  7704.347826  buck  20.0   \n",
       "9771            0.00  NaN          1195.00  1172.00  5095.652174  buck  20.0   \n",
       "10009           6.99  NaN          1006.98   983.98  4278.173913  buck  20.0   \n",
       "10936           0.00  NaN          1350.00  1327.00  5769.565217  buck  20.0   \n",
       "11054         118.07  NaN          1093.07  1070.07  4652.478261  buck  20.0   \n",
       "11098          75.00  NaN          1050.00  1027.00  4465.217391  buck  20.0   \n",
       "\n",
       "       ...                                        viewItemURL autoPay  \\\n",
       "4997   ...                                                NaN     NaN   \n",
       "9467   ...  https://www.ebay.com/itm/RARE-CUSTOM-BUCK-KNIF...   False   \n",
       "9771   ...  https://www.ebay.com/itm/RARE-CUSTOM-BUCK-KNIF...   False   \n",
       "10009  ...  https://www.ebay.com/itm/Buck-Custom-USA-Foldi...   False   \n",
       "10936  ...  https://www.ebay.com/itm/Buck-knife-Collectibl...    True   \n",
       "11054  ...  https://www.ebay.com/itm/23-collecter-knives-s...    True   \n",
       "11098  ...  https://www.ebay.com/itm/23-collector-knives-s...    True   \n",
       "\n",
       "      postalCode                                      sellingStatus  \\\n",
       "4997         NaN                                                NaN   \n",
       "9467       064**  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "9771       064**  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "10009      077**  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "10936      838**  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "11054      149**  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "11098      149**  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "\n",
       "                                            shippingInfo  \\\n",
       "4997                                                 NaN   \n",
       "9467   {'shippingServiceCost': {'_currencyId': 'USD',...   \n",
       "9771   {'shippingServiceCost': {'_currencyId': 'USD',...   \n",
       "10009  {'shippingServiceCost': {'_currencyId': 'USD',...   \n",
       "10936  {'shippingType': 'Calculated', 'shipToLocation...   \n",
       "11054  {'shippingServiceCost': {'_currencyId': 'USD',...   \n",
       "11098  {'shippingServiceCost': {'_currencyId': 'USD',...   \n",
       "\n",
       "                                             listingInfo returnsAccepted  \\\n",
       "4997                                                 NaN             NaN   \n",
       "9467   {'bestOfferEnabled': 'false', 'buyItNowAvailab...            True   \n",
       "9771   {'bestOfferEnabled': 'false', 'buyItNowAvailab...            True   \n",
       "10009  {'bestOfferEnabled': 'true', 'buyItNowAvailabl...            True   \n",
       "10936  {'bestOfferEnabled': 'true', 'buyItNowAvailabl...           False   \n",
       "11054  {'bestOfferEnabled': 'false', 'buyItNowAvailab...           False   \n",
       "11098  {'bestOfferEnabled': 'false', 'buyItNowAvailab...           False   \n",
       "\n",
       "      condition topRatedListing  \\\n",
       "4997        NaN             NaN   \n",
       "9467     1000.0           False   \n",
       "9771     1000.0           False   \n",
       "10009    3000.0           False   \n",
       "10936    1000.0           False   \n",
       "11054    3000.0           False   \n",
       "11098    3000.0           False   \n",
       "\n",
       "                                   galleryPlusPictureURL  \n",
       "4997                                                 NaN  \n",
       "9467   https://galleryplus.ebayimg.com/ws/web/1851518...  \n",
       "9771   https://galleryplus.ebayimg.com/ws/web/1851519...  \n",
       "10009                                                NaN  \n",
       "10936                                                NaN  \n",
       "11054                                                NaN  \n",
       "11098                                                NaN  \n",
       "\n",
       "[7 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(buck.loc[buck['converted_price'] > 1000])\n",
    "to_drop = buck.loc[buck['converted_price'] > 1000][5:7].index\n",
    "buck.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image                                                                  NaN\n",
       "url                                                                    NaN\n",
       "title                    kershaw 1735 zing three (3) knives - tanto, or...\n",
       "date_sold                                                              NaN\n",
       "price_in_US                                                            500\n",
       "shipping_cost                                                           45\n",
       "converted_price                                                        545\n",
       "profit                                                                 527\n",
       "ROI                                                                2927.78\n",
       "brand                                                              kershaw\n",
       "cost                                                                    15\n",
       "itemId                                                         2.24903e+11\n",
       "galleryURL               https://i.ebayimg.com/thumbs/images/g/iRAAAOSw...\n",
       "viewItemURL              https://www.ebay.com/itm/Kershaw-1735-Zing-Thr...\n",
       "autoPay                                                               True\n",
       "postalCode                                                           805**\n",
       "sellingStatus            {'currentPrice': {'_currencyId': 'USD', 'value...\n",
       "shippingInfo             {'shippingServiceCost': {'_currencyId': 'USD',...\n",
       "listingInfo              {'bestOfferEnabled': 'true', 'buyItNowAvailabl...\n",
       "returnsAccepted                                                      False\n",
       "condition                                                             1000\n",
       "topRatedListing                                                      False\n",
       "galleryPlusPictureURL                                                  NaN\n",
       "Name: 10975, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(kershaw.iloc[10975])\n",
    "kershaw.drop(10975, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165.88199999999995"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buck['converted_price'].quantile(.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>title</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>shipping_cost</th>\n",
       "      <th>url</th>\n",
       "      <th>converted_price</th>\n",
       "      <th>profit</th>\n",
       "      <th>ROI</th>\n",
       "      <th>brand</th>\n",
       "      <th>cost</th>\n",
       "      <th>...</th>\n",
       "      <th>viewItemURL</th>\n",
       "      <th>autoPay</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>sellingStatus</th>\n",
       "      <th>shippingInfo</th>\n",
       "      <th>listingInfo</th>\n",
       "      <th>returnsAccepted</th>\n",
       "      <th>condition</th>\n",
       "      <th>topRatedListing</th>\n",
       "      <th>galleryPlusPictureURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/TisA...</td>\n",
       "      <td>buck 889 strider pocket black 420hc knife</td>\n",
       "      <td>450.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>https://www.ebay.com/itm/125407444151?nordt=tr...</td>\n",
       "      <td>480.00</td>\n",
       "      <td>460.00</td>\n",
       "      <td>2300.000000</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>https://thumbs.ebaystatic.com/pict/14432053875...</td>\n",
       "      <td>buck 110 usa knife w/custom handle</td>\n",
       "      <td>175.00</td>\n",
       "      <td>5.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.95</td>\n",
       "      <td>160.95</td>\n",
       "      <td>804.750000</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>https://thumbs.ebaystatic.com/pict/17503100883...</td>\n",
       "      <td>knifes</td>\n",
       "      <td>180.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.00</td>\n",
       "      <td>160.00</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>https://thumbs.ebaystatic.com/pict/18531112012...</td>\n",
       "      <td>vintage buck pocket knife 500a u.s.a. lockback...</td>\n",
       "      <td>165.50</td>\n",
       "      <td>8.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>173.95</td>\n",
       "      <td>153.95</td>\n",
       "      <td>769.750000</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>https://i.ebayimg.com/00/s/MTIwMFgxNjAw/z/0tgA...</td>\n",
       "      <td>buck 110 amber stag folding hunting knife</td>\n",
       "      <td>204.05</td>\n",
       "      <td>8.50</td>\n",
       "      <td>https://www.ebay.com/itm/195204470340?nordt=tr...</td>\n",
       "      <td>212.55</td>\n",
       "      <td>192.55</td>\n",
       "      <td>962.750000</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11244</th>\n",
       "      <td>NaN</td>\n",
       "      <td>buck101 full tongue folding knife outdoor out ...</td>\n",
       "      <td>321.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321.83</td>\n",
       "      <td>298.83</td>\n",
       "      <td>1299.260870</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/BUCK101-Full-Tongue-F...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'true', 'buyItNowAvailabl...</td>\n",
       "      <td>True</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11257</th>\n",
       "      <td>NaN</td>\n",
       "      <td>buck knife koji model 2008 with wooden case ou...</td>\n",
       "      <td>546.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>546.27</td>\n",
       "      <td>523.27</td>\n",
       "      <td>2275.086957</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/Buck-Knife-Koji-model...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'true', 'buyItNowAvailabl...</td>\n",
       "      <td>True</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11258</th>\n",
       "      <td>NaN</td>\n",
       "      <td>buck110 folding knife outdoor knife second-han...</td>\n",
       "      <td>236.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>236.66</td>\n",
       "      <td>213.66</td>\n",
       "      <td>928.956522</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/BUCK110-Folding-Knife...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'true', 'buyItNowAvailabl...</td>\n",
       "      <td>True</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11265</th>\n",
       "      <td>NaN</td>\n",
       "      <td>buck usa   110 folding hunter knife anniversary</td>\n",
       "      <td>199.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>199.00</td>\n",
       "      <td>176.00</td>\n",
       "      <td>765.217391</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/Buck-USA-110-Folding-...</td>\n",
       "      <td>False</td>\n",
       "      <td>069**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingType': 'Calculated', 'shipToLocation...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>False</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11266</th>\n",
       "      <td>NaN</td>\n",
       "      <td>buck usa  wolverine rare 110 folding knife</td>\n",
       "      <td>199.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>199.00</td>\n",
       "      <td>176.00</td>\n",
       "      <td>765.217391</td>\n",
       "      <td>buck</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.ebay.com/itm/Buck-USA-Wolverine-ra...</td>\n",
       "      <td>False</td>\n",
       "      <td>069**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingType': 'Calculated', 'shipToLocation...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>False</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>566 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Image  \\\n",
       "88     https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/TisA...   \n",
       "120    https://thumbs.ebaystatic.com/pict/14432053875...   \n",
       "213    https://thumbs.ebaystatic.com/pict/17503100883...   \n",
       "233    https://thumbs.ebaystatic.com/pict/18531112012...   \n",
       "257    https://i.ebayimg.com/00/s/MTIwMFgxNjAw/z/0tgA...   \n",
       "...                                                  ...   \n",
       "11244                                                NaN   \n",
       "11257                                                NaN   \n",
       "11258                                                NaN   \n",
       "11265                                                NaN   \n",
       "11266                                                NaN   \n",
       "\n",
       "                                                   title  price_in_US  \\\n",
       "88             buck 889 strider pocket black 420hc knife       450.00   \n",
       "120                   buck 110 usa knife w/custom handle       175.00   \n",
       "213                                               knifes       180.00   \n",
       "233    vintage buck pocket knife 500a u.s.a. lockback...       165.50   \n",
       "257            buck 110 amber stag folding hunting knife       204.05   \n",
       "...                                                  ...          ...   \n",
       "11244  buck101 full tongue folding knife outdoor out ...       321.83   \n",
       "11257  buck knife koji model 2008 with wooden case ou...       546.27   \n",
       "11258  buck110 folding knife outdoor knife second-han...       236.66   \n",
       "11265    buck usa   110 folding hunter knife anniversary       199.00   \n",
       "11266         buck usa  wolverine rare 110 folding knife       199.00   \n",
       "\n",
       "       shipping_cost                                                url  \\\n",
       "88             30.00  https://www.ebay.com/itm/125407444151?nordt=tr...   \n",
       "120             5.95                                                NaN   \n",
       "213             0.00                                                NaN   \n",
       "233             8.45                                                NaN   \n",
       "257             8.50  https://www.ebay.com/itm/195204470340?nordt=tr...   \n",
       "...              ...                                                ...   \n",
       "11244           0.00                                                NaN   \n",
       "11257           0.00                                                NaN   \n",
       "11258           0.00                                                NaN   \n",
       "11265           0.00                                                NaN   \n",
       "11266           0.00                                                NaN   \n",
       "\n",
       "       converted_price  profit          ROI brand  cost  ...  \\\n",
       "88              480.00  460.00  2300.000000  buck  20.0  ...   \n",
       "120             180.95  160.95   804.750000  buck  20.0  ...   \n",
       "213             180.00  160.00   800.000000  buck  20.0  ...   \n",
       "233             173.95  153.95   769.750000  buck  20.0  ...   \n",
       "257             212.55  192.55   962.750000  buck  20.0  ...   \n",
       "...                ...     ...          ...   ...   ...  ...   \n",
       "11244           321.83  298.83  1299.260870  buck  20.0  ...   \n",
       "11257           546.27  523.27  2275.086957  buck  20.0  ...   \n",
       "11258           236.66  213.66   928.956522  buck  20.0  ...   \n",
       "11265           199.00  176.00   765.217391  buck  20.0  ...   \n",
       "11266           199.00  176.00   765.217391  buck  20.0  ...   \n",
       "\n",
       "                                             viewItemURL autoPay postalCode  \\\n",
       "88                                                   NaN     NaN        NaN   \n",
       "120                                                  NaN     NaN        NaN   \n",
       "213                                                  NaN     NaN        NaN   \n",
       "233                                                  NaN     NaN        NaN   \n",
       "257                                                  NaN     NaN        NaN   \n",
       "...                                                  ...     ...        ...   \n",
       "11244  https://www.ebay.com/itm/BUCK101-Full-Tongue-F...   False        NaN   \n",
       "11257  https://www.ebay.com/itm/Buck-Knife-Koji-model...   False        NaN   \n",
       "11258  https://www.ebay.com/itm/BUCK110-Folding-Knife...   False        NaN   \n",
       "11265  https://www.ebay.com/itm/Buck-USA-110-Folding-...   False      069**   \n",
       "11266  https://www.ebay.com/itm/Buck-USA-Wolverine-ra...   False      069**   \n",
       "\n",
       "                                           sellingStatus  \\\n",
       "88                                                   NaN   \n",
       "120                                                  NaN   \n",
       "213                                                  NaN   \n",
       "233                                                  NaN   \n",
       "257                                                  NaN   \n",
       "...                                                  ...   \n",
       "11244  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "11257  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "11258  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "11265  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "11266  {'currentPrice': {'_currencyId': 'USD', 'value...   \n",
       "\n",
       "                                            shippingInfo  \\\n",
       "88                                                   NaN   \n",
       "120                                                  NaN   \n",
       "213                                                  NaN   \n",
       "233                                                  NaN   \n",
       "257                                                  NaN   \n",
       "...                                                  ...   \n",
       "11244  {'shippingServiceCost': {'_currencyId': 'USD',...   \n",
       "11257  {'shippingServiceCost': {'_currencyId': 'USD',...   \n",
       "11258  {'shippingServiceCost': {'_currencyId': 'USD',...   \n",
       "11265  {'shippingType': 'Calculated', 'shipToLocation...   \n",
       "11266  {'shippingType': 'Calculated', 'shipToLocation...   \n",
       "\n",
       "                                             listingInfo returnsAccepted  \\\n",
       "88                                                   NaN             NaN   \n",
       "120                                                  NaN             NaN   \n",
       "213                                                  NaN             NaN   \n",
       "233                                                  NaN             NaN   \n",
       "257                                                  NaN             NaN   \n",
       "...                                                  ...             ...   \n",
       "11244  {'bestOfferEnabled': 'true', 'buyItNowAvailabl...            True   \n",
       "11257  {'bestOfferEnabled': 'true', 'buyItNowAvailabl...            True   \n",
       "11258  {'bestOfferEnabled': 'true', 'buyItNowAvailabl...            True   \n",
       "11265  {'bestOfferEnabled': 'false', 'buyItNowAvailab...           False   \n",
       "11266  {'bestOfferEnabled': 'false', 'buyItNowAvailab...           False   \n",
       "\n",
       "      condition topRatedListing  galleryPlusPictureURL  \n",
       "88          NaN             NaN                    NaN  \n",
       "120         NaN             NaN                    NaN  \n",
       "213         NaN             NaN                    NaN  \n",
       "233         NaN             NaN                    NaN  \n",
       "257         NaN             NaN                    NaN  \n",
       "...         ...             ...                    ...  \n",
       "11244    3000.0           False                    NaN  \n",
       "11257    1000.0           False                    NaN  \n",
       "11258    3000.0           False                    NaN  \n",
       "11265    1000.0           False                    NaN  \n",
       "11266    3000.0           False                    NaN  \n",
       "\n",
       "[566 rows x 22 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buck.loc[buck['converted_price'] > 165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_iqr_filter(df1, df2):\n",
    "    \n",
    "#     price_Q11 = df1['converted_price'].quantile(0.25)\n",
    "#     price_Q31 = df1['converted_price'].quantile(0.75)\n",
    "#     price_iqr1 = price_Q31 - price_Q11\n",
    "\n",
    "#     profit_Q11 = df1['profit'].quantile(0.25)\n",
    "#     profit_Q31 = df1['profit'].quantile(0.75)\n",
    "#     profit_iqr1 = profit_Q31 - profit_Q11\n",
    "\n",
    "#     ROI_Q11 = df1['ROI'].quantile(0.25)\n",
    "#     ROI_Q31 = df1['ROI'].quantile(0.75)\n",
    "#     ROI_iqr1 = ROI_Q31 - ROI_Q11\n",
    "\n",
    "#     price_upper_limit1 = price_Q31 + (1.5 * price_iqr1)\n",
    "#     price_lower_limit1 = price_Q11 - (1.5 * price_iqr1)\n",
    "\n",
    "#     profit_upper_limit1 = profit_Q31 + (1.5 * profit_iqr1)\n",
    "#     profit_lower_limit1 = profit_Q11 - (1.5 * profit_iqr1)\n",
    "\n",
    "#     ROI_upper_limit1 = ROI_Q31 + (1.5 * ROI_iqr1)\n",
    "#     ROI_lower_limit1 = ROI_Q11 - (1.5 * ROI_iqr1)\n",
    "    \n",
    "#     price_Q12 = df2['converted_price'].quantile(0.25)\n",
    "#     price_Q32 = df2['converted_price'].quantile(0.75)\n",
    "#     price_iqr2 = price_Q32 - price_Q12\n",
    "\n",
    "#     profit_Q12 = df2['profit'].quantile(0.25)\n",
    "#     profit_Q32 = df2['profit'].quantile(0.75)\n",
    "#     profit_iqr2 = profit_Q32 - profit_Q12\n",
    "\n",
    "#     ROI_Q12 = df2['ROI'].quantile(0.25)\n",
    "#     ROI_Q32 = df2['ROI'].quantile(0.75)\n",
    "#     ROI_iqr2 = ROI_Q32 - ROI_Q12\n",
    "\n",
    "#     price_upper_limit2 = price_Q32 + (1.5 * price_iqr2)\n",
    "#     price_lower_limit2 = price_Q12 - (1.5 * price_iqr2)\n",
    "\n",
    "#     profit_upper_limit2 = profit_Q32 + (1.5 * profit_iqr2)\n",
    "#     profit_lower_limit2 = profit_Q12 - (1.5 * profit_iqr2)\n",
    "\n",
    "#     ROI_upper_limit2 = ROI_Q32 + (1.5 * ROI_iqr2)\n",
    "#     ROI_lower_limit2 = ROI_Q12 - (1.5 * ROI_iqr2)\n",
    "    \n",
    "#     price_upper_limit = (price_upper_limit1 + price_upper_limit2)/2 \n",
    "#     price_lower_limit = (price_lower_limit1 + price_lower_limit2)/2 \n",
    "\n",
    "#     profit_upper_limit = (profit_upper_limit1 + profit_upper_limit2)/2 \n",
    "#     profit_lower_limit = (profit_lower_limit1 + profit_lower_limit2)/2 \n",
    "\n",
    "#     ROI_upper_limit = (ROI_upper_limit1 + ROI_upper_limit2)/2 \n",
    "#     ROI_lower_limit = (ROI_lower_limit1 + ROI_lower_limit2)/2 \n",
    "    \n",
    "\n",
    "#     new_df1 = df1[(df1['converted_price'] < price_upper_limit) &\n",
    "#                   (df1['converted_price'] > price_lower_limit) &\n",
    "#                   (df1['profit'] <profit_upper_limit) &\n",
    "#                   (df1['ROI'] < ROI_upper_limit) &\n",
    "#                   (df1['profit'] < profit_upper_limit) &\n",
    "#                   (df1['ROI'] > ROI_lower_limit)]\n",
    "    \n",
    "#     new_df2 = df2[(df2['converted_price'] < price_upper_limit) &\n",
    "#                   (df2['converted_price'] > price_lower_limit) &\n",
    "#                   (df2['profit'] <profit_upper_limit) &\n",
    "#                   (df2['ROI'] < ROI_upper_limit) &\n",
    "#                   (df2['profit'] < profit_upper_limit) &\n",
    "#                   (df2['ROI'] > ROI_lower_limit)]\n",
    "    \n",
    "#     return new_df1, new_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tera_bench_filt, bench_filt = apply_iqr_filter(teradf_bench, df_bench)\n",
    "# terabuck_filt, buck_filt = apply_iqr_filter(teradf_buck, df_buck)\n",
    "# teracase_filt, case_filt = apply_iqr_filter(teradf_case, df_case)\n",
    "# teracrkt_filt, crkt_filt = apply_iqr_filter(teradf_crkt, df_crkt)\n",
    "# terakershaw_filt, kershaw_filt = apply_iqr_filter(teradf_kershaw, df_kershaw)\n",
    "# teraleath_filt, leath_filt = apply_iqr_filter(teradf_leath, df_leath)\n",
    "# terasog_filt, sog_filt = apply_iqr_filter(teradf_sog, df_sog)\n",
    "# teraspyd_filt, spyd_filt = apply_iqr_filter(teradf_spyd, df_spyd)\n",
    "# teravict_filt, vict_filt = apply_iqr_filter(teradf_vict, df_vict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_cleaner(df):\n",
    "#     lot = re.compile('(?<!-\\S)lot(?![^\\s.,?!])')\n",
    "#     disp = re.compile('(display)')\n",
    "#     box = re.compile('(box)')\n",
    "#     group = re.compile('(group)')\n",
    "#     is_set = re.compile('(?<!-\\S)set(?![^\\s.,?!])')\n",
    "#     trim_list = [lot,disp,box,group,is_set]\n",
    "#     for item in trim_list:\n",
    "#         df.loc[df['title'].apply(lambda x: re.search(item, x)).isna() != True, 'trim'] = 1 \n",
    "#     to_drop = df.loc[df['trim'] == 1].index\n",
    "#     df.drop(to_drop, inplace=True)\n",
    "#     df.drop('trim', axis=1, inplace=True)\n",
    "    \n",
    "#     df = apply_iqr_filter(df)\n",
    "#     return df\n",
    "    \n",
    "#     df.loc[df['title'].apply(lambda x: re.search(lot, x)).isna() != True, 'trim'] = 1\n",
    "#     df.loc[df['title'].apply(lambda x: re.search(display, x)).isna() != True, 'trim'] = 1\n",
    "#     df.loc[df['title'].apply(lambda x: re.search(box, x)).isna() != True, 'trim'] = 1\n",
    "#     df.loc[df['title'].apply(lambda x: re.search(group, x)).isna() != True, 'trim'] = 1\n",
    "#     df.loc[df['title'].apply(lambda x: re.search(is_set, x)).isna() != True, 'trim'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(9):\n",
    "    row = n//3 \n",
    "    col = n%3 \n",
    "    print(f'row:{row}')\n",
    "    print(f'col:{col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,10), ncols=2, nrows=1, sharey=True, sharex=True)\n",
    "sns.histplot(bench_filt['converted_price'], ax=axes[0], color='lightcoral', label='Brand: Benchmade')\n",
    "sns.histplot(tera_bench_filt['converted_price'], ax=axes[1], color='yellowgreen', label='Brand: Benchmade(tera)')\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    col = n%2\n",
    "    axes[col].set_xlabel('Resale Value(US Dollars)', fontsize=15)\n",
    "    axes[col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[col].tick_params(axis='both', labelsize=13)\n",
    "    \n",
    "# axes[0][0].axvline(x = 45, color = 'black', label= 'Cost at Surplus Store')\n",
    "# axes[0][1].axvline(x = 20, color = 'black') \n",
    "# axes[0][2].axvline(x = 20, color = 'black')\n",
    "# axes[1][0].axvline(x = 15, color = 'black')\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.8, .80), fontsize='large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_used.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=3, nrows=3, sharex=True, sharey=True)\n",
    "sns.histplot(df_bench['converted_price'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "sns.histplot(df_buck['converted_price'], ax=axes[0][1], color='chocolate', label='Brand: Buck')\n",
    "sns.histplot(case_df['converted_price'], ax=axes[0][2],  color='khaki', label='Brand: Case')\n",
    "sns.histplot(df_crkt['converted_price'], ax=axes[1][0], color='palegreen', label='Brand: CRKT')\n",
    "sns.histplot(df_kershaw['converted_price'], ax=axes[1][1], color='turquoise', label='Brand: Kershaw')\n",
    "sns.histplot(df_leatherman['converted_price'], ax=axes[1][2], color='firebrick', label='Brand: Leatherman')\n",
    "sns.histplot(df_sog['converted_price'], ax=axes[2][0],  color='plum', label='Brand: Sog')\n",
    "sns.histplot(df_spyderco['converted_price'], ax=axes[2][1],  color='plum', label='Brand: Spyderco')\n",
    "sns.histplot(df_victorinox['converted_price'], ax=axes[2][2], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "for n in range(9):\n",
    "    row = n//3 \n",
    "    col = n%3 \n",
    "    axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=15)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    \n",
    "axes[0][0].axvline(x = 45, color = 'black', label= 'Cost at Surplus Store')\n",
    "axes[0][1].axvline(x = 20, color = 'black') \n",
    "axes[0][2].axvline(x = 20, color = 'black')\n",
    "axes[1][0].axvline(x = 15, color = 'black')\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.8, .80), fontsize='large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=3, nrows=3, sharex=True)\n",
    "sns.histplot(df_bench['converted_price'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "sns.histplot(df_buck['converted_price'], ax=axes[0][1], color='chocolate', label='Brand: Buck')\n",
    "sns.histplot(case_df['converted_price'], ax=axes[0][2],  color='khaki', label='Brand: Case')\n",
    "sns.histplot(df_crkt['converted_price'], ax=axes[1][0], color='palegreen', label='Brand: CRKT')\n",
    "sns.histplot(df_kershaw['converted_price'], ax=axes[1][1], color='turquoise', label='Brand: Kershaw')\n",
    "sns.histplot(df_leatherman['converted_price'], ax=axes[1][2], color='firebrick', label='Brand: Leatherman')\n",
    "sns.histplot(df_sog['converted_price'], ax=axes[2][0],  color='plum', label='Brand: Sog')\n",
    "sns.histplot(df_spyderco['converted_price'], ax=axes[2][1],  color='plum', label='Brand: Spyderco')\n",
    "sns.histplot(df_victorinox['converted_price'], ax=axes[2][2], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "for n in range(9):\n",
    "    row = n//3 \n",
    "    col = n%3 \n",
    "    axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=15)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    \n",
    "axes[0][0].axvline(x = 45, color = 'black', label= 'Cost at Surplus Store')\n",
    "axes[0][1].axvline(x = 20, color = 'black') \n",
    "axes[0][2].axvline(x = 20, color = 'black')\n",
    "axes[1][0].axvline(x = 15, color = 'black')\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.8, .80), fontsize='large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=3, nrows=3, sharey=True)\n",
    "sns.histplot(df_bench['converted_price'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "sns.histplot(df_buck['converted_price'], ax=axes[0][1], color='chocolate', label='Brand: Buck')\n",
    "sns.histplot(case_df['converted_price'], ax=axes[0][2],  color='khaki', label='Brand: Case')\n",
    "sns.histplot(df_crkt['converted_price'], ax=axes[1][0], color='palegreen', label='Brand: CRKT')\n",
    "sns.histplot(df_kershaw['converted_price'], ax=axes[1][1], color='turquoise', label='Brand: Kershaw')\n",
    "sns.histplot(df_leatherman['converted_price'], ax=axes[1][2], color='firebrick', label='Brand: Leatherman')\n",
    "sns.histplot(df_sog['converted_price'], ax=axes[2][0],  color='plum', label='Brand: Sog')\n",
    "sns.histplot(df_spyderco['converted_price'], ax=axes[2][1],  color='plum', label='Brand: Spyderco')\n",
    "sns.histplot(df_victorinox['converted_price'], ax=axes[2][2], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "for n in range(9):\n",
    "    row = n//3 \n",
    "    col = n%3 \n",
    "    axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=15)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    \n",
    "axes[0][0].axvline(x = 45, color = 'black', label= 'Cost at Surplus Store')\n",
    "axes[0][1].axvline(x = 20, color = 'black') \n",
    "axes[0][2].axvline(x = 20, color = 'black')\n",
    "axes[1][0].axvline(x = 15, color = 'black')\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.8, .80), fontsize='large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes  = plt.subplots(figsize=(15,15), ncols=2, nrows=5, sharex=True)\n",
    "# sns.histplot(df_bench['converted_price'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "# sns.histplot(df_buck['converted_price'], ax=axes[1][0], color='chocolate', label='Brand: Buck')\n",
    "# sns.histplot(df_case['converted_price'], ax=axes[2][0],  color='khaki', label='Brand: Case')\n",
    "# sns.histplot(df_caseXX['converted_price'], ax=axes[3][0],  color='khaki', label='Brand: CaseXX')\n",
    "# sns.histplot(df_crkt['converted_price'], ax=axes[4][0], color='palegreen', label='Brand: CRKT')\n",
    "# sns.histplot(df_kershaw['converted_price'], ax=axes[0][1], color='turquoise', label='Brand: Kershaw')\n",
    "# sns.histplot(df_leatherman['converted_price'], ax=axes[1][1], color='firebrick', label='Brand: Leatherman')\n",
    "# sns.histplot(df_sog['converted_price'], ax=axes[2][1],  color='plum', label='Brand: Sog')\n",
    "# sns.histplot(df_spyderco['converted_price'], ax=axes[3][1],  color='plum', label='Brand: Spyderco')\n",
    "# sns.histplot(df_victorinox['converted_price'], ax=axes[4][1], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "# for n in range(10):\n",
    "#     row = n%5 \n",
    "#     col = n//5 \n",
    "#     axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=15)\n",
    "#     axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "#     axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    \n",
    "# axes[0][0].axvline(x = 45, color = 'black', label= 'Cost at Surplus Store')\n",
    "# axes[1][0].axvline(x = 20, color = 'black') \n",
    "# axes[2][0].axvline(x = 20, color = 'black')\n",
    "# axes[3][0].axvline(x = 15, color = 'black')\n",
    "# fig.suptitle(\"Resale Value of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "# fig.legend(loc=(.8, .80), fontsize='large')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = re.compile('(?<!-\\S)lot(?![^\\s.,?!])')\n",
    "# df_buck.loc[df_buck['title'].apply(lambda x: re.search(pattern, x)).isna() != True,'title'].sample(20).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_case.loc[df_case['title'].str.match(pattern), 'is_lot'] = 1\n",
    "# pattern = 'lot'\n",
    "# df_case.loc[df_case['title'].str.match(pattern), 'is_lot'] = 1\n",
    "# pattern = 'set'\n",
    "# df_case.loc[df_case['title'].str.match(pattern), 'is_lot'] = 1\n",
    "\n",
    "# to_drop = df_case.loc[df_case['is_lot'] == 1].index\n",
    "# df_case.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_itemIds = pd.read_csv('data/casespybench.csv')\n",
    "# df_listed = pd.read_csv(\"data/full_dataset2.csv\")\n",
    "# display(df_itemIds.info())\n",
    "# df_listed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket_dict = {'benchmade': 45.0,\n",
    "#                'buck': 20.0,\n",
    "#                'case': 20.0,\n",
    "#                'crkt': 15.0,\n",
    "#                'kershaw': 15.0,\n",
    "#                'leatherman': 30.0, \n",
    "#                'sog': 15.0,\n",
    "#                'spyderco': 30.0,\n",
    "#                'victorinox': 20.0\n",
    "#               }\n",
    "\n",
    "# def add_brand(df, brand):\n",
    "#     df.loc[df[brand].isna() == False, 'brand'] = brand\n",
    "    \n",
    "# for key in list(bucket_dict.keys()):\n",
    "# add_brand(df_listed, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listed.loc[df_listed['benchmade'].isna() == False, 'brand'] = 'benchmade'\n",
    "# df_listed.loc[df_listed['buck'].isna() == False, 'brand'] = 'buck'\n",
    "# df_listed.loc[df_listed['case'].isna() == False, 'brand'] = 'case'\n",
    "# df_listed.loc[df_listed['crkt'].isna() == False, 'brand'] = 'crkt'\n",
    "# df_listed.loc[df_listed['kershaw'].isna() == False, 'brand'] = 'kershaw'\n",
    "# df_listed.loc[df_listed['leatherman'].isna() == False, 'brand'] = 'leatherman'\n",
    "# df_listed.loc[df_listed['spyderco'].isna() == False, 'brand'] = 'spyderco'\n",
    "# df_listed.loc[df_listed['sog'].isna() == False, 'brand'] = 'sog'\n",
    "# df_listed.loc[df_listed['victorinox'].isna() == False, 'brand'] = 'victorinox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_drop = df_listed.loc[(df_listed['victorinox'].isna() == False) |\n",
    "#                  (df_listed['leatherman'].isna() == False)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listed.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listed.drop(df_listed.columns[22:30], axis=1, inplace=True)\n",
    "# df_listed.drop('benchmade', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_used = df_listed[df_listed['condition'] != 1000.0].copy()\n",
    "# df_new = df_listed[df_listed['condition'] == 1000.0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df = pd.read_csv('data/tera_df_prepared.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = tera_df.columns[3:8].values\n",
    "\n",
    "for column in num_columns:\n",
    "    tera_df[column] = tera_df[column].apply(lambda x: float(x))\n",
    "\n",
    "tera_df['cost'] = tera_df['cost'].apply(lambda x: float(x))\n",
    "\n",
    "tera_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used_filtered = apply_iqr_filter(df_used).copy()\n",
    "df_new_filtered = apply_iqr_filter(df_new).copy()\n",
    "df_filtered = apply_iqr_filter(df_listed).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_itemIds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_used = df_used.merge(df_itemIds, left_on='itemId', right_on='ItemID').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_filtered.merge(df_itemIds, left_on='itemId', right_on='ItemID').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_used.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "df_merged_used['PictureURL'] = df_merged_used['PictureURL'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.dropna(subset=['PictureURL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['PictureURL'] = df_merged['PictureURL'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_used = df_merged_used.explode('PictureURL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_explode = df_merged.explode('PictureURL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_used.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_used.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_merged_explode = pd.concat([df_merge_used, tera_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_merged_explode['Image'].fillna(tera_merged_explode['PictureURL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = tera_merged_explode.loc[(tera_merged_explode['brand'] == 'leatherman') |\n",
    "                                  (tera_merged_explode['brand'] == 'victorinox')].index\n",
    "tera_merged_explode.drop(to_drop, inplace=True)\n",
    "tera_merged_explode.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_merged_explode[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('box only')\n",
    "tera_merged_explode['title'] = tera_merged_explode['title'].apply(lambda x: str(x).lower())\n",
    "tera_merged_explode['is_box'] = tera_merged_explode['title'].apply(lambda x: re.search(pattern, x))\n",
    "tera_merged_explode.drop(tera_merged_explode.loc[tera_merged_explode['is_box'].isnull() == False].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('screws')\n",
    "tera_merged_explode['screws'] = df_final['title'].apply(lambda x: re.search(pattern, x))\n",
    "tera_merged_explode.drop(tera_merged_explode.loc[tera_merged_explode['screws'].isnull() == False].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_merged_explode.drop(tera_merged_explode.loc[tera_merged_explode['converted_price'] <=5].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_merged_explode.to_csv('data/tera_merged_explode.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = apply_iqr_filter(tera_merged_explode).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('data/df_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(row):\n",
    "    filename = os.path.join(root_folder, str(row.name) + im_extension)\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.Image\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    except:\n",
    "        print(f'{filename} error')\n",
    "\n",
    "root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6'\n",
    "im_extension = '.jpg'  # or whatever type of images you are downloading\n",
    "\n",
    "df_final.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_files = []\n",
    "for filename in os.listdir('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6'):\n",
    "    if filename.endswith('.jpg'):\n",
    "        try:\n",
    "            img = Image.open('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6/'+filename)  # open the image file\n",
    "            img.verify()  # verify that it is, in fact an image\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(filename)\n",
    "            removed_files.append(filename)\n",
    "#             os.remove('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in removed_files:\n",
    "    os.remove('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_buy = [file for file in os.listdir(path) if file.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def sorted_nicely(image_list):\n",
    "    \"\"\" \n",
    "    Sorts the given iterable in the way that is expected.\n",
    " \n",
    "    Required arguments:\n",
    "    l -- The iterable to be sorted.\n",
    " \n",
    "    \"\"\"\n",
    "    convert = lambda text: int(text) if text.isdigit() else text\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(image_list, key = alphanum_key)\n",
    "              \n",
    "# Driver code\n",
    "a = [\"v1_0001.jpg\",\"v1_0002.jpg\",\"v1_0003.jpg\",\"v1_00017.jpg\",\"v1_00015.jpg\"]\n",
    "print(sorted_nicely(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgs_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_image_list = sorted_nicely(imgs_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(8, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(100,100,3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "# model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "# model.add(layers.Conv2D(100, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(100, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# opt = SGD(lr=0.001, momentum=0.9)\n",
    "# model.compile(optimizer=opt,\n",
    "#               loss='binary_crossentropy',\n",
    "#                metrics=['accuracy'])\n",
    "# #switching from regression to classifier.. no longer mse loss\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=5,\n",
    "#                     batch_size=100,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu',  padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu',  padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu',  padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu',  padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu',  padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu',  padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(250, (3,3), activation='relu',  padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(250, (3,3), activation='relu',  padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=3)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mymodel250250.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(sorted_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('(\\D)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[0] = df_new[0].apply(lambda x: re.sub(pattern, \"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[0] = df_new[0].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_indx = df_new[0] .values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old_df_indx = df_filtered.index.values\n",
    "old_df_indx = df_final.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_imgs = list(set(old_df_indx) - set(new_df_indx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_ready = df_filtered.drop(missing_imgs)\n",
    "# df_ready = concat_df.drop(missing_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = df_final.drop(missing_imgs).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_ready.loc[df_ready['ROI'] > 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = df_ready.loc[df_ready['ROI'] > 100].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for index in index_list:\n",
    "    index = str(index) + '.jpg'\n",
    "    file_names.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "    \n",
    "source_dir = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6/'\n",
    "target_dir = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6/buy/'\n",
    "    \n",
    "for file_name in file_names:\n",
    "    shutil.move(os.path.join(source_dir, file_name), target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(target_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_not = df_ready.drop(df_ready.loc[df_ready['ROI'] > 100].index).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_not = []\n",
    "for index in index_list_not:\n",
    "    index = str(index) + '.jpg'\n",
    "    file_names_not.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_names_not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "img_height = 250\n",
    "img_width = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    label_mode='binary',\n",
    "    class_names=['not_buy', 'buy'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=111,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    label_mode='binary',\n",
    "    class_names=['not_buy', 'buy'],\n",
    "    subset=\"validation\",\n",
    "    seed=111,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(labels[i].numpy())\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in listdir('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/extra_bench'):\n",
    "#     if filename.endswith(\".jpg\"):\n",
    "#         try:\n",
    "#             im = Image.load(filename)\n",
    "#             im.verify() #I perform also verify, don't know if he sees other types o defects\n",
    "#             im.close() #reload is necessary in my case\n",
    "#             im = Image.load(filename) \n",
    "#             im.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
    "#             im.close()\n",
    "#         except: \n",
    "#         print(f'{filename} is corrupted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in os.listdir('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images3'):\n",
    "#     if filename.endswith('.jpg'):\n",
    "#         try:\n",
    "#             img = Image.open('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images3/'+filename)  # open the image file\n",
    "#             img.verify()  # verify that it is, in fact an image\n",
    "#         except (IOError, SyntaxError) as e:\n",
    "#             print(filename)\n",
    "#             os.remove('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images3/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(os.listdir('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_files = []\n",
    "for filename in os.listdir('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6'):\n",
    "    if filename.endswith('.jpg'):\n",
    "        try:\n",
    "            img = Image.open('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6/'+filename)  # open the image file\n",
    "            img.verify()  # verify that it is, in fact an image\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(filename)\n",
    "            removed_files.append(filename)\n",
    "            os.remove('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6/buy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Image Resolutions\n",
    "\n",
    "# Import Packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import imagesize\n",
    "import numpy as np\n",
    "\n",
    "# Get the Image Resolutions\n",
    "imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "img_meta = {}\n",
    "for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# Convert it to Dataframe and compute aspect ratio\n",
    "img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "img_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Image Resolutions\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "ax.set_title(\"Image Resolution\")\n",
    "ax.set_xlabel(\"Width\", size=14)\n",
    "ax.set_ylabel(\"Height\", size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_meta_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemIds= df_filtered.loc[(df_filtered['brand'] == 'spyderco') | \n",
    "                         (df_filtered['brand'] == 'benchmade') |\n",
    "                         (df_filtered['brand'] == 'case'), 'itemId'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID', 'ItemSpecifics', \n",
    "                'PictureURL']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         Add to list if all values are present\n",
    "        if all(prepared_data.values()):\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_list(my_list):\n",
    " \n",
    "    api = Shopping(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "    request = {\n",
    "               'itemID': my_list,\n",
    "               'IncludeSelector': 'ItemSpecifics'\n",
    "              }\n",
    "    response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.finding import Connection\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.shopping import Connection as Shopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv('data/tera_benchmade_itemID.csv')\n",
    "buck = pd.read_csv('data/tera_buck_ItemIDs.csv')\n",
    "case = pd.read_csv('data/tera_case_itemIDs.csv')\n",
    "kershaw = pd.read_csv('data/tera_kershaw_ItemIDs.csv')\n",
    "sog = pd.read_csv('data/tera_sog_ItemIDs.csv')\n",
    "spy = pd.read_csv('data/tera_spyderco_ItemIDs.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [bench,buck,case,kershaw,sog,spy]\n",
    "for dataframe in dfs:\n",
    "    display(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dfs:\n",
    "    dataframe.rename({'Data_field': 'itemId',\n",
    "                      'Field4': 'date_sold'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dfs:\n",
    "    dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df = pd.read_csv('data/tera_df_prepared.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_iqr_filter(df):\n",
    "    \n",
    "    price_Q1 = df['converted_price'].quantile(0.25)\n",
    "    price_Q3 = df['converted_price'].quantile(0.75)\n",
    "    price_iqr = price_Q3 - price_Q1\n",
    "\n",
    "    profit_Q1 = df['profit'].quantile(0.25)\n",
    "    profit_Q3 = df['profit'].quantile(0.75)\n",
    "    profit_iqr = profit_Q3 - profit_Q1\n",
    "\n",
    "    ROI_Q1 = df['ROI'].quantile(0.25)\n",
    "    ROI_Q3 = df['ROI'].quantile(0.75)\n",
    "    ROI_iqr = ROI_Q3 - ROI_Q1\n",
    "\n",
    "    price_upper_limit = price_Q3 + (1.5 * price_iqr)\n",
    "    price_lower_limit = price_Q1 - (1.5 * price_iqr)\n",
    "\n",
    "    profit_upper_limit = profit_Q3 + (1.5 * profit_iqr)\n",
    "    profit_lower_limit = profit_Q1 - (1.5 * profit_iqr)\n",
    "\n",
    "    ROI_upper_limit = ROI_Q3 + (1.5 * ROI_iqr)\n",
    "    ROI_lower_limit = ROI_Q1 - (1.5 * ROI_iqr)\n",
    "    \n",
    "    print(f'price upper limit: ${np.round(price_upper_limit,2)}')\n",
    "    print(f'price lower limit: ${np.round(price_lower_limit,2)}')\n",
    "    print('-----------------------------------')\n",
    "    print(f'profit upper limit: ${np.round(profit_upper_limit,2)}')\n",
    "    print(f'profit lower limit: ${np.round(profit_lower_limit,2)}')\n",
    "    print('-----------------------------------')\n",
    "    print(f'ROI upper limit: {np.round(ROI_upper_limit,2)}%')\n",
    "    print(f'ROI lower limit: {np.round(ROI_lower_limit,2)}%')\n",
    "    print('-----------------------------------')\n",
    "\n",
    "    \n",
    "    new_df = df[(df['converted_price'] < price_upper_limit) &\n",
    "             (df['profit'] <profit_upper_limit) &\n",
    "             (df['ROI'] < ROI_upper_limit) &\n",
    "             (df['profit'] < profit_upper_limit) &\n",
    "             (df['ROI'] > ROI_lower_limit)]\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = tera_df.columns[3:8].values\n",
    "\n",
    "for column in num_columns:\n",
    "    tera_df[column] = tera_df[column].apply(lambda x: float(x))\n",
    "\n",
    "tera_df['cost'] = tera_df['cost'].apply(lambda x: float(x))\n",
    "\n",
    "tera_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('lot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df['title'] = tera_df['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df['is_lot'] = tera_df['title'].apply(lambda x: re.search(pattern,str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lot_drop = tera_df.loc[tera_df['is_lot'].isna() != True].index\n",
    "tera_df.drop(lot_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df['title'].sample(20).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/full_dataset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'galleryURL': 'Image'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df,tera_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['title'] = df2['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['title'] = df2['title'].apply(lambda x: re.sub('#', \"\", str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_brand(df,brand):\n",
    "    df.loc[df[brand].isna() == False, 'brand'] = brand\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = ['benchmade', 'buck', 'case', \n",
    "          'crkt', 'kershaw', \n",
    "          'leatherman', 'spyderco', \n",
    "          'sog', 'victorinox']\n",
    "\n",
    "for brand in brands:\n",
    "    df2 = fill_brand(df, brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.galleryURL.sample(10).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"\\d{4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df['title'].apply(lambda x: re.findall(pattern, str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dfs:\n",
    "    dataframe.dropna(subset=['itemId'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dfs:\n",
    "    dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kershaw.rename({'item': 'title'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "#x = position of bucket_dictionary\n",
    "def prepare_tera_df(df, x):\n",
    "    df['price'] = df['price'].str.replace(\"$\", \"\")\n",
    "    df['price'] = df['price'].str.replace(\",\", \"\")\n",
    "    df['price'] = df['price'].apply(float)\n",
    "    \n",
    "    df['shipping'] = df['shipping'].str.replace(\"$\", \"\")\n",
    "    df['shipping'] = df['shipping'].str.replace(\",\", \"\")\n",
    "    df['shipping'] = df['shipping'].apply(float)\n",
    "    \n",
    "    df['converted_price'] = (df['price'] + df['shipping'])\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - list(bucket_dict.values())[x])\n",
    "    df['ROI'] = (df['profit']/(list(bucket_dict.values())[x]))*100.0\n",
    "    \n",
    "    df['brand'] = list(bucket_dict.keys())[x]\n",
    "    df['cost'] = list(bucket_dict.values())[x]\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "bench = prepare_tera_df(bench, 0).copy()\n",
    "buck = prepare_tera_df(buck, 1).copy() \n",
    "case = prepare_tera_df(case, 2).copy()\n",
    "kershaw = prepare_tera_df(kershaw, 4).copy()\n",
    "sog = prepare_tera_df(sog, 6).copy()\n",
    "spy = prepare_tera_df(spy, 7).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([bench,buck,case,kershaw,sog,spy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['itemId'] = df['itemId'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemIds = df.itemId.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemIds[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemIds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(itemIds), 20):\n",
    "    process_list(itemIds[i:i+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids.drop_duplicates(subset=['ItemID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids['ItemID'] = df_ids['ItemID'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids['item_list'] = df_ids['ItemSpecifics'].apply(lambda x: x['NameValueList'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids.explode('item_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids.rename({'ItemID': 'itemId'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.rename({'ItemID': 'itemId'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.merge(df_ids, on='itemId').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat((df3.iloc[[type(item) == list for item in df3['item_list']]].explode('item_list'),\n",
    "                 df3.iloc[[type(item) != list for item in df3['item_list']]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['aspect_name'] = df2['item_list'].apply(lambda x: x['Name'])\n",
    "df2['aspect_value'] = df2['item_list'].apply(lambda x: x['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[df2['aspect_name'] == 'Model', 'aspect_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Model'] = df2.loc[df2['aspect_name'] == 'Model', 'aspect_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['aspect_name'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Type'] = df2.loc[df2['aspect_name'] == 'Type', 'aspect_value']\n",
    "df3['Number_of_Blades'] = df2.loc[df2['aspect_name'] == 'Number of Blades', 'aspect_value']\n",
    "df3['Color'] = df2.loc[df2['aspect_name'] == 'Color', 'aspect_value']\n",
    "df3['Opening_Mechanism'] = df2.loc[df2['aspect_name'] == 'Opening Mechanism', 'aspect_value']\n",
    "df3['Handle_Material'] = df2.loc[df2['aspect_name'] == 'Handle Material', 'aspect_value']\n",
    "df3['Region_of_Manufacture'] = df2.loc[df2['aspect_name'] == 'Country/Region of Manufacture', 'aspect_value']\n",
    "df3['Blade_Edge'] = df2.loc[df2['aspect_name'] == 'Blade Edge', 'aspect_value']\n",
    "df3['Lock_Type'] = df2.loc[df2['aspect_name'] == 'Lock Type', 'aspect_value']\n",
    "df3['Blade_Material'] = df2.loc[df2['aspect_name'] == 'Blade Material', 'aspect_value']\n",
    "df3['Blade_Type'] = df2.loc[df2['aspect_name'] == 'Blade Type', 'aspect_value']\n",
    "df3['is_Original'] = df2.loc[df2['aspect_name'] == 'Original/Reproduction', 'aspect_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blade_type_values = df3.iloc[[type(item) == list for item in df3['Blade_Type']]]['Blade_Type'].apply(lambda x: x[0]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blade_type_indx = df3.iloc[[type(item) == list for item in df3['Blade_Type']]].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[blade_type_indx, 'Blade_Type'] = blade_type_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Blade_Type'] = df3['Blade_Type'].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Blade_Type'].value_counts()[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_values = df3.iloc[[type(item) == list for item in df3['Model']]]['Model'].apply(lambda x: x[0]).values\n",
    "model_index = df3.iloc[[type(item) == list for item in df3['Model']]].index\n",
    "df3.loc[model_index, 'Model'] = model_values\n",
    "df3['Model'] = df3['Model'].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Model'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_aspects(df,aspect):\n",
    "    values = df.iloc[[type(item) == list for item in df[aspect]]][aspect].apply(lambda x: x[0]).values\n",
    "    index = df.iloc[[type(item) == list for item in df[aspect]]].index\n",
    "    df.loc[index, aspect] = values\n",
    "    df[aspect] = df[aspect].apply(lambda x: str(x).lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = prepare_aspects(df3, 'Color')\n",
    "df3 = prepare_aspects(df3, 'Opening_Mechanism')\n",
    "df3 = prepare_aspects(df3, 'Handle_Material')\n",
    "df3 = prepare_aspects(df3, 'Region_of_Manufacture')\n",
    "df3 = prepare_aspects(df3, 'Blade_Edge')\n",
    "df3 = prepare_aspects(df3, 'Lock_Type')\n",
    "df3 = prepare_aspects(df3, 'Blade_Material')\n",
    "df3 = prepare_aspects(df3, 'Blade_Type')\n",
    "df3 = prepare_aspects(df3, 'is_Original')\n",
    "df3 = prepare_aspects(df3, 'Number_of_Blades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = prepare_aspects(df3, 'Number_of_Blades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Number_of_Blades'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('data/complete_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfhfghgfhfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('data/complete_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.brand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.fillna('unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df3.loc[df3['Opening_Mechanism'].isin(['manual', 'assisted']), 'Opening_Mechanism']\n",
    "y = df3.loc[df3['Opening_Mechanism'].isin(['manual', 'assisted']), 'converted_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12,10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.barplot(x=x,y=y)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(18,8))\n",
    "plt.xticks(rotation=45)\n",
    "sns.barplot(x=x,y=y, hue=df3['brand'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df3.loc[df3['Region_of_Manufacture'].isin(['united states', \n",
    "#                                                'china', 'japan', \n",
    "#                                                'taiwan', 'italy']), 'Region_of_Manufacture']\n",
    "# y = df3.loc[df3['Region_of_Manufacture'].isin(['united states', \n",
    "#                                                'china', 'japan', \n",
    "#                                                'taiwan', 'italy']), 'converted_price']\n",
    "\n",
    "# fig = plt.subplots(figsize=(20,8))\n",
    "# plt.xticks(rotation=45)\n",
    "# sns.barplot(x=x,y=y, hue=df3['brand'])\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cardinality_threshold(column,threshold=0.75,return_categories_list=True):\n",
    "    #calculate the threshold value using\n",
    "    #the frequency of instances in column\n",
    "    threshold_value=int(threshold*len(column))\n",
    "    #initialize a new list for lower cardinality column\n",
    "    categories_list=[]\n",
    "    #initialize a variable to calculate sum of frequencies\n",
    "    s=0\n",
    "    #Create a dictionary (unique_category: frequency)\n",
    "    counts=Counter(column)\n",
    "\n",
    "    #Iterate through category names and corresponding frequencies after sorting the categories\n",
    "    #by descending order of frequency\n",
    "    for i,j in counts.most_common():\n",
    "        #Add the frequency to the total sum\n",
    "        s += dict(counts)[i]\n",
    "        #append the category name to the categories list\n",
    "        categories_list.append(i)\n",
    "        #Check if the global sum has reached the threshold value, if so break the loop\n",
    "        if s >= threshold_value:\n",
    "            break\n",
    "      #append the new 'Other' category to list\n",
    "    categories_list.append('Other')\n",
    "\n",
    "    #Take all instances not in categories below threshold  \n",
    "    #that were kept and lump them into the\n",
    "    #new 'Other' category.\n",
    "    new_column = column.apply(lambda x: x if x in categories_list else 'Other')\n",
    "\n",
    "    #Return the transformed column and\n",
    "    #unique categories if return_categories = True\n",
    "    if(return_categories_list):\n",
    "        return new_column,categories_list\n",
    "    #Return only the transformed column if return_categories=False\n",
    "    else:\n",
    "        return new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Type'] = df3['Type'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_lots = df3.loc[df3['Type'] == 'lot'].index\n",
    "df3.drop(drop_lots, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Type'].value_counts()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_blade_count = cardinality_threshold(df3['Number_of_Blades'], \n",
    "                                                threshold=0.99, \n",
    "                                                return_categories_list=False)\n",
    "transformed_blade_count.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df3.copy()\n",
    "df_transformed['blade_count'] = transformed_blade_count\n",
    "df_sorted = df_transformed.sort_values('blade_count')\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.suptitle('blade count')\n",
    "sns.barplot(x= 'blade_count', y='ROI', data=df_sorted)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_color = cardinality_threshold(df3['Color'], threshold=.9, return_categories_list=False)\n",
    "transformed_color.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df3.copy()\n",
    "df_transformed['color'] = transformed_color\n",
    "df_sorted = df_transformed.sort_values('color')\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.suptitle('knife color')\n",
    "sns.barplot(x= 'color', y='ROI', data=df_sorted)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Handle_Material'].replace({'g-10': 'g10'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_handle = cardinality_threshold(df3['Handle_Material'], threshold=.80, return_categories_list=False)\n",
    "transformed_handle.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df3.copy()\n",
    "df_transformed['handle'] = transformed_handle\n",
    "df_sorted = df_transformed.sort_values('handle')\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.suptitle('handle material')\n",
    "plt.xticks(rotation=45)\n",
    "sns.barplot(x= 'handle', y='ROI', data=df_sorted)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_edge = cardinality_threshold(df3['Blade_Edge'], threshold=.97, return_categories_list=False)\n",
    "transformed_edge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df3.copy()\n",
    "df_transformed['edge'] = transformed_edge\n",
    "df_sorted = df_transformed.sort_values('edge')\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.suptitle('blade edge')\n",
    "plt.xticks(rotation=45)\n",
    "sns.barplot(x= 'edge', y='ROI', data=df_sorted)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_material = cardinality_threshold(df3['Blade_Material'], threshold=.92, return_categories_list=False)\n",
    "transformed_material.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Blade_Material'].value_counts().index[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[df3['Blade_Material'].str.contains('s30v'), 'Blade_Material'] = 's30v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[df3['Blade_Material'].str.contains('vg-10'), 'Blade_Material'] = 'vg10'\n",
    "df3.loc[df3['Blade_Material'].str.contains('8cr13mov'), 'Blade_Material'] = '8cr13mov'\n",
    "df3.loc[df3['Blade_Material'].str.contains('420 hc'), 'Blade_Material'] = '420hc'\n",
    "df3.loc[df3['Blade_Material'].str.contains('420hc'), 'Blade_Material'] = '420hc'\n",
    "df3.loc[df3['Blade_Material'].str.contains('154'), 'Blade_Material'] = '154cm'\n",
    "df3.loc[df3['Blade_Material'].str.contains('aus-8'), 'Blade_Material'] = 'aus8'\n",
    "df3.loc[df3['Blade_Material'].str.contains('aus 8'), 'Blade_Material'] = 'aus8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Blade_Material'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_material = cardinality_threshold(df3['Blade_Material'], threshold=.92, return_categories_list=False)\n",
    "transformed_material.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df3.copy()\n",
    "df_transformed['material'] = transformed_material\n",
    "df_sorted = df_transformed.sort_values('material')\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.suptitle('blade material')\n",
    "plt.xticks(rotation=45)\n",
    "sns.barplot(x= 'material', y='ROI', data=df_sorted)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_blade_type = cardinality_threshold(df3['Blade_Type'], threshold=.96, return_categories_list=False)\n",
    "transformed_blade_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df3.copy()\n",
    "df_transformed['blade_type'] = transformed_blade_type\n",
    "df_sorted = df_transformed.sort_values('blade_type')\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.suptitle('blade type')\n",
    "plt.xticks(rotation=45)\n",
    "sns.barplot(x= 'blade_type', y='ROI', data=df_sorted)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_lock_type = cardinality_threshold(df3['Lock_Type'], threshold=.93, return_categories_list=False)\n",
    "transformed_lock_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df3.copy()\n",
    "df_transformed['lock_type'] = transformed_lock_type\n",
    "df_sorted = df_transformed.sort_values('lock_type')\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.suptitle('lock_type')\n",
    "plt.xticks(rotation=45)\n",
    "sns.barplot(x= 'lock_type', y='ROI', data=df_sorted)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_region = cardinality_threshold(df3['Region_of_Manufacture'], threshold=.99, return_categories_list=False)\n",
    "transformed_region.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df3.copy()\n",
    "df_transformed['Region_of_Manufacture'] = transformed_region\n",
    "df_sorted = df_transformed.sort_values('Region_of_Manufacture')\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.suptitle('Region_of_Manufacture')\n",
    "plt.xticks(rotation=45)\n",
    "sns.barplot(x= 'Region_of_Manufacture', y='ROI', data=df_sorted)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_opening = cardinality_threshold(df3['Opening_Mechanism'], threshold=.90, return_categories_list=False)\n",
    "transformed_opening.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df3.copy()\n",
    "df_transformed['opening_mechanism'] = transformed_opening\n",
    "df_sorted = df_transformed.sort_values('opening_mechanism')\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.suptitle('opening_mechanism')\n",
    "plt.xticks(rotation=45)\n",
    "sns.barplot(x= 'opening_mechanism', y='ROI', data=df_sorted)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_filtered = apply_iqr_filter(df3).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=3, ncols=2, sharex=True)\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'benchmade', 'converted_price']\n",
    "             , ax=axes[0][0], color='gold')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'buck', 'converted_price']\n",
    "             , ax=axes[1][0], color='firebrick')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'case', 'converted_price']\n",
    "             , ax=axes[2][0], color='skyblue')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'kershaw', 'converted_price']\n",
    "             , ax=axes[0][1], color='palegreen')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'sog', 'converted_price']\n",
    "             , ax=axes[1][1], color='lavender')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'spyderco', 'converted_price']\n",
    "             , ax=axes[2][1], color='coral')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n in range(6):\n",
    "    row = n%3\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=['benchmade',\n",
    "                   'buck',\n",
    "                   'case',\n",
    "                   'kershaw',\n",
    "                   'sog',\n",
    "                   'spyderco'], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "sns.histplot(df3_filtered['converted_price'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "sns.histplot(df3_filtered['profit'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "sns.histplot(df3_filtered['ROI'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=3, ncols=2, sharex=True)\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'benchmade', 'profit']\n",
    "             , ax=axes[0][0], color='gold')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'buck', 'profit']\n",
    "             , ax=axes[1][0], color='firebrick')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'case', 'profit']\n",
    "             , ax=axes[2][0], color='skyblue')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'kershaw', 'profit']\n",
    "             , ax=axes[0][1], color='palegreen')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'sog', 'profit']\n",
    "             , ax=axes[1][1], color='lavender')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'spyderco', 'profit']\n",
    "             , ax=axes[2][1], color='coral')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n in range(6):\n",
    "    row = n%3\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[row][col].set_xlabel('Profit(US Dollars)', fontsize=20)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Profit of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=['benchmade',\n",
    "                   'buck',\n",
    "                   'case',\n",
    "                   'kershaw',\n",
    "                   'sog',\n",
    "                   'spyderco'], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=3, ncols=2, sharex=True)\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'benchmade', 'ROI']\n",
    "             , ax=axes[0][0], color='gold')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'buck', 'ROI']\n",
    "             , ax=axes[1][0], color='firebrick')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'case', 'ROI']\n",
    "             , ax=axes[2][0], color='skyblue')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'kershaw', 'ROI']\n",
    "             , ax=axes[0][1], color='palegreen')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'sog', 'ROI']\n",
    "             , ax=axes[1][1], color='lavender')\n",
    "sns.histplot(df3_filtered.loc[df3['brand'] == 'spyderco', 'ROI']\n",
    "             , ax=axes[2][1], color='coral')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n in range(6):\n",
    "    row = n%3\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[row][col].set_xlabel('Return on Investment(% US Dollars)', fontsize=20)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Profit of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=['benchmade',\n",
    "                   'buck',\n",
    "                   'case',\n",
    "                   'kershaw',\n",
    "                   'sog',\n",
    "                   'spyderco'], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_filtered['PictureURL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_filtered.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "df3_filtered['PictureURL'] = df3_filtered['PictureURL'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df3_filtered.explode('PictureURL').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['file_index'] = df.index.values\n",
    "df['file_index'] = df['file_index'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['itemId'] = df['itemId'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'] = df['file_index'] + '_' + df['itemId'] + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images4/'\n",
    "df['filepath'] = root_folder + df['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filepath'].sample(10).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = os.listdir('nn_images4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.loc[df['filename'].isin(images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_final.sample(frac=0.8, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_final.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = df_train[['filepath', 'ROI']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2['ROI'] = (df_train2['ROI']/df_train2['ROI'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2 = df_test[['filepath', 'ROI']].copy()\n",
    "df_test2['ROI'] = (df_test2['ROI']/df_test2['ROI'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.,\n",
    "                           validation_split=0.25,\n",
    "                           rotation_range=90, \n",
    "                           horizontal_flip=True, \n",
    "                           width_shift_range=.2, \n",
    "                           height_shift_range=.2,\n",
    "                           fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train2,\n",
    "directory= None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"ROI\",\n",
    "subset=\"training\",\n",
    "batch_size=32,\n",
    "seed=55,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\")\n",
    "    \n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train2,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"ROI\",\n",
    "subset=\"validation\",\n",
    "batch_size=32,\n",
    "seed=55,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\")\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=df_test2,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col='ROI',\n",
    "batch_size=32,\n",
    "seed=55,\n",
    "shuffle=False,\n",
    "class_mode='raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "                        input_shape=(256 ,256,  3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer='Adam',\n",
    "               metrics=['mae'])\n",
    "\n",
    "summary = model.fit(train_generator, epochs=4, validation_data=valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=32,\n",
    "#                     batch_size=300,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "# STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "# STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "# model.fit_generator(generator=train_generator,\n",
    "#                     steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "#                     validation_data=valid_generator,\n",
    "#                     validation_steps=STEP_SIZE_VALID,\n",
    "#                     epochs=10\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator, epochs=10, validation_data=valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_files = []\n",
    "for filename in os.listdir('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images4'):\n",
    "    if filename.endswith('.jpg'):\n",
    "        try:\n",
    "            img = Image.open('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images4/'+filename)  # open the image file\n",
    "            img.verify()  # verify that it is, in fact an image\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(filename)\n",
    "            removed_files.append(filename)\n",
    "            os.remove('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images4/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = df.loc[df['filename'].isin(removed_files)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.8, random_state=111)\n",
    "df_test=df.drop(df_train.index)\n",
    "df_train2 = df_train[['filepath', 'ROI']].copy()\n",
    "df_train2['ROI'] = (df_train2['ROI']/df_train2['ROI'].mean())\n",
    "df_test2 = df_test[['filepath', 'ROI']].copy()\n",
    "df_test2['ROI'] = (df_test2['ROI']/df_test2['ROI'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train2,\n",
    "directory= None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"ROI\",\n",
    "subset=\"training\",\n",
    "batch_size=32,\n",
    "seed=55,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\")\n",
    "    \n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train2,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"ROI\",\n",
    "subset=\"validation\",\n",
    "batch_size=32,\n",
    "seed=55,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\")\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=df_test2,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col=None,\n",
    "batch_size=32,\n",
    "seed=55,\n",
    "shuffle=False,\n",
    "class_mode=\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=df_test2,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"ROI\",\n",
    "batch_size=32,\n",
    "seed=55,\n",
    "shuffle=False,\n",
    "class_mode=\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "                        input_shape=(256 ,256,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer='Adam',\n",
    "               metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model.fit(train_generator, epochs=10, validation_data=valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict(test_generator,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['ROI'].mean() * 0.7096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(summary.history['loss'])\n",
    "plt.plot(summary.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss(mean absolute error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test3 = df_test2.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test3.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2[27:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[27:30].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2[27:28]['ROI'].values * df_test['ROI'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[27:30].mean() * df_test['ROI'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('no_batch_norm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_absolute_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[16648:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(row):\n",
    "    filename = row.filepath\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.PictureURL\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    except:\n",
    "        print(f'{filename} error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[16648:].apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_type(line):\n",
    "#     pattern1 = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     pattern2 = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+\\s\\w+)\")\n",
    "    \n",
    "#     if re.findall(pattern2,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern2,str(line))[0]\n",
    "    \n",
    "#     elif re.findall(pattern1,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern1,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match\n",
    "\n",
    "# df2['blade_type'] = df2.ItemSpecifics.apply(extract_blade_type)\n",
    "# df2['blade_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_color(line):\n",
    "#     pattern1 = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     pattern2 = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+\\s\\w+)\")\n",
    "    \n",
    "#     if re.findall(pattern2,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern2,str(line))[0]\n",
    "        \n",
    "#     elif re.findall(pattern1,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern1,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match\n",
    "\n",
    "# df2['color'] = df2.ItemSpecifics.apply(extract_color)\n",
    "# df2['color'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_type(line):\n",
    "#     pattern1 = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     pattern2 = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+\\s\\w+)\")\n",
    "    \n",
    "#     if re.findall(pattern2,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern2,str(line))[0]\n",
    "        \n",
    "#     elif re.findall(pattern1,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern1,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match\n",
    "\n",
    "# df2['blade_type'] = df2.ItemSpecifics.apply(extract_blade_type)\n",
    "# df2['blade_type'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_manufacture_region(line):\n",
    "#     pattern = re.compile(\"Country/Region of Manufacture\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df2['region_of_Manufacture'] = df2.ItemSpecifics.apply(extract_manufacture_region)\n",
    "\n",
    "# df2['region_of_Manufacture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_handle_material(line):\n",
    "#     pattern1 = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     pattern2 = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+\\s\\w+)\")\n",
    "    \n",
    "#     if re.findall(pattern2,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern2,str(line))[0]\n",
    "        \n",
    "#     elif re.findall(pattern1,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern1,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match\n",
    "        \n",
    "# df2['handle_material'] = df2.ItemSpecifics.apply(extract_handle_material)\n",
    "\n",
    "# df2['handle_material'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_lock_type(line):\n",
    "#     pattern2 = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+\\s\\w+)\")\n",
    "#     pattern1 = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern2,str(line)):\n",
    "#         match = re.findall(pattern2,str(line))[0]\n",
    "        \n",
    "#     elif re.findall(pattern1,str(line)):\n",
    "#         match = re.findall(pattern1,str(line))[0]\n",
    "        \n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "\n",
    "# df2['lock_type'] = df2.ItemSpecifics.apply(extract_lock_type)\n",
    "\n",
    "# df2['lock_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_edge(line):\n",
    "#     pattern1 = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "# #     pattern2 = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+\\s\\w+)\")\n",
    "# #     if re.findall(pattern2,str(line)):\n",
    "# #         match = re.findall(pattern2,str(line))[0]\n",
    "        \n",
    "#     if re.findall(pattern1,str(line)):\n",
    "#         match = re.findall(pattern1,str(line))[0]\n",
    "        \n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match\n",
    "        \n",
    "# df2['blade_edge'] = df2.ItemSpecifics.apply(extract_blade_edge)\n",
    "\n",
    "# df2['blade_edge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_dexterity(line):\n",
    "#     pattern = re.compile(\"Dexterity\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df2['dexterity'] = df2.ItemSpecifics.apply(extract_dexterity)\n",
    "\n",
    "# df2['dexterity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_model(line):\n",
    "#     pattern1 = re.compile(\"Model\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+\\s\\w+)\")\n",
    "#     pattern2 = re.compile(\"Model\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+\\s\\w+\\s\\w+)\")\n",
    "#     pattern3 = re.compile(\"Model\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+\\s\\w+\\s\\w+\\s\\w+)\")\n",
    "#     pattern4 = re.compile(\"Model\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+\\s\\w+\\s\\w+\\s\\w+\\s\\w+)\")\n",
    "#     pattern5 = re.compile(\"Model\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "#     if re.findall(pattern4,str(line)):\n",
    "        \n",
    "#         match = re.findall(pattern4,str(line))[0]\n",
    "        \n",
    "#     elif re.findall(pattern3,str(line)):\n",
    "        \n",
    "#         match = re.findall(pattern3,str(line))[0]\n",
    "            \n",
    "#     elif re.findall(pattern2,str(line)):\n",
    "        \n",
    "#         match = re.findall(pattern2,str(line))[0]\n",
    "        \n",
    "#     elif re.findall(pattern1,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern1,str(line))[0]\n",
    "        \n",
    "#     elif re.findall(pattern1,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern1,str(line))[0]\n",
    "    \n",
    "#     elif re.findall(pattern5,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern5,str(line))[0]\n",
    "            \n",
    "            \n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match\n",
    "\n",
    "# df2['Model'] = df2.ItemSpecifics.apply(extract_model)\n",
    "# df2['Model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_csv('data/loaded_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv('data/loaded_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blade_types = df2.loc[df2['blade_type'] != 'NA', 'blade_type'].value_counts()[:10].index.tolist()\n",
    "\n",
    "# x = df2.loc[(df2['blade_type'].isin(blade_types)) &\n",
    "#             (df2['brand'] == 'spyderco'), 'blade_type']\n",
    "# y = df2.loc[(df2['blade_type'].isin(blade_types)) &\n",
    "#             (df2['brand'] == 'spyderco'), 'profit']\n",
    "# fig = plt.subplots(figsize=(18,8))\n",
    "# plt.xticks(rotation=45)\n",
    "# sns.barplot(x=x,y=y)\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2['color'] = df2['color'].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = df2.loc[df2['color'] != 'NA','color'].value_counts()[:20].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df2.loc[(df2['color'].isin(colors)) &\n",
    "#             (df2['brand'] == 'spyderco'), 'color']\n",
    "# y = df2.loc[(df2['color'].isin(colors)) &\n",
    "#             (df2['brand'] == 'spyderco'), 'profit']\n",
    "# fig = plt.subplots(figsize=(18,8))\n",
    "# plt.xticks(rotation=45)\n",
    "# sns.barplot(x=x,y=y)\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df2.loc[df2['color'].isin(colors), 'color']\n",
    "# y = df2.loc[df2['color'].isin(colors), 'profit']\n",
    "# fig = plt.subplots(figsize=(18,8))\n",
    "# plt.xticks(rotation=45)\n",
    "# sns.barplot(x=x,y=y)\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.loc[(df2['color'] == 'gold') &\n",
    "#         (df2['brand'] != 'buck'), 'url'].apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = df2.explode('PictureURL').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['index'] = df3.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_itemIds = df3['itemId'].apply(str).values\n",
    "# str_indx = df3['index'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['filename'] = str_indx + '_' + str_itemIds + '.jpg' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images3/'\n",
    "# df3['filepath'] = root_folder + df3['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['filepath'].sample(10).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download(row):\n",
    "#     filename = row.filepath\n",
    "\n",
    "# # create folder if it doesn't exist\n",
    "#     os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "#     url = row.PictureURL\n",
    "# #     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "#     try:\n",
    "#         r = requests.get(url, allow_redirects=True)\n",
    "#         with open(filename, 'wb') as f:\n",
    "#             f.write(r.content)\n",
    "#     except:\n",
    "#         print(f'{filename} error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('data/casespybench.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_used = df_filtered.merge(df, left_on='itemId', right_on='ItemID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_used.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.read_csv('data/tera_listed_exploded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('box only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['title'] = concat_df['title'].apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['is_box'] = concat_df['title'].apply(lambda x: re.search(pattern, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.drop(concat_df.loc[concat_df['is_box'].isnull() == False]['title'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('screws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['screws'] = concat_df['title'].apply(lambda x: re.search(pattern, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.drop(concat_df.loc[concat_df['screws'].isnull() == False]['title'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['PictureURL'].fillna(concat_df['Image'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = concat_df.loc[(concat_df['brand'] == 'victorinox') |\n",
    "              (concat_df['brand'] == 'leatherman') |\n",
    "              (concat_df['victorinox'].isna() == False) |\n",
    "              (concat_df['leatherman'].isna() == False)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready =concat_df.dropna(axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready['PictureURL'].head(3).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 100\n",
    "img_width = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPool2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = []\n",
    "Y_TRAIN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        try:\n",
    "            img_array = cv2.imread(str(path + filename))\n",
    "            new_array = cv2.resize(img_array, (img_height, img_width))\n",
    "            X_TRAIN.append(new_array)\n",
    "        except Exception as e:\n",
    "            print(f'error at {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv.imread(os.path.join(path, img))\n",
    "                new_array = cv.resize(img_array, (img_width, img_height))\n",
    "                X_TRAIN.append(new_array)\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = cv2.imread(os.path.join(path, '3.jpg'))\n",
    "new_array = cv2.resize(img_array, (img_height, img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        print(path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Interactive Plotting\n",
    "\n",
    "# # Import libraries\n",
    "# from matplotlib.widgets import LassoSelector\n",
    "# from matplotlib.path import Path as mplPath\n",
    "\n",
    "# # Lasso Selection of data points\n",
    "# class SelectFromCollection:\n",
    "#     def __init__(self, ax, collection, alpha_other=0.3):\n",
    "#         self.canvas = ax.figure.canvas\n",
    "#         self.collection = collection\n",
    "\n",
    "#         self.xys = collection.get_offsets()\n",
    "#         self.lasso = LassoSelector(ax, onselect=self.onselect)\n",
    "#         self.ind = []\n",
    "\n",
    "#     def onselect(self, verts):\n",
    "#         path = mplPath(verts)\n",
    "#         self.ind = np.nonzero(path.contains_points(self.xys))[0]\n",
    "#         self.canvas.draw_idle()\n",
    "\n",
    "#     def disconnect(self):\n",
    "#         self.canvas.draw_idle()\n",
    "\n",
    "# # Show the original image upon picking the point\n",
    "# def on_pick(event):\n",
    "#     ind = event.ind[0]\n",
    "#     w, h = points.get_offsets().data[ind]\n",
    "#     img_file = Path(img_meta_df.iloc[ind, 0])\n",
    "#     if Path(root,img_file).is_file():\n",
    "#         print(f\"Showing: {img_file}\")\n",
    "#         img = Image.open(Path(root,img_file))\n",
    "#         figs = plt.figure(figsize=(5, 5))\n",
    "#         axs = figs.add_subplot(111)\n",
    "#         axs.set_title(Path(img_file).name, size=14)\n",
    "#         axs.set_xticks([])\n",
    "#         axs.set_yticks([])\n",
    "#         axs.set_xlabel(f'Dim: {round(w)} x {round(h)}', size=14)\n",
    "#         axs.imshow(img)\n",
    "#         figs.tight_layout()\n",
    "#         figs.show()\n",
    "\n",
    "# # Save selected image filenames  \n",
    "# def save_selected_imgs(df, fileName = Path(\"Images to discard.csv\")):\n",
    "#     if fileName.is_file():\n",
    "#         orgData = pd.read_csv(fileName)\n",
    "#         df = pd.concat([orgData, df])\n",
    "#         df.set_axis(['FileName'], axis='columns').to_csv(fileName, index=False)\n",
    "\n",
    "# # Store selected points upon pressing \"enter\"\n",
    "# def accept(event):\n",
    "#     if event.key == \"enter\":\n",
    "#         selected_imgs = img_meta_df.iloc[selector.ind, 0].to_frame()\n",
    "#         save_selected_imgs(selected_imgs)\n",
    "#         print(\"Selected images:\")\n",
    "#         print(selected_imgs)\n",
    "#         selector.disconnect()\n",
    "#         fig.canvas.draw()\n",
    "        \n",
    "# # Plot the image resolutions        \n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "# ax.set_title(\"Press enter to after selecting the points.\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)\n",
    "\n",
    "# # Add interaction\n",
    "# selector = SelectFromCollection(ax, points)\n",
    "# fig.canvas.mpl_connect(\"key_press_event\", accept)\n",
    "# fig.canvas.mpl_connect('pick_event', on_pick)\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv.imread(os.path.join(path, img))\n",
    "                new_array = cv.resize(img_array, (img_width, img_height))\n",
    "                X_TRAIN.append(new_array)\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/full_dataset2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = df.loc[(df['victorinox'].isna() == False) |\n",
    "                 (df['leatherman'].isna() == False)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/concat_df.csv\",sep=',', error_bad_lines=False, index_col=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['victorinox', 'leatherman'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tera_df_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df = pd.read_csv('data/tera_df_prepared.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_iqr_filter(df):\n",
    "    \n",
    "    price_Q1 = df['converted_price'].quantile(0.25)\n",
    "    price_Q3 = df['converted_price'].quantile(0.75)\n",
    "    price_iqr = price_Q3 - price_Q1\n",
    "\n",
    "    profit_Q1 = df['profit'].quantile(0.25)\n",
    "    profit_Q3 = df['profit'].quantile(0.75)\n",
    "    profit_iqr = profit_Q3 - profit_Q1\n",
    "\n",
    "    ROI_Q1 = df['ROI'].quantile(0.25)\n",
    "    ROI_Q3 = df['ROI'].quantile(0.75)\n",
    "    ROI_iqr = ROI_Q3 - ROI_Q1\n",
    "\n",
    "    price_upper_limit = price_Q3 + (1.5 * price_iqr)\n",
    "    price_lower_limit = price_Q1 - (1.5 * price_iqr)\n",
    "\n",
    "    profit_upper_limit = profit_Q3 + (1.5 * profit_iqr)\n",
    "    profit_lower_limit = profit_Q1 - (1.5 * profit_iqr)\n",
    "\n",
    "    ROI_upper_limit = ROI_Q3 + (1.5 * ROI_iqr)\n",
    "    ROI_lower_limit = ROI_Q1 - (1.5 * ROI_iqr)\n",
    "    \n",
    "    print(f'price upper limit: ${np.round(price_upper_limit,2)}')\n",
    "    print(f'price lower limit: ${np.round(price_lower_limit,2)}')\n",
    "    print('-----------------------------------')\n",
    "    print(f'profit upper limit: ${np.round(profit_upper_limit,2)}')\n",
    "    print(f'profit lower limit: ${np.round(profit_lower_limit,2)}')\n",
    "    print('-----------------------------------')\n",
    "    print(f'ROI upper limit: {np.round(ROI_upper_limit,2)}%')\n",
    "    print(f'ROI lower limit: {np.round(ROI_lower_limit,2)}%')\n",
    "    print('-----------------------------------')\n",
    "\n",
    "    \n",
    "    new_df = df[(df['converted_price'] < price_upper_limit) &\n",
    "             (df['profit'] <profit_upper_limit) &\n",
    "             (df['ROI'] < ROI_upper_limit) &\n",
    "             (df['profit'] < profit_upper_limit) &\n",
    "             (df['ROI'] > ROI_lower_limit)]\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = tera_df.columns[3:8].values\n",
    "\n",
    "for column in num_columns:\n",
    "    tera_df[column] = tera_df[column].apply(lambda x: float(x))\n",
    "\n",
    "tera_df['cost'] = tera_df['cost'].apply(lambda x: float(x))\n",
    "\n",
    "tera_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([tera_df, df_merged_used])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_used.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_used.dropna(subset=['PictureURL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Image'].fillna(df3['PictureURL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['PictureURL'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = df3.loc[(df3['brand'] == 'leatherman') |\n",
    "                  (df3['brand'] == 'victorinox')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merg_filtered_df = apply_iqr_filter(df3).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merg_filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df = apply_iqr_filter(tera_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tera_df_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best = df[(df['brand'] == 'benchmade')| \n",
    "             (df['brand'] == 'buck')|\n",
    "             (df['brand'] == 'case') |\n",
    "             (df['brand'] == 'spyderco')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('data/full_dataset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used = df[df['condition'] != 1000.0].copy()\n",
    "df_new = df[df['condition'] == 1000.0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used_filtered = apply_iqr_filter(df_used).copy()\n",
    "df_new_filtered = apply_iqr_filter(df_new).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = apply_iqr_filter(df).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merg_filtered_df['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merg_filtered_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_image_list[39000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used_filtered[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_filtered[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used_filtered.reset_index(drop=True, inplace=True)\n",
    "df_new_filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df_used_filtered.info())\n",
    "display(df_new_filtered.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=4, sharey=True, sharex=True)\n",
    "sns.histplot(df_used_filtered['converted_price'], ax=axes[0], color='gold')\n",
    "sns.histplot(df_new_filtered['converted_price'], ax=axes[0], color='firebrick')\n",
    "sns.histplot(df_used_filtered['converted_price'], ax=axes[1], color='gold')\n",
    "sns.histplot(df_new_filtered['converted_price'], ax=axes[2], color='firebrick')\n",
    "sns.histplot(df_filtered['converted_price'], ax=axes[0])\n",
    "sns.histplot(df_filtered['converted_price'], ax=axes[3]),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"new\", \"used\", 'both'], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=3, sharey=True, sharex=True)\n",
    "sns.histplot(df_filtered['converted_price'], ax=axes[0], color='gold')\n",
    "sns.histplot(tera_df['converted_price'], ax=axes[0], color='firebrick')\n",
    "sns.histplot(df_filtered['converted_price'], ax=axes[1],color='gold')\n",
    "sns.histplot(tera_df['converted_price'], ax=axes[2], color='firebrick')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"listed\", 'sold'], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=4, sharey=True, sharex=True)\n",
    "sns.histplot(df_used_filtered['profit'], ax=axes[0], color='gold')\n",
    "sns.histplot(df_new_filtered['profit'], ax=axes[0], color='firebrick')\n",
    "sns.histplot(df_used_filtered['profit'], ax=axes[1], color='gold')\n",
    "sns.histplot(df_new_filtered['profit'], ax=axes[2], color='firebrick')\n",
    "sns.histplot(df_filtered['profit'], ax=axes[0])\n",
    "sns.histplot(df_filtered['profit'], ax=axes[3]),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"new\", \"used\", 'both'], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['brand'] = 'buck'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.loc[df_filtered['benchmade'].isna() == False, 'brand'] = 'benchmade'\n",
    "df_filtered.loc[df_filtered['buck'].isna() == False, 'brand'] = 'buck'\n",
    "df_filtered.loc[df_filtered['case'].isna() == False, 'brand'] = 'case'\n",
    "df_filtered.loc[df_filtered['crkt'].isna() == False, 'brand'] = 'crkt'\n",
    "df_filtered.loc[df_filtered['kershaw'].isna() == False, 'brand'] = 'kershaw'\n",
    "df_filtered.loc[df_filtered['spyderco'].isna() == False, 'brand'] = 'spyderco'\n",
    "df_filtered.loc[df_filtered['sog'].isna() == False, 'brand'] = 'sog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merg_filtered_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = df_filtered.columns[22:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.drop('benchmade', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = tera_df.loc[(tera_df['brand'] == 'victorinox') |\n",
    "                      (tera_df['brand'] == 'leatherman')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.concat([df_filtered, tera_df]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['Image'].fillna(df_filtered['galleryURL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=3, sharey=True, sharex=True)\n",
    "sns.histplot(df_filtered['profit'], ax=axes[0], color='gold')\n",
    "sns.histplot(tera_df['profit'], ax=axes[0], color='firebrick')\n",
    "sns.histplot(df_filtered['profit'], ax=axes[1],color='gold')\n",
    "sns.histplot(tera_df['profit'], ax=axes[2], color='firebrick')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"listed\", 'sold'], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=4, sharey=True, sharex=True)\n",
    "sns.histplot(total_df['converted_price'], ax=axes[0], color='aqua')\n",
    "sns.histplot(df_filtered['converted_price'], ax=axes[0], color='gold')\n",
    "sns.histplot(tera_df['converted_price'], ax=axes[0], color='firebrick')\n",
    "sns.histplot(df_filtered['converted_price'], ax=axes[1], color='gold')\n",
    "sns.histplot(tera_df['converted_price'], ax=axes[2], color='firebrick')\n",
    "sns.histplot(total_df['converted_price'], ax=axes[3], color='aqua')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"both\",\"listed\", \"sold\",], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=4, sharey=True, sharex=True)\n",
    "sns.histplot(total_df['profit'], ax=axes[0], color='aqua')\n",
    "sns.histplot(df_filtered['profit'], ax=axes[0], color='gold')\n",
    "sns.histplot(tera_df['profit'], ax=axes[0], color='firebrick')\n",
    "sns.histplot(df_filtered['profit'], ax=axes[1], color='gold')\n",
    "sns.histplot(tera_df['profit'], ax=axes[2], color='firebrick')\n",
    "sns.histplot(total_df['profit'], ax=axes[3], color='aqua')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"both\",\"listed\", \"sold\",], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=4, sharey=True, sharex=True)\n",
    "sns.histplot(total_df['ROI'], ax=axes[0], color='aqua')\n",
    "sns.histplot(df_filtered['ROI'], ax=axes[0], color='gold')\n",
    "sns.histplot(tera_df['ROI'], ax=axes[0], color='firebrick')\n",
    "sns.histplot(df_filtered['ROI'], ax=axes[1], color='gold')\n",
    "sns.histplot(tera_df['ROI'], ax=axes[2], color='firebrick')\n",
    "sns.histplot(total_df['ROI'], ax=axes[3], color='aqua')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"both\",\"listed\", \"sold\",], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15))\n",
    "sns.histplot(df_final['ROI'], color='aqua')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"both\",\"listed\", \"sold\",], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merg_filtered_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merg_filtered_df[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_filtered = apply_iqr_filter(total_df).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_filtered[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_filtered.to_csv('data/total_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_filtered['target'] = total_filtered['ROI']/total_filtered['ROI'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_filtered['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_filtered.dropna(subset=['Image'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = merg_filtered_df.dropna(subset=['Image']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready['Image'].tail().apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(row):\n",
    "    filename = os.path.join(root_folder, str(row.name) + im_extension)\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.Image\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    except:\n",
    "        print(f'{filename} error')\n",
    "\n",
    "root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6'\n",
    "im_extension = '.jpg'  # or whatever type of images you are downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merg_filtered_df.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_files = []\n",
    "for filename in os.listdir('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images5'):\n",
    "    if filename.endswith('.jpg'):\n",
    "        try:\n",
    "            img = Image.open('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images5/'+filename)  # open the image file\n",
    "            img.verify()  # verify that it is, in fact an image\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(filename)\n",
    "            removed_files.append(filename)\n",
    "            os.remove('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images5/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_filtered) - len(removed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = list(df_used_filtered[\"itemId\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = df_used_filtered.loc[(df_used_filtered['converted_price'] <= 60) & \n",
    "                     (df_used_filtered['benchmade'].isna() == False)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = to_drop.append(df_used_filtered.loc[(df_used_filtered['converted_price'] <= 9) & \n",
    "                     (df_used_filtered['buck'].isna() == False)]['title'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = to_drop.append(df_used_filtered.loc[(df_used_filtered['converted_price'] <= 10) & \n",
    "                     (df_used_filtered['case'].isna() == False)]['title'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_listed = df_listed[(df_listed['benchmade'].isna() == False) |\n",
    "                           (df_listed['case'].isna() == False) |\n",
    "                           (df_listed['spyderco'].isna() == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = df_used_filtered[\"itemId\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = item_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.shopping import Connection as Shopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import  json\n",
    "import numpy as np\n",
    "import re\n",
    "import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "                'ItemSpecifics', 'PictureURL']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_list(my_list):\n",
    " \n",
    "    api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "    request = {\n",
    "               'itemID': my_list,\n",
    "               'IncludeSelector': 'ItemSpecifics'\n",
    "              }\n",
    "    response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(itemIds), 20):\n",
    "    process_list(itemIds[i:i+20])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(item_ids), 20):\n",
    "    process_list(item_ids[i:i+20])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(full_dataset)\n",
    "\n",
    "df.drop_duplicates(subset='ItemID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ItemID'] = df['ItemID'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/itemSpecs_used.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/itemSpecs_used.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explode('PictureURL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_used_filtered, left_on='ItemID', right_on='itemId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='ItemID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df.explode('PictureURL').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat([df_exploded, tera_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.to_csv('data/tera_listed_exploded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/merged_item_specs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['crkt', 'sog', 'victorinox', 'kershaw', 'leatherman'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df.explode('PictureURL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.read_csv('data/tera_listed_exploded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "#x = position of bucket_dictionary\n",
    "def prepare_tera_df(df, x):\n",
    "    df['price'] = df['price'].str.replace(\"$\", \"\")\n",
    "    df['price'] = df['price'].str.replace(\",\", \"\")\n",
    "    df['price'] = df['price'].apply(float)\n",
    "    \n",
    "    df['shipping'] = df['shipping'].str.replace(\"$\", \"\")\n",
    "    df['shipping'] = df['shipping'].str.replace(\",\", \"\")\n",
    "    df['shipping'] = df['shipping'].apply(float)\n",
    "    \n",
    "    df['converted_price'] = (df['price'] + df['shipping'])\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - list(bucket_dict.values())[x])\n",
    "    df['ROI'] = (df['profit']/(list(bucket_dict.values())[x]))*100.0\n",
    "    \n",
    "    df['brand'] = list(bucket_dict.keys())[x]\n",
    "    df['cost'] = list(bucket_dict.values())[x]\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = prepare_tera_df(bench, 0).copy()\n",
    "buck = prepare_tera_df(buck, 1).copy() \n",
    "case = prepare_tera_df(case, 2).copy()\n",
    "kershaw = prepare_tera_df(kershaw, 4).copy()\n",
    "sog = prepare_tera_df(sog, 6).copy()\n",
    "spy = prepare_tera_df(spy, 7).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench2 = pd.read_csv(\"data/tera_benchmade.csv\")\n",
    "# # df_buck2 = pd.read_csv(\"data/tera_buck.csv\")\n",
    "# df_case2 = pd.read_csv(\"data/tera_case.csv\")\n",
    "# df_crkt2 = pd.read_csv(\"data/tera_CRKT.csv\")\n",
    "# df_kershaw2 = pd.read_csv(\"data/tera_kershaw.csv\")\n",
    "# df_leatherman2 = pd.read_csv(\"data/tera_leatherman.csv\")\n",
    "# df_sog = pd.read_csv(\"data/tera_SOG.csv\")\n",
    "# df_spyderco2 = pd.read_csv(\"data/tera_spyderco.csv\")\n",
    "# df_victorinox2 = pd.read_csv(\"data/tera_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tera_buck = pd.read_csv('data/tera_buckeroo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench2.rename({'Data_field':'title', \n",
    "#                   'Data_field1':'avg_price', \n",
    "#                   'Data_field2': 'avg_shipping', \n",
    "#                   'Field2': 'date_sold'\n",
    "#                  }, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench2.drop(['Field', 'Price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench2.to_csv('data/tera_benchmade_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_case2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_case2.rename({'url':'title', \n",
    "#                   'Field5':'title2', \n",
    "#                   'Field4': 'date_sold', \n",
    "#                  }, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_case2['title'].fillna(df_case2['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_case2.drop(['Field', 'units_sold', 'title2', 'Price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_case2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_case2.to_csv('data/tera_case_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tera_buck['title'].fillna(tera_buck['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tera_buck.drop('title2', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.rename({'Field2':'date_sold', \n",
    "#                  'title1': 'title', \n",
    "#                  'avg_cost':'avg_price'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2['title'].fillna(df_crkt2['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.drop(['Field', 'Price', 'title2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crkt2.to_csv('data/tera_crkt_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kershaw2.rename({'Field':'url', 'Field3':'date_sold', 'Field1': 'title'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kershaw2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kershaw2.drop(['Field2', 'Price', 'url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_kershaw2.to_csv('data/tera_kershaw_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2['title'].fillna(df_leatherman2['title1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.rename({'Field2':'date_sold'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.drop(['Field', 'Price', 'title1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_leatherman2.to_csv('data/tera_leatherman_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog.rename({'title1': 'title'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog['title'].fillna(df_sog['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog.drop(['Field', 'Price', 'title2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sog.to_csv('data/tera_sog_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.rename({'Field3': 'title', 'Field2': 'date_sold'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2['title'].fillna(df_spyderco2['name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.drop(['units_sold', 'Price', 'name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spyderco2.to_csv('data/tera_spyderco_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.rename({'FfImage':'Image', 'Field3':'date_sold'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2['title'].fillna(df_victorinox2['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.drop(['url','Field2', 'Price', 'title2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_victorinox2.to_csv('data/tera_victorinox_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "#x = position of bucket_dictionary\n",
    "def prepare_tera_df(df, x):\n",
    "    df['avg_price'] = df['avg_price'].str.replace(\"$\", \"\")\n",
    "    df['avg_price'] = df['avg_price'].str.replace(\",\", \"\")\n",
    "    df['avg_price'] = df['avg_price'].apply(float)\n",
    "    \n",
    "    df['avg_shipping'] = df['avg_shipping'].str.replace(\"$\", \"\")\n",
    "    df['avg_shipping'] = df['avg_shipping'].str.replace(\",\", \"\")\n",
    "    df['avg_shipping'] = df['avg_shipping'].apply(float)\n",
    "    \n",
    "    df['converted_price'] = (df['avg_price'] + df['avg_shipping'])\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - list(bucket_dict.values())[x])\n",
    "    df['ROI'] = (df['profit']/(list(bucket_dict.values())[x]))*100.0\n",
    "    \n",
    "    df['brand'] = list(bucket_dict.keys())[x]\n",
    "    df['cost'] = list(bucket_dict.values())[x]\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_bench = prepare_tera_df(df_bench2,0)\n",
    "tera_case = prepare_tera_df(df_case2,2)\n",
    "tera_crkt = prepare_tera_df(df_crkt2, 3)\n",
    "tera_kershaw = prepare_tera_df(df_kershaw2, 4)\n",
    "tera_leatherman = prepare_tera_df(df_leatherman2, 5)\n",
    "tera_sog = prepare_tera_df(df_sog, 6)\n",
    "tera_spyderco = prepare_tera_df(df_spyderco2, 7)\n",
    "tera_victorinox = prepare_tera_df(df_victorinox2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_buck = prepare_tera_df(tera_buck, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([tera_bench, tera_buck, tera_case, tera_crkt,\n",
    "                tera_kershaw, tera_leatherman,\n",
    "                tera_sog, tera_spyderco, tera_victorinox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Field2','Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_bench.to_csv('data/tera_bench_prepared.csv', index=False)\n",
    "tera_case.to_csv('data/tera_case_prepared.csv', index=False)\n",
    "tera_crkt.to_csv('data/tera_crkt_prepared.csv', index=False)\n",
    "tera_kershaw.to_csv('data/tera_kershaw_prepared.csv', index=False)\n",
    "tera_leatherman.to_csv('data/tera_leatherman_prepared.csv', index=False)\n",
    "tera_sog.to_csv('data/tera_sog_prepared.csv', index=False)\n",
    "tera_spyderco.to_csv('data/tera_spyderco_prepared.csv', index=False)\n",
    "tera_victorinox.to_csv('data/tera_victorinox_prepared.csv', index=False)\n",
    "\n",
    "df.to_csv('data/tera_df_prepared.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_bench = pd.read_csv('data/tera_bench_prepared.csv')\n",
    "tera_buck = pd.read_csv('data/tera_buckeroo.csv')\n",
    "tera_case = pd.read_csv('data/tera_case_prepared.csv')\n",
    "tera_crkt = pd.read_csv('data/tera_crkt_prepared.csv')\n",
    "tera_kershaw = pd.read_csv('data/tera_kershaw_prepared.csv')\n",
    "tera_leatherman = pd.read_csv('data/tera_leatherman_prepared.csv')\n",
    "tera_sog = pd.read_csv('data/tera_sog_prepared.csv')\n",
    "tera_spyderco = pd.read_csv('data/tera_spyderco_prepared.csv')\n",
    "tera_victorinox = pd.read_csv('data/tera_victorinox_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tera_df_prepared.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.brand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench2 = pd.read_csv(\"data/benchmade_scraped.csv\")\n",
    "# df_buck2 = pd.read_csv(\"data/buck_scraped.csv\")\n",
    "# df_case2 = pd.read_csv(\"data/case_scraped.csv\")\n",
    "# df_crkt2 = pd.read_csv(\"data/CRKT_scraped.csv\")\n",
    "# df_kershaw2 = pd.read_csv(\"data/kershaw_scraped.csv\")\n",
    "# df_spyderco2 = pd.read_csv(\"data/spyderco_scraped.csv\")\n",
    "# df_victorinox2 = pd.read_csv(\"data/victorinox_scraped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brands = ['victorinox', 'kershaw', 'buck', \n",
    "#           'case xx', 'benchmade', 'spyderco',\n",
    "#           'crkt', 'case']\n",
    "\n",
    "# df_buck2.brand = df_buck2.brand.str.lower()\n",
    "\n",
    "# df_buck2 = df_buck2[df_buck2.brand.isin(brands)]\n",
    "\n",
    "# df_buck2.brand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_buck2['buck'] = 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brands = ['victorinox', 'kershaw', 'buck', \n",
    "#           'case xx', 'benchmade', 'spyderco',\n",
    "#           'crkt', 'case']\n",
    "\n",
    "# def prepare_brands2(df, bucket_dict_position):\n",
    "\n",
    "#     df.brand = df.brand.str.lower()\n",
    "    \n",
    "#     df = df[df.brand.isin(brands)].copy()\n",
    "\n",
    "#     df[str(list(bucket_dict.keys())[bucket_dict_position])] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    \n",
    "#     df['profit'] = (df['price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "#     df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No duplicates\n",
    "df.title.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " display(pd.concat(g for _, g in df.groupby(\"title\") if len(g) > 1).tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " display(pd.concat(g for _, g in df.groupby(\"title\") if len(g) > 1).tail(20).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3-_pAWYVbtA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('Benchmade Value Counts')\n",
    "# display(df.benchmade.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Buck Value Counts')\n",
    "# display(df.buck.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Case Value Counts')\n",
    "# display(df.case.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('CRKT Value Counts')\n",
    "# display(df.crkt.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Kershaw Value Counts')\n",
    "# display(df.kershaw.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Leatherman Value Counts')\n",
    "# display(df.leatherman.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "\n",
    "# print('Spyderco Value Counts')\n",
    "# display(df.spyderco.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "\n",
    "# print('Victorinox Value Counts')\n",
    "# display(df.victorinox.value_counts(normalize=False))\n",
    "# print('-------------------------------------------')\n",
    "# print('')\n",
    "\n",
    "# display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench = pd.read_csv(\"data/df_bench.csv\")\n",
    "# df_buck = pd.read_csv(\"data/df_buck.csv\")\n",
    "# df_case = pd.read_csv(\"data/df_case.csv\")\n",
    "# df_crkt = pd.read_csv(\"data/df_crkt.csv\")\n",
    "# df_kershaw = pd.read_csv(\"data/df_kershaw.csv\")\n",
    "# df_leatherman = pd.read_csv(\"data/df_leatherman.csv\")\n",
    "# df_spyderco = pd.read_csv(\"data/df_spyderco.csv\")\n",
    "# df_victorinox = pd.read_csv(\"data/df_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'df_bench length: {len(df_bench)}')\n",
    "# print(f'df_buck length: {len(df_buck)}')\n",
    "# print(f'df_case length: {len(df_case)}')\n",
    "# print(f'df_crkt length: {len(df_crkt)}')\n",
    "# print(f'df_kershaw length: {len(df_kershaw)}')\n",
    "# print(f'df_spyderco length: {len(df_spyderco)}')\n",
    "# print(f'df_victorinox length: {len(df_victorinox)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['shippingInfo', 'listingInfo', 'sellingStatus'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket_dict = {'benchmade': 45.0,\n",
    "#                'buck': 20.0,\n",
    "#                'case': 20.0,\n",
    "#                'crkt': 15.0,\n",
    "#                'kershaw': 15.0,\n",
    "#                'leatherman': 30.0, \n",
    "#                'spyderco': 30.0,\n",
    "#                'victorinox': 20.0\n",
    "#               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #flip the bucket dictionary to divide dataframe by MY cost of knives\n",
    "# #at the surplus store\n",
    "\n",
    "# flipped = {}\n",
    "  \n",
    "# for key, value in bucket_dict.items():\n",
    "#     if value not in flipped:\n",
    "#         flipped[value] = [key]\n",
    "#     else:\n",
    "#         flipped[value].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are only 4 bins to work with. Much easier. \n",
    "# flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_15 = df.loc[(df['crkt'] != 0) | (df['kershaw'] != 0)]\n",
    "# df_20 = df.loc[(df['buck'] != 0) | (df['case'] != 0)| (df['victorinox'] != 0)]\n",
    "# df_30 = df.loc[(df['leatherman'] != 0) | (df['spyderco'] != 0)]\n",
    "# df_45 = df.loc[df['benchmade'] != 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = df.columns[3:8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in num_columns:\n",
    "    df[column] = df[column].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cost'] = df['cost'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_45 = df.loc[df['brand'] == 'benchmade']\n",
    "df_30 = df.loc[(df['brand'] == 'leatherman') | (df['brand'] == 'spyderco')]\n",
    "df_20 = df.loc[(df['brand'] == 'buck') | (df['brand'] == 'case') | (df['brand'] == 'victorinox')]\n",
    "df_15 = df.loc[(df['brand'] == 'crkt') | (df['brand'] == 'kershaw') | (df['brand'] == 'sog')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_45['brand'].value_counts())\n",
    "display(df_30['brand'].value_counts()) \n",
    "display(df_20['brand'].value_counts())\n",
    "display(df_15['brand'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram for each column\n",
    "df['converted_price'].hist(figsize = (18, 15), bins = 'auto');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create histogram for each column\n",
    "# df['profit'].hist(figsize = (18, 15), bins = 'auto');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['converted_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_15.info())\n",
    "display(df_20.info())\n",
    "display(df_30.info())\n",
    "display(df_45.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_iqr_filter2(df):\n",
    "    \n",
    "#     price_Q1 = df['price'].quantile(0.25)\n",
    "#     price_Q3 = df['price'].quantile(0.75)\n",
    "#     price_iqr = price_Q3 - price_Q1\n",
    "\n",
    "#     profit_Q1 = df['profit'].quantile(0.25)\n",
    "#     profit_Q3 = df['profit'].quantile(0.75)\n",
    "#     profit_iqr = profit_Q3 - profit_Q1\n",
    "\n",
    "#     ROI_Q1 = df['ROI'].quantile(0.25)\n",
    "#     ROI_Q3 = df['ROI'].quantile(0.75)\n",
    "#     ROI_iqr = ROI_Q3 - ROI_Q1\n",
    "\n",
    "#     price_upper_limit = price_Q3 + (1.5 * price_iqr)\n",
    "#     price_lower_limit = price_Q1 - (1.5 * price_iqr)\n",
    "\n",
    "#     profit_upper_limit = profit_Q3 + (1.5 * profit_iqr)\n",
    "#     profit_lower_limit = profit_Q1 - (1.5 * profit_iqr)\n",
    "\n",
    "#     ROI_upper_limit = ROI_Q3 + (1.5 * ROI_iqr)\n",
    "#     ROI_lower_limit = ROI_Q1 - (1.5 * ROI_iqr)\n",
    "    \n",
    "#     print(f'price upper limit: ${np.round(price_upper_limit,2)}')\n",
    "#     print(f'price lower limit: ${np.round(price_lower_limit,2)}')\n",
    "#     print('-----------------------------------')\n",
    "#     print(f'profit upper limit: ${np.round(profit_upper_limit,2)}')\n",
    "#     print(f'profit lower limit: ${np.round(profit_lower_limit,2)}')\n",
    "#     print('-----------------------------------')\n",
    "#     print(f'ROI upper limit: {np.round(ROI_upper_limit,2)}%')\n",
    "#     print(f'ROI lower limit: {np.round(ROI_lower_limit,2)}%')\n",
    "#     print('-----------------------------------')\n",
    "\n",
    "    \n",
    "#     new_df = df[(df['price'] < price_upper_limit) &\n",
    "#              (df['profit'] <profit_upper_limit) &\n",
    "#              (df['ROI'] < ROI_upper_limit) &\n",
    "#              (df['profit'] < profit_upper_limit) &\n",
    "#              (df['ROI'] > ROI_lower_limit)]\n",
    "    \n",
    "#     return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_bench = apply_iqr_filter(tera_bench)\n",
    "tera_buck = apply_iqr_filter(tera_bench)\n",
    "tera_case =  apply_iqr_filter(tera_case)\n",
    "tera_crkt = apply_iqr_filter(tera_crkt)\n",
    "tera_kershaw = apply_iqr_filter(tera_kershaw)\n",
    "tera_leatherman = apply_iqr_filter(tera_leatherman)\n",
    "tera_sog = apply_iqr_filter(tera_sog)\n",
    "tera_spyderco = apply_iqr_filter(tera_spyderco)\n",
    "tera_victorinox = apply_iqr_filter(tera_victorinox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15 = apply_iqr_filter(df_15)\n",
    "df_20 = apply_iqr_filter(df_20)\n",
    "df_30 = apply_iqr_filter(df_30)\n",
    "df_45 = apply_iqr_filter(df_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = apply_iqr_filter(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"data/merged_item_specs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder2 = df2.loc[df2['spyderco'].isna()==False].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder2.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder2.dropna(subset=['PictureURL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder2['PictureURL'] = df_spyder2['PictureURL'].apply(literal_eval) #convert to list type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder = df_spyder2.explode('PictureURL').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder2 = apply_iqr_filter(df_spyder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench2 = df2.loc[df2['benchmade'].isna()==False].copy()\n",
    "df_bench2.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_bench2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench2 = apply_iqr_filter(df_bench2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_bench2['converted_price']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_bench2['profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_bench2['ROI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench2['PictureURL'] = df_bench2['PictureURL'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = df_bench2.explode('PictureURL').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15 = concat_df.loc[(concat_df['crkt'].isna() == False) |\n",
    "                      (concat_df['brand'] == 'crkt') |\n",
    "                      (concat_df['kershaw'].isna() == False) | \n",
    "                      (concat_df['brand'] == 'kershaw') |\n",
    "                      (concat_df['sog'].isna() == False) |\n",
    "                     (concat_df['brand'] == 'sog')]\n",
    "df_20 = concat_df.loc[(concat_df['buck'].isna() == False) |\n",
    "                      (concat_df['brand'] == 'buck') |\n",
    "                      (concat_df['case'].isna() == False) | \n",
    "                      (concat_df['brand'] == 'case') |\n",
    "                      (concat_df['victorinox'].isna() == False) | \n",
    "                      (concat_df['brand'] == 'victorinox')]\n",
    "df_30 = concat_df.loc[(concat_df['leatherman'].isna() == False) |\n",
    "                      (concat_df['brand'] == 'leatherman') |\n",
    "                      (concat_df['spyderco'].isna() == False) | \n",
    "                     (concat_df['brand'] == 'spyderco')]\n",
    "df_45 = concat_df.loc[(concat_df['brand'] == 'benchmade') | (concat_df['benchmade'].isna() == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_45 = concat_df.loc[concat_df['benchmade'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=2, sharey=True, sharex=True)\n",
    "sns.histplot(df_new['converted_price'], ax=axes[0], color='gold')\n",
    "sns.histplot(df_used['converted_price'], ax=axes[1], color='firebrick')\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"new\", \"used\"], loc=(.77, .74), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_num_cats(df, brand):\n",
    "    display(df[df[brand].isna() == False][['converted_price', 'profit', 'ROI']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used[df_used['benchmade'] != 0][['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=2, sharey=True, sharex=True)\n",
    "sns.histplot(df_15['converted_price'], ax=axes[0], color='gold')\n",
    "sns.histplot(df_20['converted_price'], ax=axes[1], color='firebrick')\n",
    "\n",
    "\n",
    "for n in range(2):\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"$15 knives\", \"$20 knives\", \"$30 knives\", \"$45 knives\"], loc=(.77, .84), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=2, sharey=True, sharex=True)\n",
    "sns.histplot(df_30['converted_price'], ax=axes[0],  color='skyblue')\n",
    "sns.histplot(df_45['converted_price'], ax=axes[1], color='palegreen')\n",
    "\n",
    "for n in range(2):\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"$30 knives\", \"$45 knives\"], loc=(.77, .84), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=2, nrows=2, sharey=True)\n",
    "sns.histplot(df_15['converted_price'], ax=axes[0][0], color='gold')\n",
    "sns.histplot(df_20['converted_price'], ax=axes[0][1], color='firebrick')\n",
    "sns.histplot(df_30['converted_price'], ax=axes[1][0],  color='skyblue')\n",
    "sns.histplot(df_45['converted_price'], ax=axes[1][1], color='palegreen')\n",
    "\n",
    "for n in range(4):\n",
    "    row = n//2 # n divided by 2 without the remainder\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"$15 knives\", \"$20 knives\", \"$30 knives\", \"$45 knives\"], loc=(.77, .84), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=2, nrows=2, sharey=True, sharex=True)\n",
    "sns.histplot(df_15['converted_price'], ax=axes[0][0], color='gold')\n",
    "sns.histplot(df_20['converted_price'], ax=axes[0][1], color='firebrick')\n",
    "sns.histplot(df_30['converted_price'], ax=axes[1][0],  color='skyblue')\n",
    "sns.histplot(df_45['converted_price'], ax=axes[1][1], color='palegreen')\n",
    "\n",
    "for n in range(4):\n",
    "    row = n//2 # n divided by 2 without the remainder\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"$15 knives\", \"$20 knives\", \"$30 knives\", \"$45 knives\"], loc=(.77, .84), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=2, nrows=2, sharey=True)\n",
    "sns.histplot(df_15['profit'], ax=axes[0][0], color='gold', label=\"$15 Knives\")\n",
    "sns.histplot(df_20['profit'], ax=axes[0][1], color='firebrick',  label=\"$20 Knives\")\n",
    "sns.histplot(df_30['profit'], ax=axes[1][0],  color='skyblue',  label=\"$30 Knives\")\n",
    "sns.histplot(df_45['profit'], ax=axes[1][1], color='palegreen', label=\"$45 Knives\")\n",
    "\n",
    "for n in range(4):\n",
    "    row = n//2 # n divided by 2 without the remainder\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[row][col].set_xlabel('Profit (US Dollars)', fontsize=20)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    axes[row][col].axvline(x = 0, color = 'black')\n",
    "\n",
    "axes[0][0].axvline(x = 0, color = 'black', label= 'Break Even Point')\n",
    "fig.suptitle(\"Expected Profit of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.77, .80), fontsize='large')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=2, nrows=2, sharey=True)\n",
    "sns.histplot(df_15['ROI'], ax=axes[0][0], color='gold', label=\"$15 Knives\")\n",
    "sns.histplot(df_20['ROI'], ax=axes[0][1], color='firebrick',  label=\"$20 Knives\")\n",
    "sns.histplot(df_30['ROI'], ax=axes[1][0],  color='skyblue',  label=\"$30 Knives\")\n",
    "sns.histplot(df_45['ROI'], ax=axes[1][1], color='palegreen', label=\"$45 Knives\")\n",
    "\n",
    "for n in range(4):\n",
    "    row = n//2 # n divided by 2 without the remainder\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[row][col].set_xlabel('ROI (Percent US Dollars)', fontsize=20)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    axes[row][col].axvline(x = 0, color = 'black')\n",
    "\n",
    "axes[0][0].axvline(x = 0, color = 'black', label= 'Break Even Point')\n",
    "fig.suptitle(\"Expected Return On Investment of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.77, .80), fontsize='large')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20_new = pd.concat([tera_buck, tera_case])\n",
    "df_30_new = tera_spyderco.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=2, nrows=2, sharey=True)\n",
    "sns.histplot(df_15['converted_price'], ax=axes[0][0], color='gold')\n",
    "sns.histplot(df_20_new['converted_price'], ax=axes[0][1], color='firebrick')\n",
    "sns.histplot(df_30_new['converted_price'], ax=axes[1][0],  color='skyblue')\n",
    "sns.histplot(df_45['converted_price'], ax=axes[1][1], color='palegreen')\n",
    "\n",
    "for n in range(4):\n",
    "    row = n//2 # n divided by 2 without the remainder\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=20)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(labels=[\"$15 knives\", \"$20 knives\", \"$30 knives\", \"$45 knives\"], loc=(.77, .84), fontsize='x-large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=2, nrows=2, sharey=True)\n",
    "sns.histplot(df_15['profit'], ax=axes[0][0], color='gold', label=\"$15 Knives\")\n",
    "sns.histplot(df_20_new['profit'], ax=axes[0][1], color='firebrick',  label=\"$20 Knives\")\n",
    "sns.histplot(df_30_new['profit'], ax=axes[1][0],  color='skyblue',  label=\"$30 Knives\")\n",
    "sns.histplot(df_45['profit'], ax=axes[1][1], color='palegreen', label=\"$45 Knives\")\n",
    "\n",
    "for n in range(4):\n",
    "    row = n//2 # n divided by 2 without the remainder\n",
    "    col = n%2  # just the remainder of n divided by 2\n",
    "    axes[row][col].set_xlabel('Profit (US Dollars)', fontsize=20)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    axes[row][col].axvline(x = 0, color = 'black')\n",
    "\n",
    "axes[0][0].axvline(x = 0, color = 'black', label= 'Break Even Point')\n",
    "fig.suptitle(\"Expected Profit of Surplus Store Knives by Price\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.77, .80), fontsize='large')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tera_bench = df.loc[df['brand'] == 'benchmade']\n",
    "# tera_buck = df.loc[df['brand'] == 'buck']\n",
    "# tera_case = df.loc[df['brand'] == 'case']\n",
    "# tera_crkt = df.loc[df['brand'] == 'crkt']\n",
    "# tera_kershaw = df.loc[df['brand'] == 'kershaw']\n",
    "# tera_leatherman = df.loc[df['brand'] == 'leatherman']\n",
    "# tera_sog = df.loc[df['brand'] == 'sog']\n",
    "# tera_spyderco = df.loc[df['brand'] == 'spyderco']\n",
    "# tera_victorinox = df.loc[df['brand'] == 'victorinox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench2 = df2.loc[df['benchmade'] != 0]\n",
    "# df_buck2 = df2.loc[df['buck'] != 0]\n",
    "# df_case2 = df2.loc[df['case'] != 0]\n",
    "# df_crkt2 = df2.loc[df['crkt'] != 0]\n",
    "# df_kershaw2 = df2.loc[df['kershaw'] != 0]\n",
    "# df_spyderco2 = df2.loc[df['spyderco'] != 0]\n",
    "# df_victorinox2 = df2.loc[df['victorinox'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=3, nrows=3, sharey=True)\n",
    "sns.histplot(tera_bench['converted_price'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "sns.histplot(tera_buck['converted_price'], ax=axes[0][1], color='red', label='Brand: Buck')\n",
    "sns.histplot(tera_case['converted_price'], ax=axes[0][2], color='chocolate', label='Brand: Case XX')\n",
    "sns.histplot(tera_crkt['converted_price'], ax=axes[1][0],  color='khaki', label='Brand: CRKT')\n",
    "sns.histplot(tera_kershaw['converted_price'], ax=axes[1][1], color='palegreen', label='Brand: Kershaw')\n",
    "sns.histplot(tera_leatherman['converted_price'], ax=axes[1][2], color='turquoise', label='Brand: Leatherman')\n",
    "sns.histplot(tera_sog['converted_price'], ax=axes[2][0], color='firebrick', label='Brand: Sog')\n",
    "sns.histplot(tera_spyderco['converted_price'], ax=axes[2][1],  color='plum', label='Brand: Spyderco')\n",
    "sns.histplot(tera_victorinox['converted_price'], ax=axes[2][2], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "for n in range(9):\n",
    "    row = n//3 \n",
    "    col = n%3 \n",
    "    axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=15)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    \n",
    "axes[0][0].axvline(x = 45, color = 'black', label= 'Cost at Surplus Store')\n",
    "axes[0][1].axvline(x = 20, color = 'black') \n",
    "axes[0][2].axvline(x = 20, color = 'black')\n",
    "axes[1][0].axvline(x = 15, color = 'black')\n",
    "axes[1][1].axvline(x = 15, color = 'black', label= 'Cost at Surplus Store')\n",
    "axes[1][2].axvline(x = 30, color = 'black') \n",
    "axes[2][0].axvline(x = 15, color = 'black')\n",
    "axes[2][1].axvline(x = 30, color = 'black')\n",
    "axes[2][2].axvline(x = 20, color = 'black', label= 'Cost at Surplus Store')\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.8, .75), fontsize='large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=3, sharey=True)\n",
    "sns.histplot(tera_bench['converted_price'], ax=axes[0], color='lightcoral', label='listed price on ebay')\n",
    "sns.histplot(tera_buck['profit'], ax=axes[1], color='red', label='profit on ebay')\n",
    "sns.histplot(tera_case['ROI'], ax=axes[2], color='chocolate', label='ROI')\n",
    "\n",
    "\n",
    "# for n in range(9):\n",
    "#     row = n//3 \n",
    "#     col = n%3 \n",
    "#     axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=15)\n",
    "#     axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "#     axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    \n",
    "# axes[0][0].axvline(x = 45, color = 'black', label= 'Cost at Surplus Store')\n",
    "# axes[0][1].axvline(x = 20, color = 'black') \n",
    "# axes[0][2].axvline(x = 20, color = 'black')\n",
    "# axes[1][0].axvline(x = 15, color = 'black')\n",
    "# axes[1][1].axvline(x = 15, color = 'black', label= 'Cost at Surplus Store')\n",
    "# axes[1][2].axvline(x = 30, color = 'black') \n",
    "# axes[2][0].axvline(x = 15, color = 'black')\n",
    "# axes[2][1].axvline(x = 30, color = 'black')\n",
    "# axes[2][2].axvline(x = 20, color = 'black', label= 'Cost at Surplus Store')\n",
    "\n",
    "fig.suptitle(\"Resale Value of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.8, .75), fontsize='large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[['converted_price','profit','ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_filtered.loc[df_filtered['ROI'] >= 80])/len(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['binary_target'] = (df_filtered['ROI'] >= 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['binary_target'].replace({True:1,False:0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes  = plt.subplots(figsize=(15,15), ncols=4, nrows=2, sharey=True)\n",
    "# sns.histplot(df_bench['converted_price'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "# sns.histplot(df_buck['converted_price'], ax=axes[0][1], color='chocolate', label='Brand: Buck')\n",
    "# sns.histplot(df_case['converted_price'], ax=axes[0][2],  color='khaki', label='Brand: Case XX')\n",
    "# sns.histplot(df_crkt['converted_price'], ax=axes[0][3], color='palegreen', label='Brand: CRKT')\n",
    "# sns.histplot(df_kershaw['converted_price'], ax=axes[1][0], color='turquoise', label='Brand: Kershaw')\n",
    "# sns.histplot(df_leatherman['converted_price'], ax=axes[1][1], color='firebrick', label='Brand: Leatherman')\n",
    "# sns.histplot(df_spyderco['converted_price'], ax=axes[1][2],  color='plum', label='Brand: Spyderco')\n",
    "# sns.histplot(df_victorinox['converted_price'], ax=axes[1][3], color='mediumblue', label='Brand: Victorinox')\n",
    "# sns.\n",
    "\n",
    "# for n in range(8):\n",
    "#     row = n//4 \n",
    "#     col = n%4 \n",
    "#     axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=15)\n",
    "#     axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "#     axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    \n",
    "# axes[0][0].axvline(x = 45, color = 'black', label= 'Cost at Surplus Store')\n",
    "# axes[0][1].axvline(x = 20, color = 'black') \n",
    "# axes[0][2].axvline(x = 20, color = 'black')\n",
    "# axes[0][3].axvline(x = 15, color = 'black')\n",
    "# fig.suptitle(\"Resale Value of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "# fig.legend(loc=(.8, .80), fontsize='large')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes  = plt.subplots(figsize=(15,15), ncols=4, nrows=2, sharey=True)\n",
    "# sns.histplot(df_bench2['price'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "# sns.histplot(df_buck2['price'], ax=axes[0][1], color='chocolate', label='Brand: Buck')\n",
    "# sns.histplot(df_case2['price'], ax=axes[0][2],  color='khaki', label='Brand: Case XX')\n",
    "# sns.histplot(df_crkt2['price'], ax=axes[0][3], color='palegreen', label='Brand: CRKT')\n",
    "# sns.histplot(df_kershaw2['price'], ax=axes[1][0], color='turquoise', label='Brand: Kershaw')\n",
    "# sns.histplot(df_spyderco2['price'], ax=axes[1][2],  color='plum', label='Brand: Spyderco')\n",
    "# sns.histplot(df_victorinox2['price'], ax=axes[1][3], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "# for n in range(8):\n",
    "#     row = n//4 \n",
    "#     col = n%4 \n",
    "#     axes[row][col].set_xlabel('Resale Value(US Dollars)', fontsize=15)\n",
    "#     axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "#     axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    \n",
    "# axes[0][0].axvline(x = 45, color = 'black', label= 'Cost at Surplus Store')\n",
    "# axes[0][1].axvline(x = 20, color = 'black') \n",
    "# axes[0][2].axvline(x = 20, color = 'black')\n",
    "# axes[0][3].axvline(x = 15, color = 'black')\n",
    "# fig.suptitle(\"Resale Value of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "# fig.legend(loc=(.8, .80), fontsize='large')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best distributions look like Case XX and Spyderco. Should look more closely. The higher counts will make the listed prices closer to true value and will also help with training the model. A larger sample size also seems to help normalize the price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=3, nrows=3, sharey=True)\n",
    "sns.histplot(tera_bench['profit'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "sns.histplot(tera_buck['profit'], ax=axes[0][1], color='blue', label='Brand: Buck')\n",
    "sns.histplot(tera_case['profit'], ax=axes[0][2], color='chocolate', label='Brand: Case XX')\n",
    "sns.histplot(tera_crkt['profit'], ax=axes[1][0],  color='khaki', label='Brand: CRKT')\n",
    "sns.histplot(tera_kershaw['profit'], ax=axes[1][1], color='palegreen', label='Brand: Kershaw')\n",
    "sns.histplot(tera_leatherman['profit'], ax=axes[1][2], color='turquoise', label='Brand: Leatherman')\n",
    "sns.histplot(tera_sog['profit'], ax=axes[2][0], color='firebrick', label='Brand: Sog')\n",
    "sns.histplot(tera_spyderco['profit'], ax=axes[2][1],  color='plum', label='Brand: Spyderco')\n",
    "sns.histplot(tera_victorinox['profit'], ax=axes[2][2], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "for n in range(9):\n",
    "    row = n//3 \n",
    "    col = n%3 \n",
    "    axes[row][col].set_xlabel('Expected Profit (US Dollars)', fontsize=15)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    axes[row][col].axvline(x = 0, color = 'black')\n",
    "    \n",
    "axes[0][0].axvline(x = 0, color = 'black', label= 'Break Even Point')\n",
    "fig.suptitle(\"Expected Profit of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.82, .80), fontsize='large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=3, nrows=3, sharey=True, sharex=True)\n",
    "sns.histplot(tera_bench['profit'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "sns.histplot(tera_buck['profit'], ax=axes[0][1], color='blue', label='Brand: Buck')\n",
    "sns.histplot(tera_case['profit'], ax=axes[0][2], color='chocolate', label='Brand: Case XX')\n",
    "sns.histplot(tera_crkt['profit'], ax=axes[1][0],  color='khaki', label='Brand: CRKT')\n",
    "sns.histplot(tera_kershaw['profit'], ax=axes[1][1], color='palegreen', label='Brand: Kershaw')\n",
    "sns.histplot(tera_leatherman['profit'], ax=axes[1][2], color='turquoise', label='Brand: Leatherman')\n",
    "sns.histplot(tera_sog['profit'], ax=axes[2][0], color='firebrick', label='Brand: Sog')\n",
    "sns.histplot(tera_spyderco['profit'], ax=axes[2][1],  color='plum', label='Brand: Spyderco')\n",
    "sns.histplot(tera_victorinox['profit'], ax=axes[2][2], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "for n in range(9):\n",
    "    row = n//3 \n",
    "    col = n%3 \n",
    "    axes[row][col].set_xlabel('Expected Profit (US Dollars)', fontsize=15)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    axes[row][col].axvline(x = 0, color = 'black')\n",
    "    \n",
    "axes[0][0].axvline(x = 0, color = 'black', label= 'Break Even Point')\n",
    "fig.suptitle(\"Expected Profit of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.82, .80), fontsize='large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([tera_bench, tera_buck, tera_case, tera_sog, tera_spyderco])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=3, sharey=True)\n",
    "sns.histplot(df_new['converted_price'], ax=axes[0], color='lightcoral', label='Brand: Benchmade')\n",
    "sns.histplot(df_new['profit'], ax=axes[1], color='blue', label='Brand: Buck')\n",
    "sns.histplot(df_new['ROI'], ax=axes[2], color='chocolate', label='Brand: Case XX');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cutoff = df_new['converted_price'].mean() + (2*df_new['converted_price'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new2 = df_new.loc[df_new['converted_price'] < new_cutoff].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), nrows=3, sharey=True)\n",
    "sns.histplot(df_new2['converted_price'], ax=axes[0], color='lightcoral', label='Brand: Benchmade')\n",
    "sns.histplot(df_new2['profit'], ax=axes[1], color='blue', label='Brand: Buck')\n",
    "sns.histplot(df_new2['ROI'], ax=axes[2], color='chocolate', label='Brand: Case XX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes  = plt.subplots(figsize=(15,15), ncols=4, nrows=2, sharey=True)\n",
    "# sns.histplot(tera_bench2['converted_price'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "# sns.histplot(tera_case2['converted_price'], ax=axes[0][1], color='chocolate', label='Brand: Case XX')\n",
    "# sns.histplot(tera_crkt2['converted_price'], ax=axes[0][2],  color='khaki', label='Brand: CRKT')\n",
    "# sns.histplot(tera_kershaw2['converted_price'], ax=axes[0][3], color='palegreen', label='Brand: Kershaw')\n",
    "# sns.histplot(tera_leatherman2['converted_price'], ax=axes[1][0], color='turquoise', label='Brand: Leatherman')\n",
    "# sns.histplot(tera_sog2['converted_price'], ax=axes[1][1], color='firebrick', label='Brand: Sog')\n",
    "# sns.histplot(tera_spyderco2['converted_price'], ax=axes[1][2],  color='plum', label='Brand: Spyderco')\n",
    "# sns.histplot(tera_victorinox2['converted_price'], ax=axes[1][3], color='mediumblue', label='Brand: Victorinox')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes  = plt.subplots(figsize=(15,15), ncols=4, nrows=2, sharey=True)\n",
    "# sns.histplot(df_bench['profit'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "# sns.histplot(df_buck['profit'], ax=axes[0][1], color='chocolate', label='Brand: Buck')\n",
    "# sns.histplot(df_case['profit'], ax=axes[0][2],  color='khaki', label='Brand: Case XX')\n",
    "# sns.histplot(df_crkt['profit'], ax=axes[0][3], color='palegreen', label='Brand: CRKT')\n",
    "# sns.histplot(df_kershaw['profit'], ax=axes[1][0], color='turquoise', label='Brand: Kershaw')\n",
    "# sns.histplot(df_leatherman['profit'], ax=axes[1][1], color='firebrick', label='Brand: Leatherman')\n",
    "# sns.histplot(df_spyderco['profit'], ax=axes[1][2],  color='plum', label='Brand: Spyderco')\n",
    "# sns.histplot(df_victorinox['profit'], ax=axes[1][3], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "# for n in range(8):\n",
    "#     row = n//4 \n",
    "#     col = n%4 \n",
    "#     axes[row][col].set_xlabel('Expected Profit (US Dollars)', fontsize=15)\n",
    "#     axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "#     axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "#     axes[row][col].axvline(x = 0, color = 'black')\n",
    "    \n",
    "# axes[0][0].axvline(x = 0, color = 'black', label= 'Break Even Point')\n",
    "# fig.suptitle(\"Expected Profit of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "# fig.legend(loc=(.82, .80), fontsize='large')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes  = plt.subplots(figsize=(15,15), ncols=4, nrows=2, sharey=True)\n",
    "# sns.histplot(df_bench2['profit'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "# sns.histplot(df_buck2['profit'], ax=axes[0][1], color='chocolate', label='Brand: Buck')\n",
    "# sns.histplot(df_case2['profit'], ax=axes[0][2],  color='khaki', label='Brand: Case XX')\n",
    "# sns.histplot(df_crkt2['profit'], ax=axes[0][3], color='palegreen', label='Brand: CRKT')\n",
    "# sns.histplot(df_kershaw2['profit'], ax=axes[1][0], color='turquoise', label='Brand: Kershaw')\n",
    "\n",
    "# sns.histplot(df_spyderco2['profit'], ax=axes[1][2],  color='plum', label='Brand: Spyderco')\n",
    "# sns.histplot(df_victorinox2['profit'], ax=axes[1][3], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "# for n in range(8):\n",
    "#     row = n//4 \n",
    "#     col = n%4 \n",
    "#     axes[row][col].set_xlabel('Expected Profit (US Dollars)', fontsize=15)\n",
    "#     axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "#     axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "#     axes[row][col].axvline(x = 0, color = 'black')\n",
    "    \n",
    "# axes[0][0].axvline(x = 0, color = 'black', label= 'Break Even Point')\n",
    "# fig.suptitle(\"Expected Profit of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "# fig.legend(loc=(.82, .80), fontsize='large')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes  = plt.subplots(figsize=(15,15), ncols=4, nrows=2, sharey=True)\n",
    "# sns.histplot(df_bench['ROI'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "# sns.histplot(df_buck['ROI'], ax=axes[0][1], color='chocolate', label='Brand: Buck')\n",
    "# sns.histplot(df_case['ROI'], ax=axes[0][2],  color='khaki', label='Brand: Case XX')\n",
    "# sns.histplot(df_crkt['ROI'], ax=axes[0][3], color='palegreen', label='Brand: CRKT')\n",
    "# sns.histplot(df_kershaw['ROI'], ax=axes[1][0], color='turquoise', label='Brand: Kershaw')\n",
    "# sns.histplot(df_leatherman['ROI'], ax=axes[1][1], color='firebrick', label='Brand: Leatherman')\n",
    "# sns.histplot(df_spyderco['ROI'], ax=axes[1][2],  color='plum', label='Brand: Spyderco')\n",
    "# sns.histplot(df_victorinox['ROI'], ax=axes[1][3], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "# for n in range(8):\n",
    "#     row = n//4 \n",
    "#     col = n%4 \n",
    "#     axes[row][col].set_xlabel('Expected ROI(Percent US Dollars)', fontsize=12)\n",
    "#     axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "#     axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "#     axes[row][col].axvline(x = 0, color = 'black')\n",
    "    \n",
    "# axes[0][0].axvline(x = 0, color = 'black', label= 'Break Even Point')\n",
    "# fig.suptitle(\"Expected Return On Investment of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "# fig.legend(loc=(.82, .80), fontsize='large')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=3, nrows=3, sharey=True)\n",
    "sns.histplot(tera_bench['ROI'], ax=axes[0][0], color='lightcoral', label='Brand: Benchmade')\n",
    "sns.histplot(tera_buck['ROI'], ax=axes[0][1], color='blue', label='Brand: Buck')\n",
    "sns.histplot(tera_case['ROI'], ax=axes[0][2], color='chocolate', label='Brand: Case XX')\n",
    "sns.histplot(tera_crkt['ROI'], ax=axes[1][0],  color='khaki', label='Brand: CRKT')\n",
    "sns.histplot(tera_kershaw['ROI'], ax=axes[1][1], color='palegreen', label='Brand: Kershaw')\n",
    "sns.histplot(tera_leatherman['ROI'], ax=axes[1][2], color='turquoise', label='Brand: Leatherman')\n",
    "sns.histplot(tera_sog['ROI'], ax=axes[2][0], color='firebrick', label='Brand: Sog')\n",
    "sns.histplot(tera_spyderco['ROI'], ax=axes[2][1],  color='plum', label='Brand: Spyderco')\n",
    "sns.histplot(tera_victorinox['ROI'], ax=axes[2][2], color='mediumblue', label='Brand: Victorinox')\n",
    "\n",
    "\n",
    "\n",
    "for n in range(9):\n",
    "    row = n//3\n",
    "    col = n%3 \n",
    "    axes[row][col].set_xlabel('Expected ROI(Percent US Dollars)', fontsize=12)\n",
    "    axes[row][col].set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    axes[row][col].axvline(x = 0, color = 'black')\n",
    "    \n",
    "axes[0][0].axvline(x = 0, color = 'black', label= 'Break Even Point')\n",
    "fig.suptitle(\"Expected Return On Investment of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.82, .80), fontsize='large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_spyderco[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder2[['converted_price','profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder[['converted_price','profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder2.loc[df_spyder2['condition'] != 1000.0][['converted_price','profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_spyder['converted_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyder.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_spyderco.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat([tera_spyderco, df_spyder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.to_csv('data/concat_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['profit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concat_df.loc[concat_df['profit'] > 70])/len(concat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['target'] = (concat_df['profit'] > 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['target'].replace({True: 1, False:0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.to_csv('data/concat_df_extra.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat([tera_bench, df_bench])\n",
    "concat_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['PictureURL'].fillna(concat_df['Image'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.to_csv('data/concat_df_bench.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tera_spyderco['target'] = (tera_spyderco['profit'] > 52)\n",
    "# tera_spyderco['target'].replace({True: 1, False:0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tera_spyderco.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tera_spyderco.to_csv('data_spyder_ready.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes  = plt.subplots(figsize=(15,15), ncols=3, nrows=3, sharey=True, sharex=True)\n",
    "sns.boxplot(x=tera_bench['ROI'], ax=axes[0][0], color='lightcoral')\n",
    "sns.boxplot(x=tera_buck['ROI'], ax=axes[0][1], color='blue')\n",
    "sns.boxplot(x=tera_case['ROI'], ax=axes[0][2], color='chocolate')\n",
    "sns.boxplot(x=tera_crkt['ROI'], ax=axes[1][0],  color='khaki')\n",
    "sns.boxplot(x=tera_kershaw['ROI'], ax=axes[1][1], color='palegreen')\n",
    "sns.boxplot(x=tera_leatherman['ROI'], ax=axes[1][2], color='turquoise')\n",
    "sns.boxplot(x=tera_sog['ROI'], ax=axes[2][0], color='firebrick')\n",
    "sns.boxplot(x=tera_spyderco['ROI'], ax=axes[2][1],  color='plum')\n",
    "sns.boxplot(x=tera_victorinox['ROI'], ax=axes[2][2], color='mediumblue')\n",
    "\n",
    "\n",
    "for n in range(9):\n",
    "    row = n//3 \n",
    "    col = n%3 \n",
    "    axes[row][col].set_xlabel('ROI(percent US Dollars)', fontsize=15)\n",
    "    axes[row][col].tick_params(axis='both', labelsize=13)\n",
    "    axes[row][col].axvline(x = 0, color = 'black')\n",
    "    \n",
    "axes[0][0].axvline(x = 0, color = 'black', label= 'Break Even Point')\n",
    "fig.suptitle(\"Expected Return On Investment of Surplus Store Knives by Brand\", fontsize=24, x=0.44)\n",
    "fig.legend(loc=(.82, .80), fontsize='large')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_bench[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_buck[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_case[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_spyderco[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_crkt[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_sog[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_kershaw[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_leatherman[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_victorinox[['converted_price', 'profit', 'ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco['profit'].min() - df_case['profit'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco['profit'].max() - df_case['profit'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco['converted_price'].max() - df_case['converted_price'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case['profit'].mean() + 2 *df_case['profit'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco['profit'].mean() + 2 *df_spyderco['profit'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_case['profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_spyderco['profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_case['ROI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_spyderco['ROI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case['viewItemURL'].sample(10).apply(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smaller standard deviation for case and boxplot and distrubtion plot show less variance in mean profit and ROI returns. Better pick for better returns. However, must see which knife has better patterns to capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I was able to get data for nearly 6000 spyderco knives for a fixed price listing on ebay. I was able to get 7000 Case XX brand knives for a fixed price listing on ebay. At the surplus store, Spyderco knives cost 30 dollars, while Case XX knives cost 20 dollars. That is only a 10 dollar difference. However, the mean price of the listed spydercos is about 25 dollars higher. That means even though I would have to pay 10 dollars more to get a spyderco at the store, they have a higher probability of generating higher profit margins and higher returns. As you can see from the charts above, just risking ten dollars more leads to a higher profit ceiling of 89 dollars more. The mean return on investment values are fairly close, at 218.76 for case knives and about 200 mean ROI for spyderco. I feel that both knives would really do well from a business standpoint. However, Spyderco knives also have much more distinctive features and branding patterns for my model to catch when building my CNN. Therefore, for the first iteration of my project I will make a binary classification for spyderco knives, and expand first to Case XX knives and then other knives when I can gather additional data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After reviewing the distrubtions above, it seems that benchmades and case knives might be the most apporpiate targets. With a new determined target of ROI to account for buying a large number of cheap knives or a small number of expensive knives for max profit, I am going to filter the data again to remove anything outside of the range  -1.96 and +1.96 z-score for ROI (this would mean there would be less than a 5% chance of ever seeing those knives listed on ebay... and probably a much lower probability they would ever end up at the surplus store. Therefore, the data is irrelevant and can be removed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JK WENT WITH IQR RANGES FOR PRICE PROFIT AND ROI FROM RAW DF. THIS REMOVED 10.95% of outlier data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_df['PictureURL'].fillna(concat_df['Image'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['PictureURL'].fillna(concat_df['Image'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(row):\n",
    "    filename = os.path.join(root_folder, str(row.name) + im_extension)\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.PictureURL\n",
    "    print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    except:\n",
    "        print(f'{filename} error')\n",
    "\n",
    "root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images4'\n",
    "im_extension = '.jpg'  # or whatever type of images you are downloading\n",
    "\n",
    "concat_df.loc[concat_df['PictureURL'].isna() == False].apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_df.loc[concat_df['viewItemURL'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download2(row):\n",
    "    filename = os.path.join(root_folder, str(row.name) + im_extension)\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.Image\n",
    "    print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    except:\n",
    "        print(f'{filename} error')\n",
    "\n",
    "root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images4'\n",
    "im_extension = '.jpg'  # or whatever type of images you are downloading\n",
    "\n",
    "concat_df.loc[concat_df['Image'].isna() == False].apply(download2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "\n",
    "\n",
    "# def download(row):\n",
    "#     filename = os.path.join(root_folder, str(row.name) + im_extension)\n",
    "\n",
    "# # create folder if it doesn't exist\n",
    "#     os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "#     url = row.PictureURL\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "#     try:\n",
    "#         r = requests.get(url, allow_redirects=True)\n",
    "#         with open(filename, 'wb') as f:\n",
    "#             f.write(r.content)\n",
    "#     except:\n",
    "#         print(f'{filename} error')\n",
    "\n",
    "# root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/extra_spyders'\n",
    "# im_extension = '.jpg'  # or whatever type of images you are downloading\n",
    "\n",
    "# concat_df.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "\n",
    "\n",
    "# def download(row):\n",
    "#     filename = os.path.join(root_folder, str(row.name) + im_extension)\n",
    "\n",
    "# # create folder if it doesn't exist\n",
    "#     os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "#     url = row.Image\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "#     try:\n",
    "#         r = requests.get(url, allow_redirects=True)\n",
    "#         with open(filename, 'wb') as f:\n",
    "#             f.write(r.content)\n",
    "#     except:\n",
    "#         print(f'{filename} error')\n",
    "\n",
    "# root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images3'\n",
    "# im_extension = '.jpg'  # or whatever type of images you are downloading\n",
    "\n",
    "# # df.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "\n",
    "\n",
    "# def download(row):\n",
    "#     filename = os.path.join(root_folder, str(row.name) + im_extension)\n",
    "\n",
    "# # create folder if it doesn't exist\n",
    "#     os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "#     url = row.Image\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "#     try:\n",
    "#         r = requests.get(url, allow_redirects=True)\n",
    "#         with open(filename, 'wb') as f:\n",
    "#             f.write(r.content)\n",
    "#     except:\n",
    "#         print(f'{filename} error')\n",
    "\n",
    "# root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/spyders'\n",
    "# im_extension = '.jpg'  # or whatever type of images you are downloading\n",
    "\n",
    "# tera_spyderco.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "\n",
    "\n",
    "# def download(row):\n",
    "#     filename = os.path.join(root_folder, str(row.name) + im_extension)\n",
    "\n",
    "# # create folder if it doesn't exist\n",
    "#     os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "#     url = row.Image\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "#     r = requests.get(url, allow_redirects=True)\n",
    "#     with open(filename, 'wb') as f:\n",
    "#         f.write(r.content)\n",
    "\n",
    "# root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images'\n",
    "# im_extension = '.jpg'  # or whatever type of images you are downloading\n",
    "\n",
    "# df.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "img_array = cv2.imread('nn_images/105.jpg')  # convert to array\n",
    "\n",
    "img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "plt.imshow(img_rgb)  # graph it\n",
    "plt.show();\n",
    "\n",
    "\n",
    "def image_checker(index):\n",
    "    img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "    plt.imshow(img_rgb)  # graph it\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['ROI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['converted_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(1, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final processing steps for images\n",
    "\n",
    "image_list = []\n",
    "for x in range(len(df)):\n",
    "    img_array = cv2.imread('nn_images/'+str(x)+'.jpg')  # convert to array\n",
    "    try:\n",
    "        img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "        img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "        image_list.append(img_rgb)\n",
    "    except: \n",
    "        print('exception error has occured at ' + 'nn_images/'+str(x)+'.jpg')\n",
    "   \n",
    "    # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [1864,1887,6660,10837,17409,18351,19350,21949,21953,21965,23616,25521,26513,28055]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=104)# Create the Test and Final Training Datasets\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=104)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small batch\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "                        input_shape=(256 ,256, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='Adam',\n",
    "               metrics=['mse'])\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_Q1 = df['converted_price'].quantile(0.25)\n",
    "# price_Q3 = df['converted_price'].quantile(0.75)\n",
    "# price_iqr = price_Q3 - price_Q1\n",
    "\n",
    "# profit_Q1 = df['profit'].quantile(0.25)\n",
    "# profit_Q3 = df['profit'].quantile(0.75)\n",
    "# profit_iqr = profit_Q3 - profit_Q1\n",
    "\n",
    "# ROI_Q1 = df['ROI'].quantile(0.25)\n",
    "# ROI_Q3 = df['ROI'].quantile(0.75)\n",
    "# ROI_iqr = ROI_Q3 - ROI_Q1\n",
    "\n",
    "\n",
    "# price_upper_limit = price_Q3 + (1.5 * price_iqr)\n",
    "# price_lower_limit = price_Q1 - (1.5 * price_iqr)\n",
    "\n",
    "# profit_upper_limit = profit_Q3 + (1.5 * profit_iqr)\n",
    "# profit_lower_limit = profit_Q1 - (1.5 * profit_iqr)\n",
    "\n",
    "# ROI_upper_limit = ROI_Q3 + (1.5 * ROI_iqr)\n",
    "# ROI_lower_limit = ROI_Q1 - (1.5 * ROI_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'price upper limit: ${np.round(price_upper_limit,2)}')\n",
    "# print(f'price lower limit: ${np.round(price_lower_limit,2)}')\n",
    "# print('-----------------------------------')\n",
    "# print(f'profit upper limit: ${np.round(profit_upper_limit,2)}')\n",
    "# print(f'profit lower limit: ${np.round(profit_lower_limit,2)}')\n",
    "# print('-----------------------------------')\n",
    "# print(f'ROI upper limit: {np.round(ROI_upper_limit,2)}%')\n",
    "# print(f'ROI lower limit: {np.round(ROI_lower_limit,2)}%')\n",
    "# print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = df[(df['converted_price'] < price_upper_limit) &\n",
    "#              (df['profit'] <profit_upper_limit) &\n",
    "#              (df['ROI'] < ROI_upper_limit) &\n",
    "#              (df['profit'] < profit_upper_limit) &\n",
    "#              (df['ROI'] > ROI_lower_limit)]\n",
    "             \n",
    "# new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (len(new_df) - len(df))/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_benchmade2 = new_df.loc[new_df['benchmade'] != 0]\n",
    "# df_buck2 = new_df.loc[new_df['buck'] != 0]\n",
    "# df_case2 = new_df.loc[new_df['case'] != 0]\n",
    "# df_crkt2 = new_df.loc[new_df['crkt'] != 0]\n",
    "# df_kershaw2 = new_df.loc[new_df['kershaw'] != 0]\n",
    "# df_leatherman2 = new_df.loc[new_df['leatherman'] != 0]\n",
    "# df_spyderco2 = new_df.loc[new_df['spyderco'] != 0]\n",
    "# df_victorinox2 = new_df.loc[new_df['victorinox'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(12,8))\n",
    "# sns.histplot(df_benchmade2['ROI'], ax=ax, color='lime')\n",
    "\n",
    "\n",
    "# fig.suptitle(\"Expected ROI(as percent dollars invested) By Brand for $45 Knives\", fontsize=24, x=0.44)\n",
    "# ax.set_xlabel('Return On Investment (% US Dollars)', fontsize=20)\n",
    "# ax.set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "# # Add a legend to the figure\n",
    "# # (in general, these are quite nitpicky to style and position)\n",
    "# fig.legend(labels=[\"benchmade\"], loc=(.68, .78), fontsize='x-large')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_benchmade = ROI_df.loc[ROI_df['benchmade'] != 0]\n",
    "# df_buck = ROI_df.loc[ROI_df['buck'] != 0]\n",
    "# df_case = ROI_df.loc[ROI_df['case'] != 0]\n",
    "# df_crkt = ROI_df.loc[ROI_df['crkt'] != 0]\n",
    "# df_kershaw = ROI_df.loc[ROI_df['kershaw'] != 0]\n",
    "# df_leatherman = ROI_df.loc[ROI_df['leatherman'] != 0]\n",
    "# df_spyderco = ROI_df.loc[ROI_df['spyderco'] != 0]\n",
    "# df_victorinox = ROI_df.loc[ROI_df['victorinox'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(figsize=(10,12), nrows=2)\n",
    "# sns.histplot(df_crkt['ROI'], ax=ax1, color='lime')\n",
    "# sns.histplot(df_kershaw['ROI'], ax=ax2, color='aqua')\n",
    "# fig.suptitle(\"Expected ROI(as percent dollars invested) By Brand for $15 Knives\", fontsize=24, x=0.44)\n",
    "# ax1.set_xlabel('Return on Investment(% US Dollars)', fontsize=18)\n",
    "# ax1.set_ylabel('number of knives for sale', fontsize=18)\n",
    "# ax2.set_xlabel('Return on Investment(% US Dollars)', fontsize=18)\n",
    "# ax2.set_ylabel('number of knives for sale', fontsize=18)\n",
    "\n",
    "# # Add a legend to the figure\n",
    "# # (in general, these are quite nitpicky to style and position)\n",
    "# fig.legend(labels=[\"crkt\", \"kershaw\"], loc=(.68, .78), fontsize='x-large')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(figsize=(10,12), nrows=2)\n",
    "# sns.boxplot(x=df_crkt['ROI'], ax=ax1, color='lime')\n",
    "# sns.boxplot(x=df_kershaw['ROI'], ax=ax2, color='aqua') \n",
    "\n",
    "\n",
    "# fig.suptitle(\"Expected ROI(as percent dollars invested) By Brand for $15 Knives\", fontsize=24, x=0.44)\n",
    "# ax1.set_xlabel('Return on Investment(% US Dollars)', fontsize=18)\n",
    "# ax1.set_ylabel('number of knives for sale', fontsize=18)\n",
    "# ax2.set_xlabel('Return on Investment(% US Dollars)', fontsize=18)\n",
    "# ax2.set_ylabel('number of knives for sale', fontsize=18)\n",
    "\n",
    "# # Add a legend to the figure\n",
    "# # (in general, these are quite nitpicky to style and position)\n",
    "# fig.legend(labels=[\"crkt\", \"kershaw\"], loc=(.68, .78), fontsize='x-large')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(12,8))\n",
    "# sns.histplot(df_benchmade['ROI'], ax=ax, color='lime')\n",
    "\n",
    "\n",
    "# fig.suptitle(\"Expected ROI(as percent dollars invested) By Brand for $45 Knives\", fontsize=24, x=0.44)\n",
    "# ax1.set_xlabel('Return On Investment (% US Dollars)', fontsize=20)\n",
    "# ax1.set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "# ax2.set_xlabel('Return On Investment (% US Dollars)', fontsize=20)\n",
    "# ax2.set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "\n",
    "\n",
    "# # Add a legend to the figure\n",
    "# # (in general, these are quite nitpicky to style and position)\n",
    "# fig.legend(labels=[\"benchmade\"], loc=(.68, .78), fontsize='x-large')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(12,8))\n",
    "# sns.boxplot(df_benchmade['ROI'], ax=ax, color='lime')\n",
    "\n",
    "\n",
    "# fig.suptitle(\"Expected ROI(as percent dollars invested) By Brand for $45 Knives\", fontsize=24, x=0.44)\n",
    "# ax1.set_xlabel('Return On Investment (% US Dollars)', fontsize=20)\n",
    "# ax1.set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "# ax2.set_xlabel('Return On Investment (% US Dollars)', fontsize=20)\n",
    "# ax2.set_ylabel('number of knives listed (ebay)', fontsize=15)\n",
    "\n",
    "\n",
    "# # Add a legend to the figure\n",
    "# # (in general, these are quite nitpicky to style and position)\n",
    "# fig.legend(labels=[\"benchmade\"], loc=(.68, .78), fontsize='x-large')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #final processing steps for images\n",
    "\n",
    "# image_list = []\n",
    "# for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "#     img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "#     img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "#     img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "#     image_list.append(img_rgb)\n",
    "   \n",
    "#     # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_case.loc[df_case['ROI'] >= df_case['ROI'].mean()])/len(df_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_case.loc[df_case['ROI'] >= 188])/len(df_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case['ROI'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_spyderco.loc[df_spyderco['ROI'] >= df_spyderco['ROI'].mean()])/len(df_spyderco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_spyderco.loc[df_spyderco['ROI'] >= 168])/len(df_spyderco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case.loc[df_case['ROI'] >= 188, 'profit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco.loc[df_spyderco['ROI'] >= 168, \"profit\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case.loc[df_case['ROI'] >= 188, 'ROI'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_spyderco['ROI'] >= 168).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco['target'].replace(True,1.0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco['target'].replace(False,0.0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_df = df_spyderco.loc[df_spyderco['target'] == 1.0]\n",
    "dont_buy_df = df_spyderco.loc[df_spyderco['target'] == 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_buy_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_buy_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both knives are very similar from a business standpoint. Spyderco knives have higher profits, but the slightly higher cost of the knives (10 dollars more each) make the return on the invesment even out to be around the same as the case knives. Choosing to optimize for either knife would be a great bet, as long as the model can find distintcive features and there isnt alot of \"trash\" data, ie. pictures that do not show images of knives. Even though I was careful only to use the ebay API to get the URL for the images, it seems there were still alot of images of actual knife cases that snuck through for Case XX knives that are hard to deal with. I believe not enough to ruin the distrubtions, but perhaps enough to throw off my learning rates a bit when training my model. Therefore, I am going to go with spyderco knives. Also, with first hand experience at the store, the spyderco bucket always seemed to have a wider selection than the case knives and there seemed to be more chinese fake \"case\" knives that might trick my model. So based on the statistics and a bit of domain understanding, I choose my target of Spyderco knives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def download(row):\n",
    "    filename = os.path.join(root_folder,\n",
    "                           str(row.name) + im_extension)\n",
    "\n",
    "    # create folder if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.galleryURL\n",
    "    print(f\"Downloading {url} to {filename}\")\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "root_folder = 'data/not_buy/'\n",
    "im_extension = '.jpg'  # or whatever type of images you are downloading\n",
    "\n",
    "dont_buy_df.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "\n",
    "\n",
    "# def download(row):\n",
    "#     filename = os.path.join(root_folder,\n",
    "#                            str(row.name) + im_extension)\n",
    "\n",
    "#     # create folder if it doesn't exist\n",
    "#     os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "#     url = row.galleryURL\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "#     r = requests.get(url, allow_redirects=True)\n",
    "#     with open(filename, 'wb') as f:\n",
    "#         f.write(r.content)\n",
    "\n",
    "# root_folder = 'data/buy/'\n",
    "# im_extension = '.jpg'  # or whatever type of images you are downloading\n",
    "\n",
    "# buy_df.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_buy_dir = 'data/buy'\n",
    "data_not_buy_dir = 'data/not_buy'\n",
    "new_dir = 'split'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `os.listdir()` to create an object that stores all the relevant image names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_buy = [file for file in os.listdir(data_buy_dir) if file.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def sorted_nicely(image_list):\n",
    "    \"\"\" \n",
    "    Sorts the given iterable in the way that is expected.\n",
    " \n",
    "    Required arguments:\n",
    "    l -- The iterable to be sorted.\n",
    " \n",
    "    \"\"\"\n",
    "    convert = lambda text: int(text) if text.isdigit() else text\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(image_list, key = alphanum_key)\n",
    "              \n",
    "# Driver code\n",
    "a = [\"v1_0001.jpg\",\"v1_0002.jpg\",\"v1_0003.jpg\",\"v1_00017.jpg\",\"v1_00015.jpg\"]\n",
    "print(sorted_nicely(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_buy[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_buy = sorted_nicely(imgs_buy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many images there are in the `santa` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', len(imgs_buy), 'images of knives that optimize returns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, repeat this for the `not_santa` directory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_not_buy = [file for file in os.listdir(data_not_buy_dir) if file.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_not_buy = sorted_nicely(imgs_not_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_not_buy[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', len(imgs_not_buy), 'images with knives not worth buying.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all the folders and subfolders in order to get the structure represented above. You can use `os.path.join()` to create strings that will be used later on to generate new directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ImageDataGenerator()` becomes really useful when we *actually* want to generate more data. We'll show you how this works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "#                                    rotation_range=40, \n",
    "#                                    width_shift_range=0.2, \n",
    "#                                    height_shift_range=0.2, \n",
    "#                                    shear_range=0.3, \n",
    "#                                    zoom_range=0.1, \n",
    "#                                    horizontal_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = [os.path.join(train_santa, name) for name in os.listdir(train_santa)]\n",
    "# img_path = names[91]\n",
    "# img = load_img(img_path, target_size=(64, 64))\n",
    "\n",
    "# reshape_img = img_to_array(img) \n",
    "# reshape_img = reshape_img.reshape((1,) + reshape_img.shape) \n",
    "# i=0\n",
    "# for batch in train_datagen.flow(reshape_img, batch_size=1):\n",
    "#     plt.figure(i)\n",
    "#     imgplot = plt.imshow(array_to_img(batch[0]))\n",
    "#     i += 1\n",
    "#     if i % 3 == 0:\n",
    "#         break\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size = 180,\n",
    "        class_mode='binary') \n",
    "\n",
    "# get all the data in the directory split/validation (200 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_folder, \n",
    "        target_size=(64, 64),\n",
    "        batch_size = 32,\n",
    "        class_mode='binary')\n",
    "\n",
    "# get all the data in the directory split/train (542 images), and reshape them\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size = 32, \n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer= 'sgd',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = model.fit_generator(train_generator, \n",
    "                                steps_per_epoch=25, \n",
    "                                epochs=30, \n",
    "                                validation_data=val_generator, \n",
    "                                validation_steps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_knife_dir = 'knife_images'\n",
    "# data_profit_dir = 'data/profit'\n",
    "# new_dir = 'split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_folder = os.path.join(new_dir, 'train')\n",
    "# train_profit = os.path.join(train_folder, 'profit')\n",
    "# os.mkdir(train_folder)\n",
    "# os.mkdir(train_profit)\n",
    "\n",
    "# test_folder = os.path.join(new_dir, 'test')\n",
    "# test_profit = os.path.join(test_folder, 'profit')\n",
    "# os.mkdir(test_folder)\n",
    "# os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# val_folder = os.path.join(new_dir, 'validation')\n",
    "# val_profit = os.path.join(val_folder, 'profit')\n",
    "# os.mkdir(val_folder)\n",
    "# os.mkdir(val_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train knife regression images\n",
    "# #80% of data\n",
    "# imgs = knife_images[:5620]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(train_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # test knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[5620:6322]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(test_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # validation knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[6322:]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(val, img)\n",
    "#     shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_45 = df_scrub.loc[df_scrub['benchmade'] != 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_benchmade_index = df_45.sort_values(by=['profit'], ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_indexes = df_ready.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = ready_indexes.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_indexes = []\n",
    "for index in index_list:\n",
    "    image_index = str(index) + \".jpg\"\n",
    "    image_indexes.append(image_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_indexes[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def download(row):\n",
    "    filename = os.path.join(root_folder,\n",
    "                           str(row.name) + im_extension)\n",
    "\n",
    "    # create folder if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.galleryURL\n",
    "    print(f\"Downloading {url} to {filename}\")\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "root_folder = 'data/knife_images/'\n",
    "im_extension = '.jpg'  # or whatever type of images you are downloading\n",
    "\n",
    "dont_buy_df.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco['buy'] = df_spyderco['ROI'] > 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = cv2.imread('data/knife_images/918.jpg')  # convert to array\n",
    "\n",
    "img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "plt.imshow(img_rgb)  # graph it\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_list = []\n",
    "# files = glob.glob (\"C:/Users/12108/OneDrive/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/data/knife_images/*.jpg\")\n",
    "# for myFile in files:\n",
    "#     print(myFile)\n",
    "#     image_array = cv2.imread (myFile)\n",
    "# #     img_rgb = cv2.resize(image_array,(256,256),3)\n",
    "#     image_list.append (img_array)\n",
    "\n",
    "# print('image_data:', np.array(image_list).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_buy = [file for file in os.listdir(path) if file.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def sorted_nicely(image_list):\n",
    "    \"\"\" \n",
    "    Sorts the given iterable in the way that is expected.\n",
    " \n",
    "    Required arguments:\n",
    "    l -- The iterable to be sorted.\n",
    " \n",
    "    \"\"\"\n",
    "    convert = lambda text: int(text) if text.isdigit() else text\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(image_list, key = alphanum_key)\n",
    "              \n",
    "# Driver code\n",
    "a = [\"v1_0001.jpg\",\"v1_0002.jpg\",\"v1_0003.jpg\",\"v1_00017.jpg\",\"v1_00015.jpg\"]\n",
    "print(sorted_nicely(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgs_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_image_list = sorted_nicely(imgs_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(sorted_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('(\\D)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[0] = df_new[0].apply(lambda x: re.sub(pattern, \"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[0] = df_new[0].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_indx = df_new[0] .values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old_df_indx = df_filtered.index.values\n",
    "old_df_indx = merg_filtered_df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_imgs = list(set(old_df_indx) - set(new_df_indx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_ready = df_filtered.drop(missing_imgs)\n",
    "# df_ready = concat_df.drop(missing_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = merg_filtered_df.drop(missing_imgs).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for filename in os.listdir('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images4'):\n",
    "    if filename.endswith('.jpg'):\n",
    "        try:\n",
    "            img = Image.open('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images4/'+filename)  # open the image file\n",
    "            img.verify()  # verify that it is, in fact an image\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(filename)\n",
    "            file_names.append(filename)\n",
    "            os.remove('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images4/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_buy = [file for file in os.listdir(path) if file.endswith('.jpg')]\n",
    "sorted_image_list = sorted_nicely(imgs_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concat_df) - len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(sorted_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('(\\D)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[0] = df_new[0].apply(lambda x: re.sub(pattern, \"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[0] = df_new[0].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_indx = df_new[0] .values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df_indx = concat_df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_imgs = list(set(old_df_indx) - set(new_df_indx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = concat_df.drop(missing_imgs).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "for image in sorted_image_list:\n",
    "    image_array = cv2.imread(path + image)\n",
    "    try:\n",
    "        img_rgb = cv2.resize(image_array,(500,500),3) \n",
    "    except Exception as e:\n",
    "        print(str(e) + f'error at {image}')\n",
    "    \n",
    "    image_list.append(img_rgb)\n",
    "\n",
    "\n",
    "print('image_data:', np.array(image_list).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_list = []\n",
    "# for image in sorted_image_list:\n",
    "#     image_array = cv2.imread(path + image)\n",
    "#     try:\n",
    "#         img_rgb = cv2.resize(image_array,(500,500),3) \n",
    "#     except Exception as e:\n",
    "#         print(str(e) + f'error at {image}')\n",
    "    \n",
    "#     image_list.append(img_rgb)\n",
    "\n",
    "\n",
    "# print('image_data:', np.array(image_list).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(image_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_list = []\n",
    "# for image in image_indexes:\n",
    "#     image_array = cv2.imread(path + image)\n",
    "#     try:\n",
    "#         img_rgb = cv2.resize(image_array,(250,250),3) \n",
    "#         img_rgb = img_rgb*(1./255)\n",
    "#     except Exception as e:\n",
    "#         print(str(e) + f'error at {image}')\n",
    "    \n",
    "#     image_list.append(img_rgb)\n",
    "\n",
    "\n",
    "# print('image_data:', np.array(image_list).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_ready.loc[df_ready['ROI'] > 100])/len(df_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready['binary_target'] = (df_ready['ROI'] > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready['binary_target'].replace({True: 1, False: 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready['binary_target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.loc[df_ready['binary_target'] == 1].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.loc[df_ready['binary_target'] == 0].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[['converted_price','profit','ROI']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_filtered.loc[df_filtered['ROI'] >= 80])/len(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['binary_target'] = (df_filtered['ROI'] >= 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['binary_target'].replace({True:1,False:0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_ready['binary_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.drop([562, 2167, 4171, 5780, 6722, 7687, 8170, 8548, 8640, 9084, 9990, 10472, 10960, 15772, 16725, 56207])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(sorted_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('(\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['num'] = df_new[0].apply(lambda x: re.findall(pattern, str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready['target'] = (df_ready['ROI']/df_ready['ROI'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_ready.loc[df_ready['profit'] > 110]) / len(df_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready['target'] = (df_ready['profit'] > 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready['target'].replace({True: 1, False:0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ready.drop([1806,7527,12286,13101], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = df_ready['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_buy = os.listdir('split/train/buy')\n",
    "# train_buy = sorted_nicely(train_buy)\n",
    "# train_not_buy = os.listdir('split/train/not_buy')\n",
    "# train_not_buy = sorted_nicely(train_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_buy = os.listdir('split/test/buy')\n",
    "# test_buy = sorted_nicely(test_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_not_buy = os.listdir('split/test/not_buy')\n",
    "# test_not_buy = sorted_nicely(test_not_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import glob\n",
    "# import numpy as np\n",
    "\n",
    "# val_buy_data = []\n",
    "# files = glob.glob (\"C:/Users/12108/OneDrive/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/split/validation/buy/*.jpg\")\n",
    "# for myFile in files:\n",
    "#     print(myFile)\n",
    "#     image = cv2.imread (myFile)\n",
    "#     img_rgb = cv2.resize(image,(256,256),3)\n",
    "#     val_buy_data.append (img_rgb)\n",
    "\n",
    "# print('val_buy_data:', np.array(val_buy_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_not_buy_data = []\n",
    "# files = glob.glob (\"C:/Users/12108/OneDrive/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/split/validation/not_buy/*.jpg\")\n",
    "# for myFile in files:\n",
    "#     print(myFile)\n",
    "#     image = cv2.imread (myFile)\n",
    "#     img_rgb = cv2.resize(image,(256,256),3)\n",
    "#     val_not_buy_data.append (img_rgb)\n",
    "\n",
    "# print('val__notbuy_data:', np.array(val_not_buy_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_buy_data = []\n",
    "# files = glob.glob (\"C:/Users/12108/OneDrive/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/split/train/buy/*.jpg\")\n",
    "# for myFile in files:\n",
    "#     print(myFile)\n",
    "#     image = cv2.imread (myFile)\n",
    "#     img_rgb = cv2.resize(image,(256,256),3)\n",
    "#     train_buy_data.append (img_rgb)\n",
    "\n",
    "# print('train_buy_data:', np.array(train_buy_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=  df_CNN_regression['mean_profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)*.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_ready['binary_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= df_spyderco['buy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=32)# Create the Test and Final Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=32)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(100 ,100,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(100, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(100, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(500, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_absolute_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mae'])\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=5,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready['ROI'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready['ROI'].mean() * 0.7851"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.loc[df_ready['ROI'] > 100].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape([X[48825]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X[48825], interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.loc[df_ready['ROI'] > 100].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.loc[48825].apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array(X[45388].reshape(1,100,100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.6280405*df_ready['ROI'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df_ready['ROI'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.57079 * mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train*(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test*(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(16, (3, 3), padding='same', activation='relu',\n",
    "                        input_shape=(300,300,3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "#switching from regression to classifier.. no longer mse loss\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( binary_crossentropy)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( binary_crossentropy)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_regression100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X*(1./255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=32)# Create the Test and Final Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=32)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "                        input_shape=(100,100,3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(100, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(100, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "#switching from regression to classifier.. no longer mse loss\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( binary_crossentropy)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( binary_crossentropy)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_classification100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(8, (3, 3), padding='same', activation='relu',\n",
    "                        input_shape=(100,100,3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.RandomFlip(\"horizontal_and_vertical\"))\n",
    "model.add(layers.RandomRotation(0.2))\n",
    "model.add(layers.Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.RandomFlip(\"horizontal_and_vertical\"))\n",
    "model.add(layers.RandomRotation(0.2))\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.RandomFlip(\"horizontal_and_vertical\"))\n",
    "model.add(layers.RandomRotation(0.2))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(100, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(100, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "#switching from regression to classifier.. no longer mse loss\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( binary_crossentropy)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( binary_crossentropy)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a train set of 60% and a val and test size of 20% each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model showed a lot of indication that it was overfit\n",
    "#need to retry how I split the data \n",
    "#Instead of manul indexing, will use \n",
    "# from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X[:4918]\n",
    "# y_train = y[:4918]\n",
    "\n",
    "# X_train = X[4918:5971]\n",
    "# y_train = y[4918:5971]\n",
    "\n",
    "# X_test = X[5971:]\n",
    "# y_test = y[5971:]\n",
    "\n",
    "\n",
    "# display(len(X_val)/len(X))\n",
    "# display(len(X_train)/len(X))\n",
    "# len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(224 ,224,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=32,\n",
    "#                     batch_size=300,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model is showing a lot of signs of overfitting \n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub.to_csv('data/clean_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUiDAmQAWck1"
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "\n",
    "\n",
    "# path = r'C:\\Users\\12108\\Desktop\\ebay_knife_data\\dsc-5-capstone-project\\surplusStore'                     # use your path\n",
    "# all_files = glob.glob(os.path.join(path, \"*.csv\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "# df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "# concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "# concatenated_df.head()\n",
    "\n",
    "# concatenated_df.fillna(0, inplace=True)\n",
    "\n",
    "# concatenated_df.info()\n",
    "\n",
    "# concatenated_df.to_csv('surplusStore/workingDataFrame2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIS CALL TO THE WEBSITE RETURNED NO SOG KNIVES OR CRKT KNIVES\n",
    "## MUST MAKE SEPERATE CALLS\n",
    "## NOT SHOWN IN THIS NOTEBOOK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTpEYt6FV261"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_crkt = pd.read_csv('data/full_dataset_CRKT.csv')\n",
    "df_sog = pd.read_csv('data/full_dataset_SOG.csv')\n",
    "\n",
    "\n",
    "\n",
    "# df_SOG.info()\n",
    "\n",
    "# df_SOG['sog'] = 1.0\n",
    "# df_SOG['sog'] = 1.0\n",
    "\n",
    "# mkdir surplusStore\n",
    "\n",
    "# df_surplus.to_csv('surplusStore/workingDataFrame.csv', index=False)\n",
    "# df_CRKT.to_csv('surplusStore/df_CRKT.csv', index=False)\n",
    "# df_SOG.to_csv('surplusStore/df_SOG.csv', index=False)\n",
    "\n",
    "# df_surplus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XptkIA-htqb4"
   },
   "source": [
    "# Data Science Processes\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As discussed, this section is all about synthesizing your skills in order to work through a full Data Science workflow. In this lesson, you'll take a look at some general outlines for how Data Scientists organize their workflow and conceptualize their process.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- List the different data science process frameworks\n",
    "- Compare and contrast popular data science process frameworks such as CRISP-DM, KDD, OSEMN\n",
    "\n",
    "\n",
    "## What is a Data Science Process?\n",
    "\n",
    "Data Science projects are often complex, with many stakeholders, data sources, and goals. Due to this, the Data Science community has created several methodologies for helping organize and structure Data Science Projects.  In this lesson, you'll explore three of the most popular methodologies -- **_CRISP-DM_**, **_KDD_**, and **_OSEMN_**, and explore how you can make use of them to keep your projects well-structured and organized. \n",
    "\n",
    "## CRoss-Industry Standard Process for Data Mining (CRISP-DM)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_crisp-dm.png\" width=\"500\">\n",
    "\n",
    "**_CRISP-DM_** is probably the most popular Data Science process in the Data Science world right now. Take a look at the visualization above to get a feel for CRISP-DM. Notice that CRISP-DM is an iterative process!\n",
    "\n",
    "Let's take a look at the individual steps involved in CRISP-DM.\n",
    "\n",
    "**_Business Understanding:_**  This stage is all about gathering facts and requirements. Who will be using the model you build? How will they be using it? How will this help the goals of the business or organization overall? Data Science projects are complex, with many moving parts and stakeholders. They're also time intensive to complete or modify. Because of this, it is very important that the Data Science team working on the project has a deep understanding of what the problem is, and how the solution will be used. Consider the fact that many stakeholders involved in the project may not have technical backgrounds, and may not even be from the same organization.  Stakeholders from one part of the organization may have wildly different expectations about the project than stakeholders from a different part of the organization -- for instance, the sales team may be under the impression that a recommendation system project is meant to increase sales by recommending upsells to current customers, while the marketing team may be under the impression that the project is meant to help generate new leads by personalizing product recommendations in a marketing email. These are two very different interpretations of a recommendation system project, and it's understandable that both departments would immediately assume that the primary goal of the project is one that helps their organization. As a Data Scientist, it's up to you to clarify the requirements and make sure that everyone involved understands what the project is and isn't. \n",
    "\n",
    "During this stage, the goal is to get everyone on the same page and to provide clarity on the scope of the project for everyone involved, not just the Data Science team. Generate and answer as many contextual questions as you can about the project. \n",
    "\n",
    "Good questions for this stage include:\n",
    "\n",
    "- Who are the stakeholders in this project? Who will be directly affected by the creation of this project?\n",
    "- What business problem(s) will this Data Science project solve for the organization?  \n",
    "- What problems are inside the scope of this project?\n",
    "- What problems are outside the scope of this project?\n",
    "- What data sources are available to us?\n",
    "- What is the expected timeline for this project? Are there hard deadlines (e.g. \"must be live before holiday season shopping\") or is this an ongoing project?\n",
    "- Do stakeholders from different parts of the company or organization all have the exact same understanding about what this project is and isn't?\n",
    "\n",
    "**_Data Understanding:_**\n",
    "\n",
    "Once we have a solid understanding of the business implications for this project, we move on to understanding our data. During this stage, we'll aim to get a solid understanding of the data needed to complete the project.  This step includes both understanding where our data is coming from, as well as the information contained within the data. \n",
    "\n",
    "Consider the following questions when working through this stage:\n",
    "\n",
    "- What data is available to us? Where does it live? Do we have the data, or can we scrape/buy/source the data from somewhere else?\n",
    "- Who controls the data sources, and what steps are needed to get access to the data?\n",
    "- What is our target?\n",
    "- What predictors are available to us?\n",
    "- What data types are the predictors we'll be working with?\n",
    "- What is the distribution of our data?\n",
    "- How many observations does our dataset contain? Do we have a lot of data? Only a little? \n",
    "- Do we have enough data to build a model? Will we need to use resampling methods?\n",
    "- How do we know the data is correct? How is the data collected? Is there a chance the data could be wrong?\n",
    "\n",
    "**_Data Preparation:_**\n",
    "\n",
    "Once we have a strong understanding of our data, we can move onto preparing the data for our modeling steps. \n",
    "\n",
    "During this stage, we'll want to handle the following issues:\n",
    "\n",
    "- Detecting and dealing with missing values\n",
    "- Data type conversions (e.g. numeric data mistakenly encoded as strings)\n",
    "- Checking for and removing multicollinearity (correlated predictors)\n",
    "- Normalizing our numeric data\n",
    "- Converting categorical data to numeric format through one-hot encoding\n",
    "\n",
    "**_Modeling:_**\n",
    "\n",
    "Once we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task. \n",
    "\n",
    "Consider the following questions during the modeling step:\n",
    "\n",
    "- Is this a classification task? A regression task? Something else?\n",
    "- What models will we try?\n",
    "- How do we deal with overfitting?\n",
    "- Do we need to use regularization or not?\n",
    "- What sort of validation strategy will we be using to check that our model works well on unseen data?\n",
    "- What loss functions will we use?\n",
    "- What threshold of performance do we consider as successful?\n",
    "\n",
    "**_Evaluation:_**\n",
    "\n",
    "During this step, we'll evaluate the results of our modeling efforts. Does our model solve the problems that we outlined all the way back during step 1? Why or why not? Often times, evaluating the results of our modeling step will raise new questions, or will cause us to consider changing our approach to the problem.  Notice from the CRISP-DM diagram above, that the \"Evaluation\" step is unique in that it points to both _Business Understanding_ and _Deployment_.  As we mentioned before, Data Science is an iterative process -- that means that given the new information our model has provided, we'll often want to start over with another iteration, armed with our newfound knowledge! Perhaps the results of our model showed us something important that we had originally failed to consider the goal of the project or the scope.  Perhaps we learned that the model can't be successful without more data, or different data. Perhaps our evaluation shows us that we should reconsider our approach to cleaning and structuring the data, or how we frame the project as a whole (e.g. realizing we should treat the problem as a classification rather than a regression task). In any of these cases, it is totally encouraged to revisit the earlier steps.  \n",
    "\n",
    "Of course, if the results are satisfactory, then we instead move onto deployment!\n",
    "\n",
    "**_Deployment:_**\n",
    "\n",
    "During this stage, we'll focus on moving our model into production and automating as much as possible. Everything before this serves as a proof-of-concept or an investigation.  If the project has proved successful, then you'll work with stakeholders to determine the best way to implement models and insights.  For example, you might set up an automated ETL (Extract-Transform-Load) pipelines of raw data in order to feed into a database and reformat it so that it is ready for modeling. During the deployment step, you'll actively work to determine the best course of action for getting the results of your project into the wild, and you'll often be involved with building everything needed to put the software into production. \n",
    "\n",
    "This is one of the most rewarding steps of the entire Data Science process -- getting to see your work go live!\n",
    "\n",
    "## Knowledge Discovery in Databases\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_kdd.png\" width=\"800\">\n",
    "\n",
    "**_Knowledge Discovery in Databases_**, or **_KDD_** is considered the oldest Data Science process. The creation of this process is credited to Gregory Piatetsky-Shapiro, who also runs the ever-popular Data Science blog, [kdnuggets](https://www.kdnuggets.com/). If you're interested, read the original white paper on KDD, which can be found [here](https://www.kdnuggets.com/gpspubs/aimag-kdd-overview-1992.pdf)!\n",
    "\n",
    "The KDD process is quite similar to the CRISP-DM process. The diagram above illustrates every step of the KDD process, as well as the expected output at each stage. \n",
    "\n",
    "**_Selection_**:\n",
    "\n",
    "During this stage, you'll focus on selecting your problem, and the data that will help you answer it. This stage works much like the first stage of CRISP-DM -- you begin by focusing on developing an understanding of the domain the problem resides in (e.g. marketing, finance, increasing customer sales, etc), the previous work done in this domain, and the goals of the stakeholders involved with the process.  \n",
    "\n",
    "Once you've developed a strong understanding of the goals and the domain, you'll work to establish where your data is coming from, and which data will be useful to you.  Organizations and companies usually have a ton of data, and only some of it will be relevant to the problem you're trying to solve.  During this stage, you'll focus on examining the data sources available to you and gathering the data that you deem useful for the project.  \n",
    "\n",
    "The output of this stage is the dataset you'll be using for the Data Science project. \n",
    "\n",
    "**_Preprocessing_**:\n",
    "\n",
    "The preprocessing stage is pretty straightforward -- the goal of this stage is to \"clean\" the data by preprocessing it.  For text data, this may include things like tokenization.  You'll also identify and deal with issues like outliers and/or missing data in this stage.  \n",
    "\n",
    "In practice, this stage often blurs with the _Transformation_ stage. \n",
    "\n",
    "The output of this stage is preprocessed data that is more \"clean\" than it was at the start of this stage -- although the dataset is not quite ready for modeling yet. \n",
    "\n",
    "**_Transformation_**:\n",
    "\n",
    "During this stage, you'll take your preprocessed data and transform it in a way that makes it more ideal for modeling.  This may include steps like feature engineering and dimensionality reduction.  At this stage, you'll also deal with things like checking for and removing multicollinearity from the dataset. Categorical data should also be converted to numeric format through one-hot encoding during this step.\n",
    "\n",
    "The output of this stage is a dataset that is now ready for modeling. All null values and outliers are removed, categorical data has been converted to a format that a model can work with, and the dataset is generally ready for experimentation with modeling.  \n",
    "\n",
    "**_Data Mining_**:\n",
    "\n",
    "The Data Mining stage refers to using different modeling techniques to try and build a model that solves the problem we're after -- often, this is a classification or regression task. During this stage, you'll also define your parameters for given models, as well as your overall criteria for measuring the performance of a model.  \n",
    "\n",
    "You may be wondering what Data Mining is, and how it relates to Data Science. In practice, it's just an older term that essentially means the same thing as Data Science. Dr. Piatetsky-Shapiro defines Data Mining as \"the non-trivial extraction of implicit, previously unknown and potentially useful information from data.\"  Making of things such as Machine Learning algorithms to find insights in large datasets that aren't immediately obvious without these algorithms is at the heart of the concept of Data Mining, just as it is in Data Science. In a pragmatic sense, this is why the terms Data Mining and Data Science are typically used interchangeably, although the term Data Mining is considered an older term that isn't used as often nowadays. \n",
    "\n",
    "The output of this stage results from a fit to the data for the problem we're trying to solve.  \n",
    "\n",
    "**_Interpretation/Evaluation_**:\n",
    "\n",
    "During this final stage of KDD, we focus on interpreting the \"patterns\" discovered in the previous step to help us make generalizations or predictions that help us answer our original question. During this stage, you'll consolidate everything you've learned to present it to stakeholders for guiding future actions. Your output may be a presentation that you use to communicate to non-technical managers or executives (never discount the importance of knowing PowerPoint as a Data Scientist!).  Your conclusions for a project may range from \"this approach didn't work\" or \"we need more data about {X}\" to \"this is ready for production, let's build it!\".  \n",
    "\n",
    "## OSEMN\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_osemn.png\" width=\"800\">\n",
    "<a href=\"https://www.kdnuggets.com/2018/02/data-science-command-line-book-exploring-data.html\" target=\"_blank\">Adapted from: KDNuggets</a>\n",
    "\n",
    "This brings us to the Data Science process we'll be using during this section -- OSEMN (sometimes referred as OSEMiN, and pronounced \"OH-sum\", rhymes with \"possum\"). This is the most straightforward of the Data Science processes discussed so far. Note that during this process, just like the others, the stages often blur together. It is completely acceptable (and often a best practice!) to float back and forth between stages as you learn new things about your problem, dataset, requirements, etc.  It's quite common to get to the modeling step and realize that you need to scrub your data a bit more or engineer a different feature and jump back to the \"Scrub\" stage, or go all the way back to the \"Obtain\" stage when you realize your current data isn't sufficient to solve this problem. As with any of these frameworks, OSEMN is meant to be treated more like a set of guidelines for structuring your project than set-in-stone steps that cannot be violated.  \n",
    "\n",
    "**_Obtain_**:\n",
    "\n",
    "As with CRISP-DM and KDD, this step involves understanding stakeholder requirements, gathering information on the problem, and finally, sourcing data that we think will be necessary for solving this problem. \n",
    "\n",
    "**_Scrub_**:\n",
    "\n",
    "During this stage, we'll focus on preprocessing our data.  Important steps such as identifying and removing null values, dealing with outliers, normalizing data, and feature engineering/feature selection are handled around this stage.  The line with this stage really blurs with the _Explore_ stage, as it is common to only realize that certain columns require cleaning or preprocessing as a result of the visualizations and explorations done during Step 3.  \n",
    "\n",
    "Note that although technically, categorical data should be one-hot encoded during this step, in practice, it's usually done after data exploration.  This is because it is much less time-consuming to visualize and explore a few columns containing categorical data than it is to explore many different dummy columns that have been one-hot encoded. \n",
    "\n",
    "**_Explore_**:\n",
    "\n",
    "This step focuses on getting to know the dataset you're working with. As mentioned above, this step tends to blend with the _Scrub_ step mentioned above.  During this step, you'll create visualizations to really get a feel for your dataset.  You'll focus on things such as understanding the distribution of different columns, checking for multicollinearity, and other tasks like that.  If your project is a classification task, you may check the balance of the different classes in your dataset.  If your problem is a regression task, you may check that the dataset meets the assumptions necessary for a regression task.  \n",
    "\n",
    "At the end of this step, you should have a dataset ready for modeling that you've thoroughly explored and are extremely familiar with.  \n",
    "\n",
    "**_Model_**:\n",
    "\n",
    "This step, as with the last two frameworks, is also pretty self-explanatory. It consists of building and tuning models using all the tools you have in your data science toolbox.  In practice, this often means defining a threshold for success, selecting machine learning algorithms to test on the project, and tuning the ones that show promise to try and increase your results.  As with the other stages, it is both common and accepted to realize something, jump back to a previous stage like _Scrub_ or _Explore_, and make some changes to see how it affects the model.  \n",
    "\n",
    "**_Interpret_**:\n",
    "\n",
    "During this step, you'll interpret the results of your model(s), and communicate results to stakeholders.  As with the other frameworks, communication is incredibly important! During this stage, you may come to realize that further investigation is needed, or more data.  That's totally fine -- figure out what's needed, go get it, and start the process over! If your results are satisfactory to all stakeholders involved, you may also go from this stage right into putting your model into production and automating processes necessary to support it.  \n",
    "\n",
    "\n",
    "## A Note On Communicating Results\n",
    "\n",
    "Regardless of the quality of your results, it's very important that you be aware of the business requirements and stakeholder expectations at all times! Generally, no matter which of the above processes you use, you'll communicate your results in a two-pronged manner: \n",
    "\n",
    "- A short, high-level presentation covering your question, process, and results meant for non-technical audiences\n",
    "- A detailed Jupyter Notebook demonstrating your entire process meant for technical audiences\n",
    "\n",
    "In general, you can see why Data Scientists love Jupyter Notebooks! It is very easy to format results in a reproducible, easy-to-understand way.  Although a detailed Jupyter Notebook may seem like the more involved of the two deliverables listed above, the high-level presentation is often the hardest! Just remember -- even if the project took you/your team over a year and utilized the most cutting-edge machine learning techniques available, you still need to be able to communicate your results in about 5 slides (using graphics, not words, whenever possible!), in a 5 minute presentation in a way that someone that can't write code can still understand and be convinced by!\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lesson, you learned about the different data science process frameworks including CRISP-DM, KDD, and OSEMN. You also learned that the data science process is iterative and that a typical data science project involves many different stakeholders who may not have a technical background. As such, it's important to recognize that data scientists must be able to communicate their findings in a non-technical way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQTkWMsUOua0"
   },
   "outputs": [],
   "source": [
    "# #REGEX BRAND PATTERNS AFTERN LOWERCASING TITLES AND REMOVING SPEVIAL CHARACTERS\n",
    "\n",
    "\n",
    "#turn this into a dict bro\n",
    "\n",
    "# benchmade_pattern = \"benchmade\"\n",
    "# buck_pattern = \"buck\"\n",
    "# case_pattern = \"case\"\n",
    "# crkt_pattern = \"crkt\"\n",
    "# kershaw_pattern = \"kershaw\"\n",
    "# leatherman_pattern = \"leatherman\"\n",
    "# sog_pattern = \"sog\"\n",
    "# spyderco_pattern = \"spyderco\"\n",
    "# victorinox_pattern = \"victorinox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['is_profitable'] = (df['price_in_US'] - df[list(costs.keys())].sum(axis=1))>0\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #REGEX BRAND PATTERNS AFTERN LOWERCASING TITLES AND REMOVING SPEVIAL CHARACTERS\n",
    "\n",
    "\n",
    "#turn this into a dict bro\n",
    "\n",
    "# benchmade_pattern = \"benchmade\"\n",
    "# buck_pattern = \"buck\"\n",
    "# case_pattern = \"case\"\n",
    "# crkt_pattern = \"crkt\"\n",
    "# kershaw_pattern = \"kershaw\"\n",
    "# leatherman_pattern = \"leatherman\"\n",
    "# sog_pattern = \"sog\"\n",
    "# spyderco_pattern = \"spyderco\"\n",
    "# victorinox_pattern = \"victorinox\"\n",
    "\n",
    "# df['is_profitable'] = (df['price_in_US'] - df[list(costs.keys())].sum(axis=1))>0\n",
    "\n",
    "# df.info()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Capstone_RoughDraft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
