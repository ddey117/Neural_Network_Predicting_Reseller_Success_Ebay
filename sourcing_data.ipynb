{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Resale Value of Knives from a Texas Government Surplus Store\n",
    "## Using Machine Learning to Support an Ebay Store's Financial Success\n",
    "\n",
    "### Data Obtainment Notebook\n",
    "\n",
    "\n",
    "This notebook displays the code used to collect and process data from eBay using two of eBay's public APIs and scraping from their proprietary webabb \"Terapeak\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Author:** Dylan Dey\n",
    "***\n",
    "\n",
    "# Overview\n",
    "[Texas State Surplus Store](https://www.tfc.texas.gov/divisions/supportserv/prog/statesurplus/)\n",
    "\n",
    "[What happens to all those items that get confiscated by the TSA? Some end up in a Texas store.](https://www.wfaa.com/article/news/local/what-happens-to-all-those-items-that-get-confiscated-by-the-tsa-some-end-up-in-a-texas-store/287-ba80dac3-d91a-4b28-952a-0aaf4f69ff95)\n",
    "\n",
    "[Texas Surplus Store PDF](https://www.tfc.texas.gov/divisions/supportserv/prog/statesurplus/State%20Surplus%20Brochure-one%20bar_rev%201-10-2022.pdf)\n",
    "\n",
    "![Texas State Surplus Store](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRYkwyu20VBuQ52PrXdVRaGRIIg9OPXJg86lA&usqp=CAU)\n",
    "\n",
    "![Texas Knives In Stores](https://arc-anglerfish-arc2-prod-bostonglobe.s3.amazonaws.com/public/MWJCCFBSR4I6FCSNKONTFJIRAI.jpg)\n",
    "\n",
    "[Everything that doesn't make it through Texas airports can be found at one Austin store](https://cbsaustin.com/news/local/everything-that-doesnt-make-it-through-texas-airports-can-be-found-at-one-austin-store)\n",
    "\n",
    "\n",
    "> The Texas Facilities Commission collects left behind possessions, salvage, and surplus from Texas state agencies such as DPS, TXDOT, TCEQ, and Texas Parks & Wildlife. Examples of commonly available items include vehicles, furniture, office equipment and supplies, small electronics, and heavy equipment. The goal of this project is to create a predictive model in order to determine the resale value of knivse from the Texas State Surplus Store on eBay. \n",
    "\n",
    "\n",
    "# Business Problem\n",
    "\n",
    "[Family Ebay Store Front](https://www.ebay.com/str/texasdave3?mkcid=16&mkevt=1&mkrid=711-127632-2357-0&ssspo=ZW3G27tGR_m&sssrc=3418065&ssuid=&widget_ver=artemis&media=COPY)\n",
    "\n",
    "![Father's Ebay Account Since 1999](texas_dave.jpg)\n",
    "\n",
    "[Texas Dave's Knives](https://www.ebay.com/str/texasdave3/Knives/_i.html?store_cat=3393246519)\n",
    "\n",
    "\n",
    "> While taking online courses to transition careers during a difficult time of my life, I was also helping my family during a turbulent time for everyone. I have been employed at their retail store in San Antonio for the past several months and have been contributing significantly to their online reselling business on eBay. I would help source newer, cheaper products from Austin to try and resell at the retail store in San Antonio or online to earn some money, support our family business. This is how I discovered the <mark>Texas State Surplus Store.</mark> \n",
    "\n",
    "\n",
    "> My family has been running a resale shop and selling on Ebay and other sites for years and lately the business has picked up.  Consumer behavior is shifting:  getting a deal on eBay, or Goodwill, or hitting up a vintage boutique shop to find a unique treasure is now brag worthy.  Plus, people like the idea of sustainability - sending items to landfills is becoming very socially unacceptable – why not repurpose a used item?  With the pandemic related disruption of “normal” business and supply chains and the economic uncertainty of these times there is definitely an upswing in interest in the resale market. \n",
    "\n",
    "> Online sales sites like Ebay offer a worldwide robust buyer base for just about every product regardless of condition. Ebay  allows the reseller to find both  bargain hunters for common items and  enthusiasts searching for rare  collectible items. \n",
    "\n",
    "> An Ebay business has some pain points, however. <mark>Selection of an item to sell</mark> is the main pain point. The item should be readily available in decent condition for the seller to purchase at a low price but not so widely available that the market is saturated with that item.  Then there needs to be a demand for the item – it should be something collectible that with appeal to hobbyists that would pay premium prices for hard-to-get items. Alternatively, it would be something useful to a large number of people even in a used condition. The item should be small enough to be easily shipped. It should not be difficult to ship either—that is it should not have hazardous chemicals, batteries etc. that would add costs to the shipping. Additionally, Ebay has strict rules about authentication and certification in many item categories- so obvious “high value” items like jewelry or designer purses are so restricted that it is not  feasible  for the average Ebay seller to offer them . \n",
    "\n",
    "> This project recommends an item that would answer these concerns – pocket knives, These can be rare and collectible and also practical and useful. There are knife collector forums and subReddits, showing there is an interest among collectors.  A look at eBay listings shows rare knives selling for thousands of \n",
    "dollars each.  Knives are also a handy every day tool –  and based on the number  showing up in the Texas Surplus shop they are easy to lose and so need replacing often. This means there is a market for more common ones as well.  The great thing about <mark>single blade, modern, factory manufactured pocketknives</mark> is that they all weigh roughly 0.5 lbs making them cheap to ship. For my modeling purposes, it is safe to <mark>assume a flat shipping rate of 4.95(US Dollars)</mark> including the cost of wholesale purchased padded envelopes. And there are no restrictions on mailing these items and they are not fragile so no special packaging is needed. \n",
    "\n",
    "> The second pain point is <mark>buying at a cost low enough to make a profit.</mark> It is not enough to just buy low and sell at a higher price as expenses need to be considered.  Ebay collects insertion fees and final value fees on all sales.  The fees vary with seller level (rating)  and some portions  are a percent of final sale. I have been selling knives from the lower priced bins and the mean seller fee for my sales so far is about 13.5% of the sold price.  So that is a cost to consider right up front. \n",
    "\n",
    "> A third pain point is <mark>the cost of excess inventory.</mark> A seller can obtain quality items at a reasonable cost and then the inventory may sit with no sales, meaning the capital expended is sitting tied up in unwanted items. This inventory carry cost is a drain on profitability.  This project is meant to help avoid purchasing the wrong items for resale. \n",
    "\n",
    "\n",
    "> As already mentioned, I have been experimenting with low cost used knives for resale but have not risked a large capital investment in the higher end items. The goal of this project is to attempt to address the pain points to determine if a larger investment would pay off. Can I identify which knives are worth investing in so that I can turn a decent profit and hopefully avoid excess inventory? A data driven approach would help avoid costly mistakes from the \"system\" resellers currently employ, which seems to be mainly a gambler’s approach. By managing resources upfront through a model, I can effectively increase my return on investment with messy data such as pictures and titles. The magic of Neural Networks!\n",
    "\n",
    "\n",
    "> There are <mark>eight buckets</mark> of presorted brand knives that I was interested in, specifically. These bins are behind glass, presorted, branded(and therefore have specific characteristics and logos for my model to identify), and priced higher. However, the staff has a very large amount of confiscated items flowing into the facility to list for resale, and when that happens they will not have time to preset them and they end up in huge buckets of unsorted knives for people to dig through. The brands will be priced the same, they are just no longer sorted and harder to find. This particular scenario is where a NN could really shine to help add more inventory to our Ebay website without risking more money or spending extra time than simply digging through the presorted bins everytime. Expanding the bins to pull inventory from will increase the chance of finding inventory worth reselling. \n",
    "\n",
    "**sorted bucket example**\n",
    "![case price image](images/casePriceBucket2.jpg)\n",
    "\n",
    "**overflow example**\n",
    "![overflow image](images/overflow.jpeg)\n",
    "\n",
    "# Data Understanding\n",
    "\n",
    "[Family Ebay Store Front](https://www.ebay.com/str/texasdave3?mkcid=16&mkevt=1&mkrid=711-127632-2357-0&ssspo=ZW3G27tGR_m&sssrc=3418065&ssuid=&widget_ver=artemis&media=COPY)\n",
    "\n",
    "![Father's Ebay Account Since 1999](texas_dave.jpg)\n",
    "\n",
    "[Texas Dave's Knives](https://www.ebay.com/str/texasdave3/Knives/_i.html?store_cat=3393246519)\n",
    "\n",
    "\n",
    "> There are <mark>eight buckets of presorted brand knives</mark> that I was interested in exploring from the Texas Surplus Store. These bins are behind glass, presorted, branded(and therefore have specific characteristics and logos for my model to identify), and priced higher. However, the staff has a very large amount of confiscated items flowing into the facility to list for resale, and when that happens they will not have time to preset them and they end up in huge buckets of unsorted knives for people to dig through. The brands will be priced the same, they are just no longer sorted and harder to find. This particular scenario is where a NN could really shine to help add more inventory to our Ebay website without risking more money or spending extra time than simply digging through the presorted bins everytime. Expanding the bins to pull inventory from will increase the chance of finding inventory worth reselling. \n",
    "\n",
    "\n",
    "The Eight Pocketknife brands and their associated cost at the Texas Surplus Store:\n",
    "<ul>\n",
    "  <li>Benchmade: \\$45.00</li>\n",
    "  <li>Buck: \\$20.00</li>\n",
    "  <li>Case/Casexx: \\$20.00</li>\n",
    "  <li>CRKT: \\$15.00</li>\n",
    "  <li>Kershaw: \\$15.00</li>\n",
    "  <li>SOG: \\$15.00</li>\n",
    "  <li>Spyderco: \\$30.00</li>\n",
    "  <li>Victorinox: \\$20.00</li>\n",
    "</ul>\n",
    "\n",
    "[Ebay Developer Website](https://developer.ebay.com/)\n",
    "> Ebay has a website for developers to create an account and register an application keyset in order to make API call requests to their live website. By making a findItemsAdvanced call to the eBay Finding APIVersion 1.13.0, I was able to get a large dataset of [category_id=<48818>](https://www.ebay.com/sch/48818/i.html?_from=R40&_nkw=knife) knives listed for sale. This data is limited to anything listed within the past 90 days from when the API call was made.\n",
    "\n",
    "> When you log into Ebay as a buyer and search knife in the search bar, the response that loads outputs  Knives, Swords & Blades. Nested one category furtheris Collectible Folding Knives with an id of 182981. Nested one further is Modern Folding Knives(43333), and then finally, the category_id of most interest, 48818, Factory Manufactured Modern Collectible Folding Knives. \n",
    "\n",
    "> The eBay Finding APIVersion 1.13.0 [findItemsAdvanced](https://developer.ebay.com/devzone/finding/callref/finditemsadvanced.html) call returns a lot of usefull information about listings, including itemId(a unique identifier for ebay listings),price, shipping price, area code, the title of the listing, the url for the listing, whether the seller set autoPay for the listing or whether the seller is a top rated seller or not, the condition of the item being sold, whether the seller accepts returns, and various links to images of the item being sold at different resolutions. If you look at a typical eBay listing, however, there is usually more minute information available that is required to be filled out by the seller upon posting the listing. To get this information, another API must be used that accepts the itemId of listings to return more details.\n",
    "\n",
    "> The eBay Shopping APIVersion 1247 [GetMultipleItems](https://developer.ebay.com/Devzone/shopping/docs/CallRef/GetMultipleItems.html) call accepts itemIds and returns seller authored details on the item for sale in their listing. This was used to get information such as of the model or product line for the knife being listed, blade material, blade type, blade edge type, color, the number of blades, opening mechanism, handle material, lock type, and blade range.\n",
    "\n",
    "> All of the data gathered from eBay's public API is limited to listed data posted in the past 90 days and doesn't include a \"sold\" price. Sold data is locked behind eBay's proprietary webapp, known as Terapeak. Data on this pay to play webapp has an option for sold data that goes back 2 years! Therefore, gaining access to this webapp and scraping all relevant pages proved to be very valuable and bypasses the limits of the free API. I used my reletavely new eBay seller's account to sign up for a free trial of terapeak and scraped useful data for sold, used knives of the 8 relevant brands. Information scraped includes Images, titles, price sold, shipping cost. \n",
    "\n",
    ">A majority of the data was scraped from eBays proprietary Terapeak webapp, as this data goes back 2 years as compared to the API listed data that only goes back 90 days. It is assumed a large enough amount of listed data should approximate sold data well enough to prove useful for this project. \n",
    "\n",
    "> The target feature for the model to predict is the total price (shipping included) that a knife should be listed on eBay. One model will be using titles and images in order to find potential listings that are undervalued and could be worth investing in. Another model will accept only images as input, as this is an input that can easily be obtained in person at the store. This model will use past sold data of knives on eBay in order to determine within an acceptable amount of error the price it will resale for on eBay (shipping included) using only an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $3.00\n",
    "- Ebay's comission, with 13\\% being a reasonable approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Obtainment \n",
    "## 'Ebay FindingService',  '1.12.0', 'findItemsAdvanced', 'eBaySDK/2.2.0 Python/3.8.5 Windows/10'\n",
    "\n",
    "[Ebay suggested  SDKs on ebay developer website](https://developer.ebay.com/develop/ebay-sdks)\n",
    "\n",
    "[Python SDK to simplify making calls](https://github.com/timotheus/ebaysdk-python/wiki/Trading-API-Class)\n",
    "\n",
    "[eBay Finding APIVersion 1.13.0 call index](https://developer.ebay.com/devzone/finding/CallRef/index.html)\n",
    "\n",
    "[finditemsAdvanced Call Reference](https://developer.ebay.com/devzone/finding/CallRef/findItemsAdvanced.html)\n",
    "\n",
    "\n",
    "\n",
    "The Ebay developer website suggests using an SDK in order to make a call to their APIs. I decided to git clone [the Python SDK to simplify making calls](https://github.com/timotheus/ebaysdk-python/wiki/Trading-API-Class) and used the .yaml file from the github repository in order to store all of my necessary developer/security keys. Please feel free to read through the documentation in the github and the documentation in the API reference to see what all is available using this SDK and API.\n",
    "\n",
    "the API limits you to the first 100 pages of whatever response you recieve from a request. \n",
    "\n",
    "Ebay offers people with a basic seller subscription and above to access their research tools. Terapeak product research allows \"targeted insights about markets you're interested in, simply search by keyword or product, and apply filters such as Listing type, Start price, Buyer country, and Time of day.\" [Terapeak](https://www.ebay.com/help/selling/selling-tools/terapeak-research?id=4853)\n",
    "\n",
    "The Terapeak product research website allows access to the sale price of products that go back 2 years. The Terapeak product research website was filtered for USED Factory Manufactured Modern Collectible Folding Knives (category_id 48818) sold prices for all 8 knife models of interest in the last 2 years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.finding import Connection\n",
    "import requests\n",
    "from ebaysdk.shopping import Connection as Shopping\n",
    "import pandas as pd \n",
    "import  json\n",
    "import numpy as np\n",
    "import re\n",
    "# import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import ast\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is a helper function created for the \"knife_request\" below. \n",
    "#It unpacks some of the nested data from eBay API calls \n",
    "#It also creates the new feature \"converted_price\"\n",
    "#\"converted_price\" is the price of the item for sale plus shipping cost.\n",
    "def prepare_df(df):\n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    condition = None\n",
    "    for row in full_dataset:\n",
    "        listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "        price_list.append(listed_price)\n",
    "     \n",
    "        try:\n",
    "            listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "\n",
    "        try:\n",
    "            condition = float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#dictionary for preparing brands\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "#a helper function used with knife_request \n",
    "#it is used to create new columns of interest\n",
    "#the brand of knife from the API call\n",
    "#the cost of the knife from the Surplus Store\n",
    "#profit for reselling a used surplus knife on eBay\n",
    "#Return on Investment for reselling the knife\n",
    "#All columns in US dollars\n",
    "def prepare_brands(df, bucket_dict_position, overhead_cost=3):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    " \n",
    "    #remove special characters\n",
    "#     df.title.apply(pp.remove_special_chars)\n",
    "    df['brand'] = str(list(bucket_dict.keys())[bucket_dict_position])\n",
    "    df['cost'] = float(list(bucket_dict.values())[bucket_dict_position]+4.95+overhead_cost)\n",
    "    df['profit'] = ((df['converted_price']*.87) -  df['cost']) \n",
    "    df['ROI'] = (df['profit']/( df['cost']))*100.0\n",
    "    \n",
    "    return df\n",
    "# Help organize paginated data from API calls\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "                'viewItemURL', 'autoPay', 'postalCode', \n",
    "                'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "                'returnsAccepted', 'condition', 'topRatedListing',\n",
    "                'galleryPlusPictureURL','pictureURLLarge', \n",
    "                'pictureURLSuperSize']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "#main function for making findingAPI calls to eBay\n",
    "def knife_request(Brand, dict_pos):\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "    #first request gets number of pages from paginationOutput of first page \n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': 1\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "    #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "\n",
    "    response_pages = response.dict()\n",
    "\n",
    "    full_dataset = []\n",
    "    \n",
    "    total_pages = int(response_pages['paginationOutput']['totalPages'])\n",
    "\n",
    "    if total_pages > 100:\n",
    "        pages_to_request = 100\n",
    "        \n",
    "    else:\n",
    "        pages_to_request = total_pages - 1\n",
    "        #subtract number of pages by one to avoid errors\n",
    "        \n",
    "    #Loop through available pages\n",
    "    for page in range(1, pages_to_request):\n",
    "        # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "        # Make the query and get the response\n",
    "\n",
    "        api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "        request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "\n",
    "        response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "        #save the response as a json dict\n",
    "        response_dict = response.dict()\n",
    "\n",
    "\n",
    "        #index dict to appropriate index\n",
    "        results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "        # Call the prepare_data function to get a list of processed data\n",
    "        prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "        # Extend full_dataset with this list (don't append, or you'll get\n",
    "        # a list of lists instead of a flat list)\n",
    "        full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "    display(len(full_dataset))\n",
    "    \n",
    "    df = pd.DataFrame(full_dataset)\n",
    "    \n",
    "    df = prepare_df(df)\n",
    "    \n",
    "    df = prepare_brands(df, dict_pos)\n",
    "    \n",
    "    return df\n",
    "#Used to prepare data from eBays shopping API\n",
    "#Shopping API used to collect more detailed info\n",
    "#about individual knives\n",
    "def prepare_dataIds(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID','GalleryURL','PictureURL',\n",
    "                'Location','ConvertedCurrentPrice',\n",
    "                'Title','ItemSpecifics', \n",
    "                'Country','ConditionID']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "#Shopping api accepts a max of 20 itemIDs\n",
    "#this function was created to automate\n",
    "#making API calls in 20 unique itemId chuncks\n",
    "def process_list(my_list):\n",
    " \n",
    "    api = Shopping(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "    request = {\n",
    "               'itemID': my_list,\n",
    "               'IncludeSelector': 'ItemSpecifics'\n",
    "              }\n",
    "    response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_dataIds(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "    return full_dataset\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "#special function for reformatting terapeak scraped data\n",
    "#x = position of bucket_dictionary\n",
    "def prepare_tera_df(df, x, overhead_cost=3):\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\"$\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\",\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].apply(float)\n",
    "    \n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\"$\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\",\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].apply(float)\n",
    "    \n",
    "    df['brand'] = list(bucket_dict.keys())[x]\n",
    "    df['converted_price'] = (df['price_in_US'] + df['shipping_cost'])\n",
    "    df['cost'] = list(bucket_dict.values())[x] + overhead_cost + 4.95\n",
    "    df['profit'] = ((df['converted_price']*.87) -  df['cost'])\n",
    "    df['ROI'] = (df['profit']/ df['cost'])*100.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "# helper function with \"transform_item_specifics\"\n",
    "def fix(col):\n",
    "    dd = dict()\n",
    "    for d in col:\n",
    "        values = list(d.values())\n",
    "        if len(values) == 2:\n",
    "            dd[values[0]] = values[1]\n",
    "    return dd\n",
    "\n",
    "#function for extracted item Specifics from Shopping API data\n",
    "def transform_item_specifics(df, perc=65.0):\n",
    "\n",
    "    df.dropna(subset=['ItemSpecifics'], inplace=True)\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(lambda x: ast.literal_eval(x))\n",
    "    df['item_list'] = df['ItemSpecifics'].apply(lambda x: x['NameValueList'])\n",
    "\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(lambda x: [x['NameValueList']] if isinstance(x['NameValueList'], dict) else x['NameValueList'])\n",
    "\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(fix)\n",
    "\n",
    "    df = pd.json_normalize(df['ItemSpecifics'])\n",
    "\n",
    "    min_count =  int(((100-perc)/100)*df.shape[0] + 1)\n",
    "    mod_df = df.dropna(axis=1, \n",
    "                       thresh=min_count)\n",
    "\n",
    "    return mod_df\n",
    "\n",
    "# This function removes noisy data\n",
    "#lots/sets/groups of knives can\n",
    "#confuse the model from predicting\n",
    "#the appropriate value of individual knives\n",
    "def data_cleaner(df):\n",
    "    lot = re.compile('(?<!-\\S)lot(?![^\\s.,:?!])')\n",
    "    group = re.compile('(group)')\n",
    "    is_set = re.compile('(?<!-\\S)set(?![^\\s.,?!])')\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    trim_list = [lot,group,is_set]\n",
    "    for item in trim_list:\n",
    "        df.loc[df['title'].apply(lambda x: re.search(item, x)).notnull(), 'trim'] = 1 \n",
    "    to_drop = df.loc[df['trim'] == 1].index\n",
    "    df.drop(to_drop, inplace=True)\n",
    "    df.drop('trim', axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmade': 45.0,\n",
       " 'buck': 20.0,\n",
       " 'case': 20.0,\n",
       " 'crkt': 15.0,\n",
       " 'kershaw': 15.0,\n",
       " 'sog': 15.0,\n",
       " 'spyderco': 30.0,\n",
       " 'victorinox': 20.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of API calls for listed data. To be merged with item specific data using ebay itemIds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listed Data\n",
    "\n",
    "Running functions to call the Finding API and return datasets for cat () knives for sale listed on ebay in the last 90 days. (explain how ebay rules work)\n",
    "\n",
    "```\n",
    "bench_df = knife_request('Benchmade', 0)\n",
    "buck_df = knife_request('Buck', 1)\n",
    "case_df = knife_request('Case', 2)\n",
    "df_caseXX = knife_request('Case XX', 2)\n",
    "df_crkt = knife_request(\"CRKT\", 3)\n",
    "df_sog = knife_request('SOG', 5)\n",
    "df_spyderco = knife_request('Spyderco', 6)\n",
    "\n",
    "\n",
    "bench_df.to_csv('listed_data/df_bench1.csv', index=False)\n",
    "buck_df.to_csv('listed_data/df_buck.csv', index=False)\n",
    "case_df.to_csv('listed_data/df_case.csv', index=False)\n",
    "df_caseXX.to_csv('listed_data/df_CaseXX.csv', index=False)\n",
    "df_crkt.to_csv('listed_data/df_crkt.csv', index=False)\n",
    "df_sog.to_csv('listed_data/df_sog.csv', index=False)\n",
    "df_spyderco.to_csv('listed_data/df_spyderco.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kershaw and victorinox data was requested using the FindingAPI below after tweaking some pagination through trial and error to maximize data.\n",
    "\n",
    "```\n",
    "full_dataset = []\n",
    "for page in range(1, 57):\n",
    "#         # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "#         # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "        #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "\n",
    "    df = pd.DataFrame(full_dataset)\n",
    "    \n",
    "df_kershaw = prepare_df(df)\n",
    "df_kershaw = prepare_brands(df_kershaw, 4)\n",
    "df_kershaw.to_csv('listed_data/df_kershaw.csv', index=False)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "full_dataset = []\n",
    "for page in range(1, 86):\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "df_victorinox = pd.DataFrame(full_dataset)\n",
    "df_victorinox = prepare_df(df_victorinox)\n",
    "df_victorinox = prepare_brands(df_victorinox, 7)\n",
    "df_victorinox.to_csv('listed_data/df_victorinox.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start of API call section using IDs from preview listed datasets to get Item Specific data from ebay. This will return more descriptive information about the knives, pulling from a container on the website that sellers must complete to post a listing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = pd.read_csv(\"listed_data/df_bench.csv\")\n",
    "df_buck = pd.read_csv(\"listed_data/df_buck.csv\")\n",
    "df_case = pd.read_csv(\"listed_data/df_case.csv\")\n",
    "df_caseXX = pd.read_csv(\"listed_data/df_CaseXX.csv\")\n",
    "df_crkt = pd.read_csv(\"listed_data/df_crkt.csv\")\n",
    "df_kersh = pd.read_csv(\"listed_data/df_kershaw.csv\")\n",
    "df_sog = pd.read_csv(\"listed_data/df_sog.csv\")\n",
    "df_spyd = pd.read_csv(\"listed_data/df_spyderco.csv\")\n",
    "df_vict = pd.read_csv(\"listed_data/df_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_bench,df_buck,\n",
    "           df_case,df_caseXX,\n",
    "           df_crkt,df_kersh,\n",
    "           df_sog,df_spyd,\n",
    "           df_vict]\n",
    "\n",
    "for dataframe in df_list:\n",
    "    dataframe.drop('galleryPlusPictureURL', axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchIds = df_bench.itemId.values.tolist()\n",
    "buckIds = df_buck.itemId.values.tolist()\n",
    "caseIds = df_case.itemId.values.tolist()\n",
    "caseXXIds = df_caseXX.itemId.values.tolist()\n",
    "crktIds = df_crkt.itemId.values.tolist()\n",
    "kershawIds = df_kersh.itemId.values.tolist()\n",
    "sogIds = df_sog.itemId.values.tolist()\n",
    "spydIds = df_spyd.itemId.values.tolist()\n",
    "victIds = df_vict.itemId.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return benchmade item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(benchIds), 20):\n",
    "    process_list(benchIds[i:i+20])\n",
    "\n",
    "bench = pd.DataFrame(full_dataset)\n",
    "bench.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "bench.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return buck item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(buckIds), 20):\n",
    "    process_list(buckIds[i:i+20])\n",
    "\n",
    "buck = pd.DataFrame(full_dataset)\n",
    "buck.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "buck.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return case brand item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(caseIds), 20):\n",
    "    process_list(caseIds[i:i+20])\n",
    "\n",
    "df_case = pd.DataFrame(full_dataset)\n",
    "df_case.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_case.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return caseXX brand item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(caseXXIds), 20):\n",
    "    process_list(caseXXIds[i:i+20])\n",
    "\n",
    "df_caseXX = pd.DataFrame(full_dataset)\n",
    "df_caseXX.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_caseXX.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return crkt item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(crktIds), 20):\n",
    "    process_list(crktIds[i:i+20])\n",
    "\n",
    "crkt = pd.DataFrame(full_dataset)\n",
    "crkt.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "crkt.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return kershaw item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(kershawIds), 20):\n",
    "    process_list(kershawIds[i:i+20])\n",
    "\n",
    "kershaw = pd.DataFrame(full_dataset)\n",
    "kershaw.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "kershaw.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return SOG item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(sogIds), 20):\n",
    "    process_list(sogIds[i:i+20])\n",
    "\n",
    "sog = pd.DataFrame(full_dataset)\n",
    "sog.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "sog.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ShoppingAPI call to return spyderco item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(spydIds), 20):\n",
    "    process_list(spydIds[i:i+20])\n",
    "spyd = pd.DataFrame(full_dataset)\n",
    "spyd.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "spyd.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return victorinox item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(victIds), 20):\n",
    "    process_list(victIds[i:i+20])\n",
    "    \n",
    "vict = pd.DataFrame(full_dataset)\n",
    "vict.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "vict.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bench.to_csv(\"listed_data/benchIds.csv\", index=False)\n",
    "buck.to_csv(\"listed_data/buckIds.csv\", index=False)\n",
    "df_case.to_csv(\"listed_data/caseIds.csv\", index=False)\n",
    "df_caseXX.to_csv(\"listed_data/caseXXIds.csv\", index=False)\n",
    "crkt.to_csv(\"listed_data/crktIds.csv\", index=False)\n",
    "kershaw.to_csv(\"listed_data/kershawIds.csv\", index=False)\n",
    "leath.to_csv(\"listed_data/leathIds.csv\", index=False)\n",
    "sog.to_csv(\"listed_data/sogIds.csv\", index=False)\n",
    "spyd.to_csv(\"listed_data/spydIds.csv\", index=False)\n",
    "vict.to_csv(\"listed_data/victIds.csv\", index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of prep to merge original listed data with item specific data requested using a seperate API for more complete details about all listings gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv(\"listed_data/benchIds.csv\")\n",
    "buck = pd.read_csv(\"listed_data/buckIds.csv\")\n",
    "case = pd.read_csv(\"listed_data/caseIds.csv\")\n",
    "caseXX = pd.read_csv(\"listed_data/caseXXIds.csv\")\n",
    "crkt = pd.read_csv(\"listed_data/crktIds.csv\")\n",
    "kershaw = pd.read_csv(\"listed_data/kershawIds.csv\")\n",
    "sog = pd.read_csv(\"listed_data/sogIds.csv\")\n",
    "spyd = pd.read_csv(\"listed_data/spydIds.csv\")\n",
    "vict = pd.read_csv(\"listed_data/victIds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids_df = pd.concat([bench,buck,\n",
    "                   case,caseXX,\n",
    "                   crkt,kershaw,\n",
    "                   sog,spyd,vict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [bench,buck,\n",
    "           case,caseXX,\n",
    "           crkt,kershaw,\n",
    "           sog,spyd]\n",
    "\n",
    "for dataframe in df_list:\n",
    "    display(dataframe.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [bench,buck,\n",
    "           case,caseXX,\n",
    "           crkt,kershaw,\n",
    "           sog,spyd,vict]\n",
    "\n",
    "for dataframe in df_list:\n",
    "    dataframe.rename({'Title': 'title',\n",
    "                      'ItemID': 'itemId'},\n",
    "                     axis=1,inplace=True)\n",
    "    \n",
    "    dataframe.drop(['ConditionID','ConvertedCurrentPrice'], \n",
    "                   axis=1, inplace=True)\n",
    "    dataframe['title'] = dataframe['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge Item Specific dataframes with original listed data using itemIds and title\n",
    "bench_merged = df_bench.merge(bench)\n",
    "buck_merged = df_buck.merge(buck)\n",
    "case_merged = df_case.merge(case)\n",
    "caseXX_merged = df_caseXX.merge(caseXX)\n",
    "crkt_merged = df_crkt.merge(crkt)\n",
    "kershaw_merged = df_kersh.merge(kershaw)\n",
    "spyd_merged = df_spyd.merge(spyd)\n",
    "sog_merged = df_sog.merge(sog)\n",
    "vict_merged = df_vict.merge(vict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_spec = transform_item_specifics(bench_merged)\n",
    "buck_spec = transform_item_specifics(buck_merged)\n",
    "case_spec = transform_item_specifics(case_merged)\n",
    "caseXX_spec = transform_item_specifics(caseXX_merged)\n",
    "crkt_spec = transform_item_specifics(crkt_merged)\n",
    "kershaw_spec = transform_item_specifics(kershaw_merged)\n",
    "sog_spec = transform_item_specifics(sog_merged)\n",
    "spyd_spec = transform_item_specifics(spyd_merged)\n",
    "vict_spec = transform_item_specifics(vict_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs_list = [bench_spec, buck_spec,\n",
    "              case_spec, caseXX_spec,\n",
    "              crkt_spec, kershaw_spec,\n",
    "              sog_spec, spyd_spec,\n",
    "              vict_spec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in specs_list:\n",
    "    dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in specs_list:\n",
    "    dataframe.drop('Brand', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_bench = bench_merged.join(bench_spec)\n",
    "tot_buck = buck_merged.join(buck_spec)\n",
    "tot_case = case_merged.join(case_spec)\n",
    "tot_caseXX = caseXX_merged.join(caseXX_spec)\n",
    "tot_crkt = crkt_merged.join(crkt_spec)\n",
    "tot_kershaw = kershaw_merged.join(kershaw_spec)\n",
    "tot_sog = sog_merged.join(sog_spec)\n",
    "tot_spyd = spyd_merged.join(spyd_spec)\n",
    "tot_vict = vict_merged.join(vict_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_bench.to_csv('listed_data/total_list_bench.csv', index=False)\n",
    "tot_buck.to_csv('listed_data/total_list_buck.csv', index=False)\n",
    "tot_case.to_csv('listed_data/total_list_case.csv', index=False)\n",
    "tot_caseXX.to_csv('listed_data/total_list_caseXX.csv', index=False)\n",
    "tot_crkt.to_csv('listed_data/total_list_crkt.csv', index=False)\n",
    "tot_kershaw.to_csv('listed_data/total_list_kershaw.csv', index=False)\n",
    "tot_sog.to_csv('listed_data/total_list_sog.csv', index=False)\n",
    "tot_spyd.to_csv('listed_data/total_list_spyd.csv', index=False)\n",
    "tot_vict.to_csv('listed_data/total_list_vict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = pd.read_csv(\"listed_data/df_bench.csv\")\n",
    "df_buck = pd.read_csv(\"listed_data/df_buck.csv\")\n",
    "df_case = pd.read_csv(\"listed_data/df_case.csv\")\n",
    "df_caseXX = pd.read_csv(\"listed_data/df_CaseXX.csv\")\n",
    "df_crkt = pd.read_csv(\"listed_data/df_crkt.csv\")\n",
    "df_kersh = pd.read_csv(\"listed_data/df_kershaw.csv\")\n",
    "df_sog = pd.read_csv(\"listed_data/df_sog.csv\")\n",
    "df_spyd = pd.read_csv(\"listed_data/df_spyderco.csv\")\n",
    "df_vict = pd.read_csv(\"listed_data/df_victorinox.csv\")\n",
    "\n",
    "\n",
    "\n",
    "bench = pd.read_csv(\"listed_data/benchIds.csv\")\n",
    "buck = pd.read_csv(\"listed_data/buckIds.csv\")\n",
    "case = pd.read_csv(\"listed_data/caseIds.csv\")\n",
    "caseXX = pd.read_csv(\"listed_data/caseXXIds.csv\")\n",
    "crkt = pd.read_csv(\"listed_data/crktIds.csv\")\n",
    "kershaw = pd.read_csv(\"listed_data/kershawIds.csv\")\n",
    "sog = pd.read_csv(\"listed_data/sogIds.csv\")\n",
    "spyd = pd.read_csv(\"listed_data/spydIds.csv\")\n",
    "vict = pd.read_csv(\"listed_data/victIds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_df = pd.concat([df_bench,df_buck,\n",
    "                       df_case,df_caseXX,\n",
    "                       df_crkt,df_kersh,\n",
    "                       df_sog,df_spyd,\n",
    "                       df_vict])\n",
    "\n",
    "listed_df.drop('galleryPlusPictureURL', axis=1, inplace=True)\n",
    "\n",
    "Ids_df = pd.concat([bench,buck,\n",
    "                   case,caseXX,\n",
    "                   crkt,kershaw,\n",
    "                   sog,spyd,vict])\n",
    "\n",
    "\n",
    "\n",
    "Ids_df.rename({'Title': 'title',\n",
    "               'ItemID': 'itemId'},\n",
    "               axis=1,inplace=True)\n",
    "    \n",
    "Ids_df.drop(['ConditionID','ConvertedCurrentPrice'], \n",
    "             axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ids_df['title'] = Ids_df['title'].str.lower()\n",
    "\n",
    "\n",
    "df_merged = listed_df.merge(Ids_df)\n",
    "\n",
    "df_spec = transform_item_specifics(df_merged, perc=65.0)\n",
    "\n",
    "df_spec.drop('Brand', axis=1, inplace=True)\n",
    "\n",
    "tot_listed_df = df_merged.join(df_spec)\n",
    "\n",
    "listed_knives = data_cleaner(tot_listed_df).copy()\n",
    "listed_knives.drop(['sellingStatus', 'shippingInfo', \n",
    "                    'GalleryURL', 'ItemSpecifics', \n",
    "                    'item_list', 'listingInfo'], \n",
    "                    axis=1, inplace=True)\n",
    "listed_used_knives = listed_knives.loc[listed_knives['condition'] != 1000.0]\n",
    "listed_used_knives.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_used_knives.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives.drop(['Original/Reproduction'],\n",
    "                    axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives.to_csv(\"listed_data/listed_knives_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of section for obtaining listed data from eBay APIs. Below is the start of processing scraped data from eBay's seller exclusive website. This data goes back 2 years and is filtered to include only used knives with final sale values. The listed data above only goes back 90 days and only shows listings currently up for sale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_bench = pd.read_csv(\"terapeak_data/bench_scraped2.csv\")\n",
    "sold_buck1 = pd.read_csv(\"terapeak_data/buck_scraped2.csv\")\n",
    "sold_buck2 = pd.read_csv(\"terapeak_data/buck_scraped2_reversed.csv\")\n",
    "sold_case = pd.read_csv(\"terapeak_data/case_scraped2.csv\")\n",
    "sold_caseXX1 = pd.read_csv(\"terapeak_data/caseXX_scraped2.csv\")\n",
    "sold_caseXX2 = pd.read_csv(\"terapeak_data/caseXX2_reversed.csv\")\n",
    "sold_crkt = pd.read_csv(\"terapeak_data/crkt_scraped.csv\")\n",
    "sold_kershaw1 = pd.read_csv(\"terapeak_data/kershaw_scraped2.csv\")\n",
    "sold_kershaw2 = pd.read_csv(\"terapeak_data/kershaw_scraped2_reversed.csv\")\n",
    "sold_sog = pd.read_csv(\"terapeak_data/SOG_scraped2.csv\")\n",
    "sold_spyd = pd.read_csv(\"terapeak_data/spyd_scraped2.csv\")\n",
    "sold_vict1 = pd.read_csv(\"terapeak_data/vict_scraped.csv\")\n",
    "sold_vict2 = pd.read_csv(\"terapeak_data/vict_reversed.csv\")\n",
    "\n",
    "sold_list = [sold_bench,sold_buck1,\n",
    "             sold_buck2,sold_case,\n",
    "             sold_caseXX1,sold_caseXX2,\n",
    "             sold_crkt,sold_kershaw1,\n",
    "             sold_kershaw2,sold_sog, \n",
    "             sold_spyd, sold_vict1,\n",
    "             sold_vict2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmade\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8791 entries, 0 to 8790\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        8791 non-null   object\n",
      " 1   url          1843 non-null   object\n",
      " 2   date_sold    8791 non-null   object\n",
      " 3   price_in_US  8791 non-null   object\n",
      " 4   shipping_    8791 non-null   object\n",
      " 5   Text         8791 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 412.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buck1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9999 entries, 0 to 9998\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        9999 non-null   object\n",
      " 1   url          3659 non-null   object\n",
      " 2   date_sold    9999 non-null   object\n",
      " 3   price_in_US  9999 non-null   object\n",
      " 4   shipping_    9999 non-null   object\n",
      " 5   Text         9999 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 468.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buck2\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8918 entries, 0 to 8917\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        8918 non-null   object\n",
      " 1   url          4 non-null      object\n",
      " 2   date_sold    8918 non-null   object\n",
      " 3   price_in_US  8918 non-null   object\n",
      " 4   shipping_    8918 non-null   object\n",
      " 5   Text         8918 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 418.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9999 entries, 0 to 9998\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        9999 non-null   object\n",
      " 1   url          2198 non-null   object\n",
      " 2   date_sold    9999 non-null   object\n",
      " 3   price_in_US  9999 non-null   object\n",
      " 4   shipping_    9999 non-null   object\n",
      " 5   Text         9999 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 468.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caseXX1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9999 entries, 0 to 9998\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        9999 non-null   object\n",
      " 1   url          3309 non-null   object\n",
      " 2   date_sold    9999 non-null   object\n",
      " 3   price_in_US  9999 non-null   object\n",
      " 4   shipping_    9999 non-null   object\n",
      " 5   Text         9999 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 468.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caseXX2\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8494 entries, 0 to 8493\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        8494 non-null   object\n",
      " 1   url          10 non-null     object\n",
      " 2   date_sold    8494 non-null   object\n",
      " 3   price_in_US  8494 non-null   object\n",
      " 4   shipping_    8494 non-null   object\n",
      " 5   Text         8494 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 398.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crkt\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6742 entries, 0 to 6741\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        6742 non-null   object\n",
      " 1   url          1272 non-null   object\n",
      " 2   date_sold    6742 non-null   object\n",
      " 3   price_in_US  6742 non-null   object\n",
      " 4   shipping_    6742 non-null   object\n",
      " 5   Text         6742 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 316.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kershaw1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        10000 non-null  object\n",
      " 1   url          3370 non-null   object\n",
      " 2   date_sold    10000 non-null  object\n",
      " 3   price_in_US  10000 non-null  object\n",
      " 4   shipping_    10000 non-null  object\n",
      " 5   Text         10000 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 468.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kershaw2\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9447 entries, 0 to 9446\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        9447 non-null   object\n",
      " 1   url          11 non-null     object\n",
      " 2   date_sold    9447 non-null   object\n",
      " 3   price_in_US  9447 non-null   object\n",
      " 4   shipping_    9447 non-null   object\n",
      " 5   Text         9447 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 443.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sog\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4858 entries, 0 to 4857\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        4858 non-null   object\n",
      " 1   url          900 non-null    object\n",
      " 2   date_sold    4858 non-null   object\n",
      " 3   price_in_US  4858 non-null   object\n",
      " 4   shipping_    4858 non-null   object\n",
      " 5   Text         4858 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 227.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spyderco\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9206 entries, 0 to 9205\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        9206 non-null   object\n",
      " 1   url          1936 non-null   object\n",
      " 2   date_sold    9206 non-null   object\n",
      " 3   price_in_US  9206 non-null   object\n",
      " 4   shipping_    9206 non-null   object\n",
      " 5   Text         9206 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 431.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vict1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7799 entries, 0 to 7798\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        7799 non-null   object\n",
      " 1   url          6115 non-null   object\n",
      " 2   date_sold    7799 non-null   object\n",
      " 3   price_in_US  7799 non-null   object\n",
      " 4   shipping_    7799 non-null   object\n",
      " 5   Text         7799 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 365.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vict2\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7068 entries, 0 to 7067\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Image        7068 non-null   object\n",
      " 1   url          20 non-null     object\n",
      " 2   date_sold    7068 non-null   object\n",
      " 3   price_in_US  7068 non-null   object\n",
      " 4   shipping_    7068 non-null   object\n",
      " 5   Text         7068 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 331.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_dict = {'benchmade': sold_bench, \n",
    "           'buck1': sold_buck1,\n",
    "           'buck2': sold_buck2,\n",
    "           'case':sold_case,\n",
    "           'caseXX1':sold_caseXX1,\n",
    "           'caseXX2':sold_caseXX2,\n",
    "           'crkt':sold_crkt,\n",
    "           'kershaw1':sold_kershaw1,\n",
    "           'kershaw2':sold_kershaw2,\n",
    "           'sog':sold_sog, \n",
    "           'spyderco':sold_spyd,\n",
    "           'vict1':sold_vict1,\n",
    "           'vict2':sold_vict2}\n",
    "          \n",
    "\n",
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in df_dict.values():\n",
    "    val.rename({'Text': 'title',\n",
    "                'shipping_': 'shipping_cost'},\n",
    "               axis=1, inplace=True)\n",
    "\n",
    "    val['date_sold'] = pd.to_datetime(val['date_sold'])\n",
    "\n",
    "sold_buck = pd.concat([sold_buck1,sold_buck2])\n",
    "sold_caseXX = pd.concat([sold_caseXX1,sold_caseXX2])\n",
    "sold_kershaw = pd.concat([sold_kershaw1,sold_kershaw2])\n",
    "sold_vict = pd.concat([sold_vict1,sold_vict2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmade\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buck1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buck2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caseXX1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caseXX2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crkt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kershaw1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kershaw2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spyderco\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vict1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vict2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'date_sold', 'price_in_US', 'shipping_cost', 'title'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sold_buck.drop_duplicates(\n",
    "#     subset = ['date_sold', 'price_in_US', 'shipping_cost'],\n",
    "#     keep = 'last', inplace=True)\n",
    "\n",
    "# sold_caseXX.drop_duplicates(\n",
    "#     subset = ['date_sold', 'price_in_US', 'shipping_cost'],\n",
    "#     keep = 'last', inplace=True)\n",
    "\n",
    "# sold_kershaw.drop_duplicates(\n",
    "#     subset = ['date_sold', 'price_in_US', 'shipping_cost'],\n",
    "#     keep = 'last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_bench = prepare_tera_df(sold_bench, 0)\n",
    "sold_buck = prepare_tera_df(sold_buck, 1)\n",
    "sold_case = prepare_tera_df(sold_case, 2)\n",
    "sold_caseXX = prepare_tera_df(sold_caseXX, 2)\n",
    "sold_crkt = prepare_tera_df(sold_crkt, 3)\n",
    "sold_kershaw = prepare_tera_df(sold_kershaw, 4)\n",
    "sold_sog = prepare_tera_df(sold_sog, 5)\n",
    "sold_spyd = prepare_tera_df(sold_spyd, 6)\n",
    "sold_vict = prepare_tera_df(sold_vict, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in df_dict.values():\n",
    "    dataframe['title'] = dataframe['title'].str.lower()\n",
    "    dataframe['title'] = dataframe['title'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_df = pd.concat([sold_bench, sold_buck,\n",
    "                     sold_case, sold_caseXX, \n",
    "                     sold_crkt, sold_kershaw,\n",
    "                     sold_sog, sold_spyd,\n",
    "                     sold_vict]) \n",
    "\n",
    "sold_df['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_df.to_csv(\"terapeak_data/terapeak_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_knives = data_cleaner(sold_df).copy()\n",
    "sold_knives.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_knives.brand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sold_knives.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_knives['cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_bench.to_csv(\"terapeak_data/tera_bench_prepared.csv\", index=False)\n",
    "sold_buck.to_csv(\"terapeak_data/tera_buck_prepared.csv\", index=False)\n",
    "sold_case.to_csv(\"terapeak_data/tera_case_prepared.csv\", index=False)\n",
    "sold_caseXX.to_csv(\"terapeak_data/tera_caseXX_prepared.csv\", index=False)\n",
    "sold_crkt.to_csv(\"terapeak_data/tera_crkt_prepared.csv\", index=False)\n",
    "sold_kershaw.to_csv(\"terapeak_data/tera_kershaw_prepared.csv\", index=False)\n",
    "sold_sog.to_csv(\"terapeak_data/tera_sog_prepared.csv\", index=False)\n",
    "sold_spyd.to_csv(\"terapeak_data/tera_spyd_prepared.csv\", index=False)\n",
    "sold_knives.to_csv(\"terapeak_data/sold_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below block of code merged all available teraform ebay itemIds with the appropriate data. This was done in order to call the ebay Shopping API that will only accept itemIds as input. However, much of the data is older than 90 days and can no longer be accessed using the ebay Shopping API. Therefore, the teraform data will unfortunatly lack additional item specific data.  \n",
    "\n",
    "```\n",
    "teradf_benchIDs = pd.read_csv(\"teraform_data/tera_benchmade_itemID.csv\")\n",
    "teradf_buckIDs = pd.read_csv(\"teraform_data/tera_buck_ItemIDs.csv\")\n",
    "teradf_caseIDs = pd.read_csv(\"teraform_data/tera_case_itemIDs.csv\")\n",
    "teradf_kershawIDs = pd.read_csv(\"teraform_data/tera_kershaw_ItemIDs.csv\")\n",
    "teradf_sogIDs = pd.read_csv(\"teraform_data/tera_sog_ItemIDs.csv\")\n",
    "teradf_spydIDs = pd.read_csv(\"teraform_data/tera_spyderco_ItemIDs.csv\")\n",
    "\n",
    "dfID_list = [teradf_benchIDs,teradf_buckIDs,\n",
    "             teradf_caseIDs, teradf_kershawIDs,\n",
    "             teradf_sogIDs, teradf_spydIDs]\n",
    "\n",
    "for dataframe in dfID_list:\n",
    "    dataframe.rename({'Field4': 'date_sold',\n",
    "                      'Data_field': 'itemID',\n",
    "                      'Title': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "    \n",
    "teradf_kershawIDs.rename({'item': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "                       \n",
    "for dataframe in dfID_list:\n",
    "    dataframe.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "for dataframe in dfID_list:\n",
    "    dataframe.rename({'Field4': 'date_sold',\n",
    "                      'Data_field': 'itemID',\n",
    "                      'Title': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "    dataframe.dropna(inplace=True)\n",
    "    dataframe['itemID'] = dataframe['itemID'].apply(int)\n",
    "\n",
    "teradf_kershawIDs.rename({'item': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "\n",
    "tera_benchIds = teradf_benchIDs.itemID.values.tolist()\n",
    "tera_buckIds = teradf_buckIDs.itemID.values.tolist()\n",
    "tera_caseIds = teradf_caseIDs.itemID.values.tolist()\n",
    "tera_kershawIds = teradf_kershawIDs.itemID.values.tolist()\n",
    "tera_sogIds = teradf_sogIDs.itemID.values.tolist()\n",
    "tera_spydIds = teradf_spydIDs.itemID.values.tolist()\n",
    "\n",
    "idMerge_bench = teradf_bench.merge(teradf_benchIDs, on='Image')\n",
    "idMerge_buck = teradf_buck.merge(teradf_buckIDs)\n",
    "idMerge_case = teradf_case.merge(teradf_caseIDs)\n",
    "idMerge_kershaw = teradf_kershaw.merge(teradf_kershawIDs)\n",
    "idMerge_spyd = teradf_spyd.merge(teradf_spydIDs)\n",
    "idMerge_sog = teradf_sog.merge(teradf_sogIDs)\n",
    "\n",
    "# idMerge_bench.to_csv('teraform_data/tera_bench_idMerge.csv', index=False)\n",
    "# idMerge_buck.to_csv('teraform_data/tera_buck_idMerge.csv', index=False)\n",
    "# idMerge_case.to_csv('teraform_data/tera_case_idMerge.csv', index=False)\n",
    "# idMerge_kershaw.to_csv('teraform_data/tera_kershaw_idMerge.csv', index=False)\n",
    "# idMerge_spyd.to_csv('teraform_data/tera_spyd_idMerge.csv', index=False)\n",
    "# idMerge_sog.to_csv('teraform_data/tera_sog_idMerge.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "    price_list.append(listed_price)\n",
    "    \n",
    "df['price_in_US'] = price_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#atttempt to pull shipping cost from json dict\n",
    "shipping_cost_list = []\n",
    "for row in full_dataset:\n",
    "    shipping_cost = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "    shipping_cost_list.append(shipping_cost)\n",
    "    \n",
    "df['shipping_price'] = shipping_cost_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#pull shipping cost from json dict with regex \n",
    "df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#create new feature 'converted price'\n",
    "df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "df = df.drop_duplicates(subset=['title', 'galleryURL'], keep='first')\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "\n",
    "df.to_csv('data/full_dataset.csv', index=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
