{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.finding import Connection\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.shopping import Connection as Shopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import  json\n",
    "import numpy as np\n",
    "import re\n",
    "# import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create function for organizing API call\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['itemId', 'title', 'galleryURL', \n",
    "#         'viewItemURL', 'autoPay', 'postalCode', \n",
    "#         'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "#         'returnsAccepted', 'condition', 'topRatedListing',\n",
    "#         'galleryPlusPictureURL']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         # Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_df(df):\n",
    "#     price_list = []\n",
    "#     for row in full_dataset:\n",
    "#         listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#         price_list.append(listed_price)\n",
    "\n",
    "#     df['price_in_US'] = price_list\n",
    "#     #pull shipping cost from json dict with regex \n",
    "#     df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "#     df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#     #create new feature 'converted price'\n",
    "#     df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#     df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     display(df.head())\n",
    "#     display(df.info())\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    condition = None\n",
    "    for row in full_dataset:\n",
    "        listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "        price_list.append(listed_price)\n",
    "     \n",
    "        try:\n",
    "            listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "\n",
    "        try:\n",
    "            condition = float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0,\n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "overhead_cost = 3\n",
    "def prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    " \n",
    "    #remove special characters\n",
    "#     df.title.apply(pp.remove_special_chars)\n",
    "    df['brand'] = str(list(bucket_dict.keys())[bucket_dict_position])\n",
    "    df['cost'] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    df['profit'] = (df['converted_price'] -  df['cost'] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/( df['cost'] + overhead_cost))*100.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "                'viewItemURL', 'autoPay', 'postalCode', \n",
    "                'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "                'returnsAccepted', 'condition', 'topRatedListing',\n",
    "                'galleryPlusPictureURL','pictureURLLarge', \n",
    "                'pictureURLSuperSize']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "\n",
    "def knife_request(Brand, dict_pos):\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': 1\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "    #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "\n",
    "    response_pages = response.dict()\n",
    "\n",
    "    full_dataset = []\n",
    "    \n",
    "    total_pages = int(response_pages['paginationOutput']['totalPages'])\n",
    "\n",
    "    if total_pages > 100:\n",
    "        pages_to_request = 100\n",
    "        \n",
    "    else:\n",
    "        pages_to_request = total_pages - 1\n",
    "        \n",
    "        \n",
    "\n",
    "    for page in range(1, pages_to_request):\n",
    "        # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "        # Make the query and get the response\n",
    "\n",
    "        api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "        request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "\n",
    "        response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "        #save the response as a json dict\n",
    "        response_dict = response.dict()\n",
    "\n",
    "\n",
    "        #index dict to appropriate index\n",
    "        results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "        # Call the prepare_data function to get a list of processed data\n",
    "        prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "        # Extend full_dataset with this list (don't append, or you'll get\n",
    "        # a list of lists instead of a flat list)\n",
    "        full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "    display(len(full_dataset))\n",
    "    \n",
    "    df = pd.DataFrame(full_dataset)\n",
    "    \n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    condition = None\n",
    "    for row in full_dataset:\n",
    "        try:\n",
    "            listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "            price_list.append(listed_price)\n",
    "        except:\n",
    "            listed_price = \"Na\"\n",
    "            price_list.append(listed_price)\n",
    "        try:\n",
    "            listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        try:\n",
    "            condition = float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df = prepare_brands(df, dict_pos)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_dataIds(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID','GalleryURL','PictureURL',\n",
    "                'Location','ConvertedCurrentPrice',\n",
    "                'Title','ItemSpecifics', \n",
    "                'Country','ConditionID']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_list(my_list):\n",
    " \n",
    "    api = Shopping(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "    request = {\n",
    "               'itemID': my_list,\n",
    "               'IncludeSelector': 'ItemSpecifics'\n",
    "              }\n",
    "    response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_dataIds(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "    return full_dataset\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "#x = position of bucket_dictionary\n",
    "def prepare_tera_df(df, x):\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\"$\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\",\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].apply(float)\n",
    "    \n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\"$\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\",\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].apply(float)\n",
    "    \n",
    "    df['converted_price'] = (df['price_in_US'] + df['shipping_cost'])\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - list(bucket_dict.values())[x])\n",
    "    df['ROI'] = (df['profit']/(list(bucket_dict.values())[x]))*100.0\n",
    "    \n",
    "    df['brand'] = list(bucket_dict.keys())[x]\n",
    "    df['cost'] = list(bucket_dict.values())[x]\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of API calls for listed data. To be merged with item specific data using ebay itemIds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_df = knife_request('Benchmade', 0)\n",
    "buck_df = knife_request('Buck', 1)\n",
    "case_df = knife_request('Case', 2)\n",
    "df_caseXX = knife_request('Case XX', 2)\n",
    "df_crkt = knife_request(\"CRKT\", 3)\n",
    "df_leatherman = knife_request('Leatherman', 5)\n",
    "df_sog = knife_request('SOG', 6)\n",
    "df_spyderco = knife_request('Spyderco', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_df.to_csv('data/df_bench1.csv', index=False)\n",
    "buck_df.to_csv('data/df_buck.csv', index=False)\n",
    "case_df.to_csv('data/df_case.csv', index=False)\n",
    "df_caseXX.to_csv('data/df_CaseXX.csv', index=False)\n",
    "df_crkt.to_csv('data/df_crkt.csv', index=False)\n",
    "df_leatherman.to_csv('data/df_leatherman.csv', index=False)\n",
    "df_sog.to_csv('data/df_sog.csv', index=False)\n",
    "df_spyderco.to_csv('data/df_spyderco.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for page in range(1, 57):\n",
    "#         # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "#         # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "        #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "\n",
    "    df = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kershaw = prepare_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kershaw = prepare_brands(df_kershaw, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kershaw.to_csv('data/df_kershaw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for page in range(1, 86):\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "df_victorinox = pd.DataFrame(full_dataset)\n",
    "df_victorinox = prepare_df(df_victorinox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_victorinox = prepare_brands(df_victorinox, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_victorinox.to_csv('data/df_victorinox.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end of API call for listed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start of API call section using IDs from preview listed datasets to get Item Specific data from ebay. This will return more descriptive information about the knives, pulling from a container on the website that sellers must complete to post a listing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = pd.read_csv(\"listed_data/df_bench1.csv\")\n",
    "df_buck = pd.read_csv(\"listed_data/df_buck.csv\")\n",
    "df_case = pd.read_csv(\"listed_data/df_case.csv\")\n",
    "df_caseXX = pd.read_csv(\"listed_data/df_CaseXX.csv\")\n",
    "df_crkt = pd.read_csv(\"listed_data/df_crkt.csv\")\n",
    "df_kershaw = pd.read_csv(\"listed_data/df_kershaw.csv\")\n",
    "df_leatherman = pd.read_csv(\"listed_data/df_leatherman.csv\")\n",
    "df_sog = pd.read_csv(\"listed_data/df_sog.csv\")\n",
    "df_spyderco = pd.read_csv(\"listed_data/df_spyderco.csv\")\n",
    "df_victorinox = pd.read_csv(\"listed_data/df_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchIds = df_bench.itemId.values.tolist()\n",
    "buckIds = df_buck.itemId.values.tolist()\n",
    "caseIds = df_case.itemId.values.tolist()\n",
    "caseXXIds = df_caseXX.itemId.values.tolist()\n",
    "crktIds = df_crkt.itemId.values.tolist()\n",
    "kershawIds = df_kershaw.itemId.values.tolist()\n",
    "leathIds = df_leatherman.itemId.values.tolist()\n",
    "sogIds = df_sog.itemId.values.tolist()\n",
    "spydIds = df_spyderco.itemId.values.tolist()\n",
    "victIds = df_victorinox.itemId.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(benchIds), 20):\n",
    "    process_list(benchIds[i:i+20])\n",
    "\n",
    "bench = pd.DataFrame(full_dataset)\n",
    "bench.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "bench.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(buckIds), 20):\n",
    "    process_list(buckIds[i:i+20])\n",
    "\n",
    "buck = pd.DataFrame(full_dataset)\n",
    "buck.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "buck.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(caseIds), 20):\n",
    "    process_list(caseIds[i:i+20])\n",
    "\n",
    "df_case = pd.DataFrame(full_dataset)\n",
    "df_case.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_case.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(caseXXIds), 20):\n",
    "    process_list(caseXXIds[i:i+20])\n",
    "\n",
    "df_caseXX = pd.DataFrame(full_dataset)\n",
    "df_caseXX.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_caseXX.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(crktIds), 20):\n",
    "    process_list(crktIds[i:i+20])\n",
    "\n",
    "crkt = pd.DataFrame(full_dataset)\n",
    "crkt.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "crkt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(kershawIds), 20):\n",
    "    process_list(kershawIds[i:i+20])\n",
    "\n",
    "kershaw = pd.DataFrame(full_dataset)\n",
    "kershaw.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "kershaw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(leathIds), 20):\n",
    "    process_list(leathIds[i:i+20])\n",
    "\n",
    "leath = pd.DataFrame(full_dataset)\n",
    "leath.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "leath.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(sogIds), 20):\n",
    "    process_list(sogIds[i:i+20])\n",
    "\n",
    "sog = pd.DataFrame(full_dataset)\n",
    "sog.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "sog.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(spydIds), 20):\n",
    "    process_list(spydIds[i:i+20])\n",
    "\n",
    "spyd = pd.DataFrame(full_dataset)\n",
    "spyd.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "spyd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(victIds), 20):\n",
    "    process_list(victIds[i:i+20])\n",
    "    \n",
    "vict = pd.DataFrame(full_dataset)\n",
    "vict.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "vict.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv(\"listed_data/benchIds.csv\")\n",
    "buck = pd.read_csv(\"listed_data/buckIds.csv\")\n",
    "case = pd.read_csv(\"listed_data/caseIds.csv\")\n",
    "caseXX = pd.read_csv(\"listed_data/caseXXIds.csv\")\n",
    "crkt = pd.read_csv(\"listed_data/crktIds.csv\")\n",
    "kershaw = pd.read_csv(\"listed_data/kershawIds.csv\")\n",
    "leath = pd.read_csv(\"listed_data/leathIds.csv\")\n",
    "sog = pd.read_csv(\"listed_data/sogIds.csv\")\n",
    "spyd = pd.read_csv(\"listed_data/spydIds.csv\")\n",
    "vict = pd.read_csv(\"listed_data/victIds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2709 entries, 0 to 2708\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 2709 non-null   int64  \n",
      " 1   title                  2709 non-null   object \n",
      " 2   galleryURL             2707 non-null   object \n",
      " 3   viewItemURL            2709 non-null   object \n",
      " 4   autoPay                2709 non-null   bool   \n",
      " 5   postalCode             2673 non-null   object \n",
      " 6   sellingStatus          2709 non-null   object \n",
      " 7   shippingInfo           2709 non-null   object \n",
      " 8   listingInfo            2709 non-null   object \n",
      " 9   returnsAccepted        2709 non-null   bool   \n",
      " 10  condition              2709 non-null   float64\n",
      " 11  topRatedListing        2709 non-null   bool   \n",
      " 12  galleryPlusPictureURL  355 non-null    object \n",
      " 13  pictureURLLarge        2530 non-null   object \n",
      " 14  pictureURLSuperSize    2525 non-null   object \n",
      " 15  shipping_cost          2709 non-null   float64\n",
      " 16  price_in_US            2709 non-null   float64\n",
      " 17  converted_price        2709 non-null   float64\n",
      " 18  brand                  2709 non-null   object \n",
      " 19  cost                   2709 non-null   float64\n",
      " 20  profit                 2709 non-null   float64\n",
      " 21  ROI                    2709 non-null   float64\n",
      "dtypes: bool(3), float64(7), int64(1), object(11)\n",
      "memory usage: 410.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_buck.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2709 entries, 0 to 2708\n",
      "Data columns (total 9 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   ItemID                 2709 non-null   int64  \n",
      " 1   GalleryURL             2691 non-null   object \n",
      " 2   PictureURL             2709 non-null   object \n",
      " 3   Location               2709 non-null   object \n",
      " 4   ConvertedCurrentPrice  2709 non-null   object \n",
      " 5   Title                  2709 non-null   object \n",
      " 6   ItemSpecifics          2671 non-null   object \n",
      " 7   Country                2709 non-null   object \n",
      " 8   ConditionID            2387 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 190.6+ KB\n"
     ]
    }
   ],
   "source": [
    "buck.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [bench,buck,\n",
    "           case,caseXX,\n",
    "           crkt,kershaw,\n",
    "           leath,sog,\n",
    "           spyd,vict]\n",
    "\n",
    "for dataframe in df_list:\n",
    "    dataframe.rename({'Title': 'title',\n",
    "                      'ItemID': 'itemId'},\n",
    "                     axis=1,inplace=True)\n",
    "    \n",
    "    dataframe.drop(['ConditionID','ConvertedCurrentPrice'], \n",
    "                   axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1886 entries, 0 to 1885\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   itemId         1886 non-null   int64 \n",
      " 1   GalleryURL     1876 non-null   object\n",
      " 2   PictureURL     1886 non-null   object\n",
      " 3   Location       1886 non-null   object\n",
      " 4   title          1886 non-null   object\n",
      " 5   ItemSpecifics  1875 non-null   object\n",
      " 6   Country        1886 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 103.3+ KB\n"
     ]
    }
   ],
   "source": [
    "bench.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22    DISCONTINUED Benchmade Bedlam 860 Satin Plain ...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench.title.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(df):\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    return df\n",
    "\n",
    "bench = lower(bench)\n",
    "buck = lower(buck)\n",
    "case = lower(case)\n",
    "caseXX = lower(caseXX)\n",
    "crkt = lower(crkt)\n",
    "kershaw = lower(kershaw)\n",
    "leath = lower(leath)\n",
    "spyd = lower(spyd)\n",
    "sog = lower(sog)\n",
    "vict = lower(vict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6014    spyderco roadie notched joint blue plain sheep...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spyd.title.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge Item Specific dataframes with original listed data using itemIds and title\n",
    "bench_merged = df_bench.merge(bench)\n",
    "buck_merged = df_buck.merge(buck)\n",
    "case_merged = df_case.merge(case)\n",
    "caseXX_merged = df_caseXX.merge(caseXX)\n",
    "crkt_merged = df_crkt.merge(crkt)\n",
    "kershaw_merged = df_kershaw.merge(kershaw)\n",
    "leath_merged = df_leatherman.merge(leath)\n",
    "spyd_merged = df_spyderco.merge(spyd)\n",
    "sog_merged = df_sog.merge(sog)\n",
    "vict_merged = df_victorinox.merge(vict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge Item Specific dataframes with original listed data using itemIds and title\n",
    "\n",
    "bench_merged.to_csv('teraform_data/bench_merged.csv',index=False)\n",
    "buck_merged.to_csv('teraform_data/buck_merged.csv',index=False)\n",
    "case_merged.to_csv('teraform_data/case_merged.csv',index=False)\n",
    "caseXX_merged.to_csv('teraform_data/caseXX_merged.csv',index=False)\n",
    "crkt_merged.to_csv('teraform_data/crkt_merged.csv',index=False)\n",
    "kershaw_merged.to_csv('teraform_data/kershaw_merged.csv',index=False)\n",
    "sog_merged.to_csv('teraform_data/sog_merged.csv',index=False)\n",
    "spyd_merged.to_csv('teraform_data/spyd_merged.csv',index=False)\n",
    "vict_merged.to_csv('teraform_data/vict_merged.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench = pd.read_csv(\"teraform_data/tera_benchmade.csv\")\n",
    "teradf_buck = pd.read_csv(\"teraform_data/tera_buck.csv\")\n",
    "teradf_case = pd.read_csv(\"teraform_data/tera_case.csv\")\n",
    "teradf_crkt = pd.read_csv(\"teraform_data/tera_CRKT.csv\")\n",
    "teradf_kershaw = pd.read_csv(\"teraform_data/tera_kershaw.csv\")\n",
    "teradf_leath = pd.read_csv(\"teraform_data/tera_leatherman.csv\")\n",
    "teradf_sog = pd.read_csv(\"teraform_data/tera_SOG.csv\")\n",
    "teradf_spyd = pd.read_csv(\"teraform_data/tera_spyderco.csv\")\n",
    "teradf_vict = pd.read_csv(\"teraform_data/tera_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'benchmade': teradf_bench, \n",
    "           'buck': teradf_buck,\n",
    "           'case':teradf_case,\n",
    "           'crkt':teradf_crkt,\n",
    "           'kershaw':teradf_kershaw,\n",
    "           'leatherman':teradf_leath,\n",
    "           'sog':teradf_sog, \n",
    "           'spyderco':teradf_spyd,\n",
    "           'victorinox':teradf_vict}\n",
    "\n",
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench.rename({'Data_field':'title', \n",
    "                     'Data_field1':'price_in_US', \n",
    "                     'Data_field2':'shipping_cost', \n",
    "                     'Field2':'date_sold'},\n",
    "                     axis=1, inplace=True)\n",
    "\n",
    "teradf_buck.rename({'avg_price':'price_in_US', \n",
    "                    'avg_shipping':'shipping_cost'},\n",
    "                     axis=1, inplace=True)\n",
    "\n",
    "teradf_case.rename({'url':'title', \n",
    "                    'Field5':'title2', \n",
    "                    'Field4': 'date_sold',\n",
    "                    'Field': 'url',\n",
    "                    'avg_price': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "\n",
    "teradf_crkt.rename({'Field2':'date_sold', \n",
    "                    'title1':'title', \n",
    "                    'avg_cost': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "\n",
    "teradf_kershaw.rename({'Field':'url', \n",
    "                       'Field1':'title', \n",
    "                       'Field3':'date_sold',\n",
    "                       'avg_price': 'price_in_US',\n",
    "                       'avg_shipping': 'shipping_cost'\n",
    "                       }, axis=1, inplace=True)\n",
    "\n",
    "teradf_leath.rename({'Field2':'date_sold', \n",
    "                     'title1':'title2', \n",
    "                     'avg_price': 'price_in_US',\n",
    "                     'avg_shipping': 'shipping_cost'\n",
    "                     }, axis=1, inplace=True)\n",
    "\n",
    "teradf_sog.rename({'Field2':'date_sold', \n",
    "                   'title1':'title', \n",
    "                   'avg_price': 'price_in_US',\n",
    "                   'avg_shipping': 'shipping_cost'\n",
    "                   }, axis=1, inplace=True)\n",
    "\n",
    "teradf_spyd.rename({'Field2':'date_sold', \n",
    "                    'Field3':'title', \n",
    "                    'name':'title2',\n",
    "                    'avg_price': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "\n",
    "teradf_vict.rename({'FfImage':'Image', \n",
    "                    'Field3':'date_sold', \n",
    "                    'avg_price': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "# teradf_bench.drop(['Field', 'Price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_case.drop(['units_sold', 'Price'], axis=1, inplace=True)\n",
    "teradf_crkt.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_kershaw.drop(['Field2', 'Price'], axis=1, inplace=True)\n",
    "teradf_leath.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_sog.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_spyd.drop(['units_sold', 'Price'], axis=1, inplace=True)\n",
    "teradf_vict.drop(['Field2', 'Price'], axis=1, inplace=True)\n",
    "\n",
    "teradf_buck['title'].fillna(teradf_buck['title2'], inplace=True)\n",
    "teradf_case['title'].fillna(teradf_case['title2'], inplace=True)\n",
    "teradf_crkt['title'].fillna(teradf_crkt['title2'], inplace=True)\n",
    "teradf_leath['title'].fillna(teradf_leath['title2'], inplace=True)\n",
    "teradf_sog['title'].fillna(teradf_sog['title2'], inplace=True)\n",
    "teradf_spyd['title'].fillna(teradf_spyd['title2'], inplace=True)\n",
    "teradf_vict['title'].fillna(teradf_vict['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title2_list = [teradf_buck, teradf_case,\n",
    "               teradf_crkt, teradf_leath,\n",
    "               teradf_sog, teradf_spyd,\n",
    "               teradf_vict]\n",
    "               \n",
    "for dataframe in title2_list:\n",
    "    dataframe.drop('title2', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'benchmade': teradf_bench, \n",
    "           'buck': teradf_buck,\n",
    "           'case':teradf_case,\n",
    "           'crkt':teradf_crkt,\n",
    "           'kershaw':teradf_kershaw,\n",
    "           'leatherman':teradf_leath,\n",
    "           'sog':teradf_sog, \n",
    "           'spyderco':teradf_spyd,\n",
    "           'victorinox':teradf_vict}\n",
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench = prepare_tera_df(teradf_bench, 0)\n",
    "teradf_buck = prepare_tera_df(teradf_buck, 1)\n",
    "teradf_case = prepare_tera_df(teradf_case, 2)\n",
    "teradf_crkt = prepare_tera_df(teradf_crkt, 3)\n",
    "teradf_kershaw = prepare_tera_df(teradf_kershaw, 4)\n",
    "teradf_leath = prepare_tera_df(teradf_leath, 5)\n",
    "teradf_sog = prepare_tera_df(teradf_sog, 6)\n",
    "teradf_spyd = prepare_tera_df(teradf_spyd, 7)\n",
    "teradf_vict = prepare_tera_df(teradf_vict, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teradf_bench.to_csv(\"teraform_data/tera_bench_prepared.csv\", index=False)\n",
    "# teradf_buck.to_csv(\"teraform_data/tera_buck_prepared.csv\", index=False)\n",
    "# teradf_case.to_csv(\"teraform_data/tera_case_prepared.csv\", index=False)\n",
    "# teradf_crkt.to_csv(\"teraform_data/tera_crkt_prepared.csv\", index=False)\n",
    "# teradf_kershaw.to_csv(\"teraform_data/tera_kershaw_prepared.csv\", index=False)\n",
    "# teradf_leath.to_csv(\"teraform_data/tera_leatherman_prepared.csv\", index=False)\n",
    "# teradf_sog.to_csv(\"teraform_data/tera_sog_prepared.csv\", index=False)\n",
    "# teradf_spyd.to_csv(\"teraform_data/tera_spyd_prepared.csv\", index=False)\n",
    "# teradf_vict.to_csv(\"teraform_data/tera_victorinox_prepared.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench = pd.read_csv(\"teraform_data/tera_bench_prepared.csv\")\n",
    "teradf_buck = pd.read_csv(\"teraform_data/tera_buck_prepared.csv\")\n",
    "teradf_case = pd.read_csv(\"teraform_data/tera_case_prepared.csv\")\n",
    "teradf_crkt = pd.read_csv(\"teraform_data/tera_crkt_prepared.csv\")\n",
    "teradf_kershaw = pd.read_csv(\"teraform_data/tera_kershaw_prepared.csv\")\n",
    "teradf_leath = pd.read_csv(\"teraform_data/tera_leatherman_prepared.csv\")\n",
    "teradf_sog = pd.read_csv(\"teraform_data/tera_sog_prepared.csv\")\n",
    "teradf_spyd = pd.read_csv(\"teraform_data/tera_spyd_prepared.csv\")\n",
    "teradf_vict = pd.read_csv(\"teraform_data/tera_victorinox_prepared.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_benchIDs = pd.read_csv(\"teraform_data/tera_benchmade_itemID.csv\")\n",
    "tera_buckIDs = pd.read_csv(\"teraform_data/tera_buck_ItemIDs.csv\")\n",
    "tera_caseIDs = pd.read_csv(\"teraform_data/tera_case_itemIDs.csv\")\n",
    "tera_kershawIDs = pd.read_csv(\"teraform_data/tera_kershaw_ItemIDs.csv\")\n",
    "tera_sogIDs = pd.read_csv(\"teraform_data/tera_sog_ItemIDs.csv\")\n",
    "tera_spydIDs = pd.read_csv(\"teraform_data/tera_spyderco_ItemIDs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfID_list = [tera_benchIDs,tera_buckIDs,\n",
    "             tera_caseIDs, tera_kershawIDs,\n",
    "             tera_sogIDs, tera_spydIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dfID_list:\n",
    "    dataframe.rename({'Field4': 'date_sold',\n",
    "                        'Data_field': 'itemID'}, \n",
    "                       axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dfID_list:\n",
    "    dataframe.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dfID_list:\n",
    "    dataframe['itemID'] = dataframe['itemID'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_benchIDs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_bench,df_buck,df_case,df_crkt,df_kershaw,df_leatherman,df_spyderco,df_sog,df_victorinox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/full_dataset2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After succesfully going through 10,000 items on ebay's website and extracting everything possible, there is still a little bit more extracting to do from the json dictionary before saving the dataframe again. We need to get the price of the knives out of the nested dictionary in the dataframe as well as the shipping cost. After that, I would like to create a new feature called\n",
    "**\"converted price,\" which is simply the price of the knife listed on the ebay's website plus shipping.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tera_df_prepared.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create function for organizing API call\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "#                 'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "#                 'Country', 'Location', 'ConvertedCurrentPrice',\n",
    "#                 'PictureURL']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "# #         Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create function for organizing API call\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "#                 'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "\n",
    "#                 'PictureURL']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "# #         Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_list(my_list):\n",
    " \n",
    "#     api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "#     request = {\n",
    "#                'itemID': my_list,\n",
    "#                'IncludeSelector': 'ItemSpecifics'\n",
    "#               }\n",
    "#     response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "    \n",
    "#     return full_dataset\n",
    "\n",
    "# full_dataset = []\n",
    "# for i in range(0, len(itemIds), 20):\n",
    "#     process_list(itemIds[i:i+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset = []\n",
    "# for i in range(0, len(itemIds), 20):\n",
    "#     process_list(itemIds[i:i+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemIds = list(df.itemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset = []\n",
    "# for i in range(0, len(itemIds), 20):\n",
    "#     process_list(itemIds[i:i+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_opening_mech(line):\n",
    "#     pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['opening_mechanism'] = df3.ItemSpecifics.apply(extract_opening_mech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_type(line):\n",
    "#     pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.ItemSpecifics.sample(10).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_color(line):\n",
    "#     pattern = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['color'] = df3.ItemSpecifics.apply(extract_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_type(line):\n",
    "#     pattern = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_manufacture_region(line):\n",
    "#     pattern = re.compile(\"Country/Region of Manufacture\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['region_of_Manufacture'] = df3.ItemSpecifics.apply(extract_manufacture_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['region_of_Manufacture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_handle_material(line):\n",
    "#     pattern = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['handle_material'] = df3.ItemSpecifics.apply(extract_handle_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['handle_material'].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_lock_type(line):\n",
    "#     pattern = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "\n",
    "# df3['lock_type'] = df3.ItemSpecifics.apply(extract_lock_type)\n",
    "\n",
    "# df3['lock_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_edge(line):\n",
    "#     pattern = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['blade_edge'] = df3.ItemSpecifics.apply(extract_blade_edge)\n",
    "\n",
    "# df3['blade_edge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_dexterity(line):\n",
    "#     pattern = re.compile(\"Dexterity\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['dexterity'] = df3.ItemSpecifics.apply(extract_dexterity)\n",
    "\n",
    "# df3['dexterity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.to_csv('data/item_specifics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "# request = {\n",
    "#            'itemID': list(itemIds),\n",
    "#            'IncludeSelector': 'ItemSpecifics'\n",
    "#           }\n",
    "# response = api.execute('GetMultipleItems', request)\n",
    "# print(response.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import listdir\n",
    "# from PIL import Image\n",
    "\n",
    "# for imageFolder in listdir('./nn_images2'):\n",
    "#     try:\n",
    "#         img = Image.open('./nn_images2/'+imageFolder)\n",
    "#         img.verify()     # to veify if its an img\n",
    "#         img.close()     #to close img and free memory space\n",
    "#     except (IOError, SyntaxError) as e:\n",
    "#         print('Bad file:', imageFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listdir('/nnimages2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root='C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pandas as pd\n",
    "# import matplotlib.pyplot  as plt\n",
    "# from PIL import Image\n",
    "# from pathlib import Path\n",
    "# # import imagesize\n",
    "# import numpy as np\n",
    "\n",
    "# # Get the Image Resolutions\n",
    "# imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "# img_meta = {}\n",
    "# for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# # Convert it to Dataframe and compute aspect ratio\n",
    "# img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "# img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "# img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "# print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "# img_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "    price_list.append(listed_price)\n",
    "    \n",
    "df['price_in_US'] = price_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#atttempt to pull shipping cost from json dict\n",
    "shipping_cost_list = []\n",
    "for row in full_dataset:\n",
    "    shipping_cost = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "    shipping_cost_list.append(shipping_cost)\n",
    "    \n",
    "df['shipping_price'] = shipping_cost_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#pull shipping cost from json dict with regex \n",
    "df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#create new feature 'converted price'\n",
    "df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "df = df.drop_duplicates(subset=['title', 'galleryURL'], keep='first')\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "\n",
    "df.to_csv('data/full_dataset.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwLbGNr0TbIj"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* Were there variables you dropped or created?\n",
    "* How did you address missing values or outliers?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "\n",
    "\n",
    "# here you run your code to clean the data\n",
    "\n",
    "```\n",
    "import code.data_cleaning as dc\n",
    "\n",
    "full_dataset = dc.full_clean()\n",
    "```\n",
    "\n",
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "# here you run your code to model the data\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do in the future to improve this project?\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_knife_dir = 'knife_images'\n",
    "# data_profit_dir = 'data/profit'\n",
    "# new_dir = 'split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_folder = os.path.join(new_dir, 'train')\n",
    "# train_profit = os.path.join(train_folder, 'profit')\n",
    "# os.mkdir(train_folder)\n",
    "# os.mkdir(train_profit)\n",
    "\n",
    "# test_folder = os.path.join(new_dir, 'test')\n",
    "# test_profit = os.path.join(test_folder, 'profit')\n",
    "# os.mkdir(test_folder)\n",
    "# os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# val_folder = os.path.join(new_dir, 'validation')\n",
    "# val_profit = os.path.join(val_folder, 'profit')\n",
    "# os.mkdir(val_folder)\n",
    "# os.mkdir(val_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train knife regression images\n",
    "# #80% of data\n",
    "# imgs = knife_images[:5620]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(train_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # test knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[5620:6322]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(test_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # validation knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[6322:]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(val, img)\n",
    "#     shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "plt.imshow(img_rgb)  # graph it\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_checker(index,):\n",
    "    img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "    plt.imshow(img_rgb)  # graph it\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_benchmade_index[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(2286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(1879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6094)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final processing steps for images\n",
    "\n",
    "image_list = []\n",
    "for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "    img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "    img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "    image_list.append(img_rgb)\n",
    "   \n",
    "    # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=  df_CNN_regression['mean_profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small batch\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256, 3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=30,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub['profit'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.2164 * 41.374303202846974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_batch32.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a train set of 60% and a val and test size of 20% each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model showed a lot of indication that it was overfit\n",
    "#need to retry how I split the data \n",
    "#Instead of manul indexing, will use \n",
    "# from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X[:4918]\n",
    "# y_train = y[:4918]\n",
    "\n",
    "# X_train = X[4918:5971]\n",
    "# y_train = y[4918:5971]\n",
    "\n",
    "# X_test = X[5971:]\n",
    "# y_test = y[5971:]\n",
    "\n",
    "\n",
    "# display(len(X_val)/len(X))\n",
    "# display(len(X_train)/len(X))\n",
    "# len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(224 ,224,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=32,\n",
    "#                     batch_size=300,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model is showing a lot of signs of overfitting \n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from ebaysdk.finding import Connection\n",
    "import requests\n",
    "\n",
    "from ebaysdk.shopping import Connection as Shopping\n",
    "\n",
    "import pandas as pd \n",
    "import  json\n",
    "import numpy as np\n",
    "import re\n",
    "import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "df = pd.read_csv('data/tera_df_prepared.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')\n",
    "\n",
    "df.info()\n",
    "\n",
    "api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "request = {\n",
    "           'itemID': list(itemIds),\n",
    "           'IncludeSelector': 'ItemSpecifics'\n",
    "          }\n",
    "response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "response_dict = response.dict()\n",
    "\n",
    "results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "\n",
    "\n",
    "response_dict = response.dict()\n",
    "\n",
    "    #index dict to appropriate index\n",
    "# results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "                'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "                'Country', 'Location', 'ConvertedCurrentPrice',\n",
    "                'PictureURL']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "                'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "\n",
    "                'PictureURL']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_list(my_list):\n",
    " \n",
    "    api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "    request = {\n",
    "               'itemID': my_list,\n",
    "               'IncludeSelector': 'ItemSpecifics'\n",
    "              }\n",
    "    response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "    return full_dataset\n",
    "\n",
    "full_dataset = []\n",
    "for i in range(0, len(itemIds), 20):\n",
    "    process_list(itemIds[i:i+20])\n",
    "\n",
    "itemIds = list(df.itemId)\n",
    "\n",
    "full_dataset = []\n",
    "for i in range(0, len(itemIds), 20):\n",
    "    process_list(itemIds[i:i+20])\n",
    "\n",
    "len(full_dataset)\n",
    "\n",
    "full_dataset[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(full_dataset)\n",
    "\n",
    "df3 = df2.drop_duplicates(subset='ItemID').copy()\n",
    "\n",
    "df.info()\n",
    "\n",
    "df3.ItemSpecifics.sample(5).apply(print)\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "def extract_opening_mech(line):\n",
    "    pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['opening_mechanism'] = df3.ItemSpecifics.apply(extract_opening_mech)\n",
    "\n",
    "df3\n",
    "\n",
    "pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "def extract_blade_type(line):\n",
    "    pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)\n",
    "\n",
    "df3.ItemSpecifics.sample(10).apply(print)\n",
    "\n",
    "def extract_color(line):\n",
    "    pattern = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['color'] = df3.ItemSpecifics.apply(extract_color)\n",
    "\n",
    "def extract_blade_type(line):\n",
    "    pattern = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)\n",
    "\n",
    "df3['blade_type'].value_counts()[:20]\n",
    "\n",
    "def extract_manufacture_region(line):\n",
    "    pattern = re.compile(\"Country/Region of Manufacture\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "\n",
    "\n",
    "df3['region_of_Manufacture'] = df3.ItemSpecifics.apply(extract_manufacture_region)\n",
    "\n",
    "df3['region_of_Manufacture'].value_counts()\n",
    "\n",
    "def extract_handle_material(line):\n",
    "    pattern = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "\n",
    "\n",
    "df3['handle_material'] = df3.ItemSpecifics.apply(extract_handle_material)\n",
    "\n",
    "df3['handle_material'].value_counts()[:50]\n",
    "\n",
    "def extract_lock_type(line):\n",
    "    pattern = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "\n",
    "df3['lock_type'] = df3.ItemSpecifics.apply(extract_lock_type)\n",
    "\n",
    "df3['lock_type'].value_counts()\n",
    "\n",
    "def extract_blade_edge(line):\n",
    "    pattern = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "df3['blade_edge'] = df3.ItemSpecifics.apply(extract_blade_edge)\n",
    "\n",
    "df3['blade_edge'].value_counts()\n",
    "\n",
    "def extract_dexterity(line):\n",
    "    pattern = re.compile(\"Dexterity\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "df3['dexterity'] = df3.ItemSpecifics.apply(extract_dexterity)\n",
    "\n",
    "df3['dexterity'].value_counts()\n",
    "\n",
    "df3.to_csv('data/item_specifics_df.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "# request = {\n",
    "#            'itemID': list(itemIds),\n",
    "#            'IncludeSelector': 'ItemSpecifics'\n",
    "#           }\n",
    "# response = api.execute('GetMultipleItems', request)\n",
    "# print(response.dict())\n",
    "\n",
    "ls\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "for imageFolder in listdir('./nn_images2'):\n",
    "    try:\n",
    "        img = Image.open('./nn_images2/'+imageFolder)\n",
    "        img.verify()     # to veify if its an img\n",
    "        img.close()     #to close img and free memory space\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        print('Bad file:', imageFolder)\n",
    "\n",
    "listdir('/nnimages2')\n",
    "\n",
    "root='C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images2/'\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import imagesize\n",
    "import numpy as np\n",
    "\n",
    "# Get the Image Resolutions\n",
    "imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "img_meta = {}\n",
    "for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# Convert it to Dataframe and compute aspect ratio\n",
    "img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "img_meta_df.head()\n",
    "\n",
    "# Visualize Image Resolutions\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, picker=True)\n",
    "ax.set_title(\"Image Resolution\")\n",
    "ax.set_xlabel(\"Width\", size=14)\n",
    "ax.set_ylabel(\"Height\", size=14)\n",
    "\n",
    "# Visualize Image Resolutions\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "ax.set_title(\"Image Resolution\")\n",
    "ax.set_xlabel(\"Width\", size=14)\n",
    "ax.set_ylabel(\"Height\", size=14)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "        'viewItemURL', 'autoPay', 'postalCode', \n",
    "        'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "        'returnsAccepted', 'condition', 'topRatedListing',\n",
    "        'galleryPlusPictureURL', 'subtitle', 'discountPriceInfo',\n",
    "        'secondaryCategory']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# def prepare_df(df):\n",
    "#     price_list = []\n",
    "#     for row in full_dataset:\n",
    "#         listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#         price_list.append(listed_price)\n",
    "\n",
    "#     df['price_in_US'] = price_list\n",
    "#     #pull shipping cost from json dict with regex \n",
    "#     df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "#     df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#     #create new feature 'converted price'\n",
    "#     df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#     df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     display(df.head())\n",
    "#     display(df.info())\n",
    "#     return df\n",
    "\n",
    "def prepare_df(df):\n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    for row in full_dataset:\n",
    "        listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "        price_list.append(listed_price)\n",
    "\n",
    "        try:\n",
    "            listed_ship_price = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "\n",
    "        try:\n",
    "            condition = None\n",
    "            condition = np.float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    display(df.head())\n",
    "    display(df.info())\n",
    "    return df\n",
    "\n",
    "overhead_cost = 3\n",
    "def prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    "\n",
    "    #remove special characters\n",
    "    df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "    df[str(list(bucket_dict.keys())[bucket_dict_position])] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from base64 import b64encode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this function was used at first to check accuracy of calls using ebay API\n",
    "#the less complex \"prepare_brands\" function seems sufficient enough\n",
    "#it is redudant to use regex with working ebay API call\n",
    "\n",
    "def modified_prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    "\n",
    "    #remove special characters\n",
    "    df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "    pattern = re.compile(list(bucket_dict.keys())[bucket_dict_position])\n",
    "    df[list(bucket_dict.keys())[bucket_dict_position]] = df_bench.title.apply(lambda x: re.sub(pattern, x, str(list(bucket_dict.values())[bucket_dict_position])))\n",
    "\n",
    "    df[list(bucket_dict.keys())[bucket_dict_position]] = df[list(bucket_dict.keys())[bucket_dict_position]].apply(lambda x: np.float(x))\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "\n",
    "    return df\n",
    "\n",
    "request = {\n",
    "           'categoryId': 48818,\n",
    "            'itemFilter': [\n",
    "                            {'name': 'Condition', 'value': 'Used'},\n",
    "                            {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                          ],\n",
    "            'aspectFilter': [\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Benchmade'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Buck'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Case'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'CRKT'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Leatherman'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Spyderco'},\n",
    "                             ],\n",
    "            'paginationInput': {\n",
    "                                'entriesPerPage': 100,\n",
    "                                'pageNumber': 1\n",
    "\n",
    "                                }}\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger error at page 23\n",
    "\n",
    "\n",
    "for page in range(1, 22):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "#                                 {'name': 'Condition', 'value': 'Used'},\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Benchmade'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_bench = pd.DataFrame(full_dataset)\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "df_bench = prepare_df(df_bench)\n",
    "\n",
    "df_bench = prepare_brands(df_bench, 0)\n",
    "\n",
    "df_bench['benchmade'].value_counts()\n",
    "\n",
    "df_bench.info()\n",
    "\n",
    "df_bench[df_bench['galleryPlusPictureURL'].notna()]['galleryPlusPictureURL'].sample(30).apply(print)\n",
    "\n",
    "df_bench['profit'].describe()\n",
    "\n",
    "df_bench['ROI'].describe()\n",
    "\n",
    "df_bench.to_csv('data/df_bench.csv', index=False)\n",
    "\n",
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $7\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger error at page 42\n",
    "\n",
    "\n",
    "for page in range(1, 41):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                 {'aspectName': 'Brand', 'aspectValueName': 'Buck'}],\n",
    "\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_buck = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_buck = prepare_df(df_buck)\n",
    "\n",
    "df_buck = prepare_brands(df_buck, 1)\n",
    "\n",
    "df_buck.info()\n",
    "\n",
    "df_buck['buck'].value_counts()\n",
    "\n",
    "df_buck['profit'].describe()\n",
    "\n",
    "df_buck['ROI'].describe()\n",
    "\n",
    "df_buck.to_csv('data/df_buck.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1, 100):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "#                               \n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Case XX'},\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Case'}\n",
    "                                 ],\n",
    "\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_case = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_case = prepare_df(df_case)\n",
    "\n",
    "df_case = prepare_brands(df_case, 2)\n",
    "\n",
    "df_case.info()\n",
    "\n",
    "df_case.to_csv('data/df_case.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 18\n",
    "\n",
    "\n",
    "for page in range(1, 17):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                    {'aspectName': 'Brand', 'aspectValueName': 'CRKT'}],\n",
    "                 \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_crkt = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_crkt = prepare_df(df_crkt)\n",
    "\n",
    "bucket_dict \n",
    "\n",
    "df_crkt = prepare_brands(df_crkt, 3)\n",
    "\n",
    "display(df_crkt.head())\n",
    "display(df_crkt.info())\n",
    "\n",
    "df_crkt.to_csv('data/df_crkt.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger threw error at page 70\n",
    "\n",
    "\n",
    "for page in range(1, 69):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'}\n",
    "                                ],\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_kershaw = pd.DataFrame(full_dataset)\n",
    "\n",
    "bucket_dict\n",
    "\n",
    "df_kershaw = prepare_df(df_kershaw)\n",
    "\n",
    "df_kershaw = prepare_brands(df_kershaw, 4)\n",
    "\n",
    "df_kershaw.info()\n",
    "\n",
    "df_kershaw.to_csv('data/df_kershaw.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 18\n",
    "\n",
    "\n",
    "for page in range(1, 17):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Leatherman'}],\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_leatherman = pd.DataFrame(full_dataset)\n",
    "\n",
    "bucket_dict\n",
    "\n",
    "df_leatherman = prepare_df(df_leatherman)\n",
    "\n",
    "df_leatherman = prepare_brands(df_leatherman, 5)\n",
    "\n",
    "df_leatherman.info()\n",
    "\n",
    "df_leatherman.to_csv('data/df_leatherman.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 72\n",
    "\n",
    "for page in range(1, 71):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Spyderco'}\n",
    "                              ],\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "df_spyderco = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_spyderco = prepare_df(df_spyderco)\n",
    "\n",
    "bucket_dict \n",
    "\n",
    "df_spyderco = prepare_brands(df_spyderco, 6)\n",
    "\n",
    "df_spyderco.info()\n",
    "\n",
    "df_spyderco.to_csv('data/df_spyderco.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "for page in range(1, 21):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'SOG'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_sog = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_sog = prepare_df(df_sog)\n",
    "\n",
    "bucket_dict['sog'] = 15.0\n",
    "\n",
    "df_sog = prepare_brands(df_sog, 8)\n",
    "\n",
    "df_sog.info()\n",
    "\n",
    "df_sog.to_csv('data/df_sog.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "for page in range(1, 100):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_victorinox = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_victorinox = prepare_df(df_victorinox)\n",
    "\n",
    "bucket_dict\n",
    "\n",
    "df_victorinox = prepare_brands(df_victorinox, 7)\n",
    "\n",
    "df_victorinox.info()\n",
    "\n",
    "df_victorinox.to_csv('data/df_victorinox.csv', index=False)\n",
    "\n",
    "# df_bench = pd.read_csv(\"data/df_bench.csv\")\n",
    "# df_buck = pd.read_csv(\"data/df_buck.csv\")\n",
    "# df_case = pd.read_csv(\"data/df_case.csv\")\n",
    "# df_crkt = pd.read_csv(\"data/df_crkt.csv\")\n",
    "# df_kershaw = pd.read_csv(\"data/df_kershaw.csv\")\n",
    "# df_leatherman = pd.read_csv(\"data/df_leatherman.csv\")\n",
    "# df_spyderco = pd.read_csv(\"data/df_spyderco.csv\")\n",
    "# df_victorinox = pd.read_csv(\"data/df_victorinox.csv\")\n",
    "\n",
    "df = pd.concat([df_bench,df_buck,df_case,df_crkt,df_kershaw,df_leatherman,df_spyderco,df_sog,df_victorinox])\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "df.to_csv(\"data/full_dataset2.csv\", index=False)\n",
    "\n",
    "After succesfully going through 10,000 items on ebay's website and extracting everything possible, there is still a little bit more extracting to do from the json dictionary before saving the dataframe again. We need to get the price of the knives out of the nested dictionary in the dataframe as well as the shipping cost. After that, I would like to create a new feature called\n",
    "**\"converted price,\" which is simply the price of the knife listed on the ebay's website plus shipping.**\n",
    "\n",
    "```\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "    price_list.append(listed_price)\n",
    "    \n",
    "df['price_in_US'] = price_list\n",
    "```\n",
    "\n",
    "```\n",
    "#atttempt to pull shipping cost from json dict\n",
    "shipping_cost_list = []\n",
    "for row in full_dataset:\n",
    "    shipping_cost = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "    shipping_cost_list.append(shipping_cost)\n",
    "    \n",
    "df['shipping_price'] = shipping_cost_list\n",
    "```\n",
    "\n",
    "```\n",
    "#pull shipping cost from json dict with regex \n",
    "df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#create new feature 'converted price'\n",
    "df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "df = df.drop_duplicates(subset=['title', 'galleryURL'], keep='first')\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "\n",
    "df.to_csv('data/full_dataset.csv', index=False)\n",
    "```\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* Were there variables you dropped or created?\n",
    "* How did you address missing values or outliers?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "\n",
    "\n",
    "# here you run your code to clean the data\n",
    "\n",
    "```\n",
    "import code.data_cleaning as dc\n",
    "\n",
    "full_dataset = dc.full_clean()\n",
    "```\n",
    "\n",
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "# here you run your code to model the data\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do in the future to improve this project?\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "# data_knife_dir = 'knife_images'\n",
    "# data_profit_dir = 'data/profit'\n",
    "# new_dir = 'split'\n",
    "\n",
    "# os.mkdir(new_dir)\n",
    "\n",
    "# train_folder = os.path.join(new_dir, 'train')\n",
    "# train_profit = os.path.join(train_folder, 'profit')\n",
    "# os.mkdir(train_folder)\n",
    "# os.mkdir(train_profit)\n",
    "\n",
    "# test_folder = os.path.join(new_dir, 'test')\n",
    "# test_profit = os.path.join(test_folder, 'profit')\n",
    "# os.mkdir(test_folder)\n",
    "# os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# val_folder = os.path.join(new_dir, 'validation')\n",
    "# val_profit = os.path.join(val_folder, 'profit')\n",
    "# os.mkdir(val_folder)\n",
    "# os.mkdir(val_profit)\n",
    "\n",
    "# val_profit\n",
    "\n",
    "# # train knife regression images\n",
    "# #80% of data\n",
    "# imgs = knife_images[:5620]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(train_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # test knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[5620:6322]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(test_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # validation knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[6322:]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(val, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "plt.imshow(img_rgb)  # graph it\n",
    "plt.show();\n",
    "\n",
    "\n",
    "def image_checker(index,):\n",
    "    img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "    plt.imshow(img_rgb)  # graph it\n",
    "    plt.show();\n",
    "\n",
    "top_benchmade_index[:50]\n",
    "\n",
    "image_checker(6158)\n",
    "\n",
    "image_checker(2286)\n",
    "\n",
    "image_checker(1879)\n",
    "\n",
    "image_checker(4326)\n",
    "\n",
    "image_checker(6094)\n",
    "\n",
    "#final processing steps for images\n",
    "\n",
    "image_list = []\n",
    "for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "    img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "    img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "    image_list.append(img_rgb)\n",
    "   \n",
    "    # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n",
    "\n",
    "\n",
    "df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())\n",
    "\n",
    "df_CNN_regression['mean_profit'].describe()\n",
    "\n",
    "X = np.array(image_list)\n",
    "\n",
    "y=  df_CNN_regression['mean_profit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets\n",
    "\n",
    "X.shape\n",
    "\n",
    "y.shape\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#small batch\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256, 3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=30,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "df_scrub['profit'].mean()\n",
    "\n",
    "2.2164 * 41.374303202846974\n",
    "\n",
    "df_scrub.head()\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();\n",
    "\n",
    "model.save('my_model_batch32.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#a train set of 60% and a val and test size of 20% each \n",
    "\n",
    "#this model showed a lot of indication that it was overfit\n",
    "#need to retry how I split the data \n",
    "#Instead of manul indexing, will use \n",
    "# from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X[:4918]\n",
    "# y_train = y[:4918]\n",
    "\n",
    "# X_train = X[4918:5971]\n",
    "# y_train = y[4918:5971]\n",
    "\n",
    "# X_test = X[5971:]\n",
    "# y_test = y[5971:]\n",
    "\n",
    "\n",
    "# display(len(X_val)/len(X))\n",
    "# display(len(X_train)/len(X))\n",
    "# len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(224 ,224,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=32,\n",
    "#                     batch_size=300,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')\n",
    "\n",
    "results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# model.save('my_model_batch500.h5')\n",
    "\n",
    "history.history.keys()\n",
    "\n",
    "#The model is showing a lot of signs of overfitting \n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket_dict = {'benchmade': 45.0,\n",
    "#                'buck': 20.0,\n",
    "#                'case': 20.0,\n",
    "#                'crkt': 15.0,\n",
    "#                'kershaw': 15.0,\n",
    "#                'leatherman': 30.0,\n",
    "#                'sog': 15.0,\n",
    "#                'spyderco': 30.0,\n",
    "#                'victorinox': 20.0\n",
    "#               }\n",
    "\n",
    "# overhead_cost = 3\n",
    "# def prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "#     df.title = df.title.apply(str.lower)\n",
    " \n",
    "#     #remove special characters\n",
    "# #     df.title.apply(pp.remove_special_chars)\n",
    "#     df['brand'] = str(list(bucket_dict.keys())[bucket_dict_position])\n",
    "#     df['cost'] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "#     df['profit'] = (df['converted_price'] -  df['cost'] - overhead_cost)\n",
    "#     df['ROI'] = (df['profit']/( df['cost'] + overhead_cost))*100.0\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['itemId', 'title', 'galleryURL', \n",
    "#         'viewItemURL', 'autoPay', 'postalCode', \n",
    "#         'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "#         'returnsAccepted', 'condition', 'topRatedListing',\n",
    "#         'galleryPlusPictureURL']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         # Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def knife_request(Brand, dict_pos, Lots=False):\n",
    "#     api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "#     full_dataset = []\n",
    "#     price_list = []\n",
    "#     ship_price_list = []\n",
    "#     condition_list = []\n",
    "#     condition = None\n",
    "#     if Lots == True:\n",
    "#         request = {\n",
    "#                     'categoryId': 48818,\n",
    "#                     'itemFilter': [\n",
    "#                                     {'name': 'LotsOnly', 'value': 'True'},\n",
    "#                                     {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                                   ],\n",
    "#                     'aspectFilter': [\n",
    "#                                       {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "\n",
    "#                     'paginationInput': {\n",
    "#                                         'entriesPerPage': 100,\n",
    "#                                         'pageNumber': 1\n",
    "\n",
    "#                                         },\n",
    "\n",
    "#                     }\n",
    "\n",
    "\n",
    "#         response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "\n",
    "#         response_pages = response.dict()\n",
    "\n",
    "    \n",
    "#         total_pages = int(response_pages['paginationOutput']['totalPages'])\n",
    "        \n",
    "#         if total_pages > 100:\n",
    "#             pages_to_request = 100\n",
    "        \n",
    "#         else:\n",
    "#             pages_to_request = total_pages - 1\n",
    "\n",
    "\n",
    "#         for page in range(1, pages_to_request):\n",
    "#         # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "#         # Make the query and get the response\n",
    "\n",
    "#             api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "#             request = {\n",
    "#                         'categoryId': 48818,\n",
    "#                         'itemFilter': [\n",
    "#                                         {'name': 'LotsOnly', 'value': 'True'},\n",
    "#                                         {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                                       ],\n",
    "#                         'aspectFilter': [\n",
    "#                                           {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "\n",
    "#                         'paginationInput': {\n",
    "#                                             'entriesPerPage': 100,\n",
    "#                                             'pageNumber': page\n",
    "\n",
    "#                                             },\n",
    "\n",
    "#                         }\n",
    "\n",
    "\n",
    "#             response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#             #save the response as a json dict\n",
    "#             response_dict = response.dict()\n",
    "\n",
    "\n",
    "#             #index dict to appropriate index\n",
    "#             results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#             # Call the prepare_data function to get a list of processed data\n",
    "#             prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#             # Extend full_dataset with this list (don't append, or you'll get\n",
    "#             # a list of lists instead of a flat list)\n",
    "#             full_dataset.extend(prepared_knives)\n",
    "            \n",
    "#         display(len(full_dataset))\n",
    "    \n",
    "#         df = pd.DataFrame(full_dataset)\n",
    "        \n",
    "#         for row in full_dataset:\n",
    "#             try:\n",
    "#                 listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#                 price_list.append(listed_price)\n",
    "#             except:\n",
    "#                 listed_price = \"Na\"\n",
    "#                 price_list.append(listed_price)\n",
    "#             try:\n",
    "#                 listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "#                 ship_price_list.append(listed_ship_price)\n",
    "#             except: \n",
    "#                 listed_ship_price = 0\n",
    "#                 ship_price_list.append(listed_ship_price)\n",
    "#             try:\n",
    "#                 condition = float(row['condition']['conditionId'])\n",
    "#                 condition_list.append(condition)\n",
    "#             except: \n",
    "#                 conditon = 0\n",
    "#                 condition_list.append(condition)\n",
    "\n",
    "#         df['shipping_cost'] = ship_price_list\n",
    "#         df['price_in_US'] = price_list\n",
    "#         df['condition'] = condition_list\n",
    "    \n",
    "#     #create new feature 'converted price'\n",
    "#         df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#         df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#         df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#         df = prepare_brands(df, dict_pos)\n",
    "        \n",
    "        \n",
    "#     else: \n",
    "#         request = {\n",
    "#                     'categoryId': 48818,\n",
    "#                     'itemFilter': [\n",
    "#                                     {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                                   ],\n",
    "#                     'aspectFilter': [\n",
    "#                                       {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "\n",
    "#                     'paginationInput': {\n",
    "#                                         'entriesPerPage': 100,\n",
    "#                                         'pageNumber': 1\n",
    "\n",
    "#                                         },\n",
    "\n",
    "#                     }\n",
    "\n",
    "\n",
    "#         response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "\n",
    "#         response_pages = response.dict()\n",
    "\n",
    "    \n",
    "#         total_pages = int(response_pages['paginationOutput']['totalPages'])\n",
    "#         if total_pages > 100:\n",
    "#             pages_to_request = 100\n",
    "        \n",
    "#         else:\n",
    "#             pages_to_request = total_pages - 1\n",
    "        \n",
    "        \n",
    "#         for page in range(1, pages_to_request):\n",
    "#         # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "#         # Make the query and get the response\n",
    "\n",
    "#             api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "#             request = {\n",
    "#                         'categoryId': 48818,\n",
    "#                         'itemFilter': [\n",
    "#                                         {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                                       ],\n",
    "#                         'aspectFilter': [\n",
    "#                                           {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "\n",
    "#                         'paginationInput': {\n",
    "#                                             'entriesPerPage': 100,\n",
    "#                                             'pageNumber': page\n",
    "\n",
    "#                                             },\n",
    "\n",
    "#                         }\n",
    "\n",
    "\n",
    "#             response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#             #save the response as a json dict\n",
    "#             response_dict = response.dict()\n",
    "\n",
    "\n",
    "#             #index dict to appropriate index\n",
    "#             results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#             # Call the prepare_data function to get a list of processed data\n",
    "#             prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#             # Extend full_dataset with this list (don't append, or you'll get\n",
    "#             # a list of lists instead of a flat list)\n",
    "#         full_dataset.extend(prepared_knives)\n",
    "#         display(len(full_dataset))\n",
    "    \n",
    "#         df = pd.DataFrame(full_dataset)\n",
    "        \n",
    "#         for row in full_dataset:\n",
    "#             try:\n",
    "#                 listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#                 price_list.append(listed_price)\n",
    "#             except:\n",
    "#                 listed_price = \"Na\"\n",
    "#                 price_list.append(listed_price)\n",
    "#             try:\n",
    "#                 listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "#                 ship_price_list.append(listed_ship_price)\n",
    "#             except: \n",
    "#                 listed_ship_price = 0\n",
    "#                 ship_price_list.append(listed_ship_price)\n",
    "#             try:\n",
    "#                 condition = float(row['condition']['conditionId'])\n",
    "#                 condition_list.append(condition)\n",
    "#             except: \n",
    "#                 conditon = 0\n",
    "#                 condition_list.append(condition)\n",
    "\n",
    "#         df['shipping_cost'] = ship_price_list\n",
    "#         df['price_in_US'] = price_list\n",
    "#         df['condition'] = condition_list\n",
    "    \n",
    "#     #create new feature 'converted price'\n",
    "#         df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#         df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#         df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#         df = prepare_brands(df, dict_pos)\n",
    "        \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
