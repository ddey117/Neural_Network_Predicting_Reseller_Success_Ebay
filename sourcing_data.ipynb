{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Resale Value of Knives from a Texas Government Surplus Store\n",
    "## Using Machine Learning to Support an Ebay Store's Financial Success\n",
    "\n",
    "### Data Obtainment Notebook\n",
    "\n",
    "\n",
    "This notebook displays the code used to collect and process data from eBay using two of eBay's public APIs and scraping from their proprietary webabb \"Terapeak\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.finding import Connection\n",
    "import requests\n",
    "from ebaysdk.shopping import Connection as Shopping\n",
    "import pandas as pd \n",
    "import  json\n",
    "import numpy as np\n",
    "import re\n",
    "# import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import ast\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is a helper function created for the \"knife_request\" below. \n",
    "#It unpacks some of the nested data from eBay API calls \n",
    "#It also creates the new feature \"converted_price\"\n",
    "#\"converted_price\" is the price of the item for sale plus shipping cost.\n",
    "def prepare_df(df):\n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    condition = None\n",
    "    for row in full_dataset:\n",
    "        listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "        price_list.append(listed_price)\n",
    "     \n",
    "        try:\n",
    "            listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "\n",
    "        try:\n",
    "            condition = float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#dictionary for preparing brands\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "#a helper function used with knife_request \n",
    "#it is used to create new columns of interest\n",
    "#the brand of knife from the API call\n",
    "#the cost of the knife from the Surplus Store\n",
    "#profit for reselling a used surplus knife on eBay\n",
    "#Return on Investment for reselling the knife\n",
    "#All columns in US dollars\n",
    "def prepare_brands(df, bucket_dict_position, overhead_cost=3):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    " \n",
    "    #remove special characters\n",
    "#     df.title.apply(pp.remove_special_chars)\n",
    "    df['brand'] = str(list(bucket_dict.keys())[bucket_dict_position])\n",
    "    df['cost'] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    df['profit'] = ((df['converted_price']*.87) -  df['cost'] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/( df['cost'] + overhead_cost))*100.0\n",
    "    \n",
    "    return df\n",
    "# Help organize paginated data from API calls\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "                'viewItemURL', 'autoPay', 'postalCode', \n",
    "                'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "                'returnsAccepted', 'condition', 'topRatedListing',\n",
    "                'galleryPlusPictureURL','pictureURLLarge', \n",
    "                'pictureURLSuperSize']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "#main function for making findingAPI calls to eBay\n",
    "def knife_request(Brand, dict_pos):\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': 1\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "    #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "\n",
    "    response_pages = response.dict()\n",
    "\n",
    "    full_dataset = []\n",
    "    \n",
    "    total_pages = int(response_pages['paginationOutput']['totalPages'])\n",
    "\n",
    "    if total_pages > 100:\n",
    "        pages_to_request = 100\n",
    "        \n",
    "    else:\n",
    "        pages_to_request = total_pages - 1\n",
    "        \n",
    "        \n",
    "\n",
    "    for page in range(1, pages_to_request):\n",
    "        # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "        # Make the query and get the response\n",
    "\n",
    "        api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "        request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "\n",
    "        response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "        #save the response as a json dict\n",
    "        response_dict = response.dict()\n",
    "\n",
    "\n",
    "        #index dict to appropriate index\n",
    "        results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "        # Call the prepare_data function to get a list of processed data\n",
    "        prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "        # Extend full_dataset with this list (don't append, or you'll get\n",
    "        # a list of lists instead of a flat list)\n",
    "        full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "    display(len(full_dataset))\n",
    "    \n",
    "    df = pd.DataFrame(full_dataset)\n",
    "    \n",
    "    df = prepare_df(df)\n",
    "    \n",
    "    df = prepare_brands(df, dict_pos)\n",
    "    \n",
    "    return df\n",
    "#Used to prepare data from eBays shopping API\n",
    "#Shopping API used to collect more detailed info\n",
    "#about individual knives\n",
    "def prepare_dataIds(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID','GalleryURL','PictureURL',\n",
    "                'Location','ConvertedCurrentPrice',\n",
    "                'Title','ItemSpecifics', \n",
    "                'Country','ConditionID']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "#Shopping api accepts a max of 20 itemIDs\n",
    "#this function was created to automate\n",
    "#making API calls in 20 unique itemId chuncks\n",
    "def process_list(my_list):\n",
    " \n",
    "    api = Shopping(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "    request = {\n",
    "               'itemID': my_list,\n",
    "               'IncludeSelector': 'ItemSpecifics'\n",
    "              }\n",
    "    response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_dataIds(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "    return full_dataset\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "#special function for reformatting terapeak scraped data\n",
    "#x = position of bucket_dictionary\n",
    "def prepare_tera_df(df, x, overhead_cost=3):\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\"$\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\",\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].apply(float)\n",
    "    \n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\"$\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\",\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].apply(float)\n",
    "    \n",
    "    df['converted_price'] = (df['price_in_US'] + df['shipping_cost'])\n",
    "    \n",
    "    df['profit'] = ((df['converted_price']*.87) - list(bucket_dict.values())[x] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/(list(bucket_dict.values())[x]))*100.0\n",
    "    \n",
    "    df['brand'] = list(bucket_dict.keys())[x]\n",
    "    df['cost'] = list(bucket_dict.values())[x]\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "# helper function with \"transform_item_specifics\"\n",
    "def fix(col):\n",
    "    dd = dict()\n",
    "    for d in col:\n",
    "        values = list(d.values())\n",
    "        if len(values) == 2:\n",
    "            dd[values[0]] = values[1]\n",
    "    return dd\n",
    "\n",
    "#function for extracted item Specifics from Shopping API data\n",
    "def transform_item_specifics(df, perc=90.0):\n",
    "\n",
    "    df.dropna(subset=['ItemSpecifics'], inplace=True)\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(lambda x: ast.literal_eval(x))\n",
    "    df['item_list'] = df['ItemSpecifics'].apply(lambda x: x['NameValueList'])\n",
    "\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(lambda x: [x['NameValueList']] if isinstance(x['NameValueList'], dict) else x['NameValueList'])\n",
    "\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(fix)\n",
    "\n",
    "    df = pd.json_normalize(df['ItemSpecifics'])\n",
    "\n",
    "    min_count =  int(((100-perc)/100)*df.shape[0] + 1)\n",
    "    mod_df = df.dropna(axis=1, \n",
    "                       thresh=min_count)\n",
    "\n",
    "    return mod_df\n",
    "\n",
    "# This function removes noisy data\n",
    "#lots/sets/groups of knives can\n",
    "#confuse the model from predicting\n",
    "#the appropriate value of individual knives\n",
    "def data_cleaner(df):\n",
    "    lot = re.compile('(?<!-\\S)lot(?![^\\s.,:?!])')\n",
    "    group = re.compile('(group)')\n",
    "    is_set = re.compile('(?<!-\\S)set(?![^\\s.,?!])')\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    trim_list = [lot,group,is_set]\n",
    "    for item in trim_list:\n",
    "        df.loc[df['title'].apply(lambda x: re.search(item, x)).notnull(), 'trim'] = 1 \n",
    "    to_drop = df.loc[df['trim'] == 1].index\n",
    "    df.drop(to_drop, inplace=True)\n",
    "    df.drop('trim', axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmade': 45.0,\n",
       " 'buck': 20.0,\n",
       " 'case': 20.0,\n",
       " 'crkt': 15.0,\n",
       " 'kershaw': 15.0,\n",
       " 'sog': 15.0,\n",
       " 'spyderco': 30.0,\n",
       " 'victorinox': 20.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of API calls for listed data. To be merged with item specific data using ebay itemIds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $3\n",
    "- Ebay's comission, with 13\\% being a reasonable approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running functions to call the Finding API and return datasets for cat () knives for sale listed on ebay in the last 90 days. (explain how ebay rules work)\n",
    "\n",
    "```\n",
    "bench_df = knife_request('Benchmade', 0)\n",
    "buck_df = knife_request('Buck', 1)\n",
    "case_df = knife_request('Case', 2)\n",
    "df_caseXX = knife_request('Case XX', 2)\n",
    "df_crkt = knife_request(\"CRKT\", 3)\n",
    "df_sog = knife_request('SOG', 5)\n",
    "df_spyderco = knife_request('Spyderco', 6)\n",
    "\n",
    "\n",
    "bench_df.to_csv('listed_data/df_bench1.csv', index=False)\n",
    "buck_df.to_csv('listed_data/df_buck.csv', index=False)\n",
    "case_df.to_csv('listed_data/df_case.csv', index=False)\n",
    "df_caseXX.to_csv('listed_data/df_CaseXX.csv', index=False)\n",
    "df_crkt.to_csv('listed_data/df_crkt.csv', index=False)\n",
    "df_sog.to_csv('listed_data/df_sog.csv', index=False)\n",
    "df_spyderco.to_csv('listed_data/df_spyderco.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kershaw and victorinox data was requested using the FindingAPI below after tweaking some pagination through trial and error to maximize data.\n",
    "\n",
    "```\n",
    "full_dataset = []\n",
    "for page in range(1, 57):\n",
    "#         # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "#         # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "        #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "\n",
    "    df = pd.DataFrame(full_dataset)\n",
    "    \n",
    "df_kershaw = prepare_df(df)\n",
    "df_kershaw = prepare_brands(df_kershaw, 4)\n",
    "df_kershaw.to_csv('listed_data/df_kershaw.csv', index=False)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "full_dataset = []\n",
    "for page in range(1, 86):\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "df_victorinox = pd.DataFrame(full_dataset)\n",
    "df_victorinox = prepare_df(df_victorinox)\n",
    "df_victorinox = prepare_brands(df_victorinox, 7)\n",
    "df_victorinox.to_csv('listed_data/df_victorinox.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start of API call section using IDs from preview listed datasets to get Item Specific data from ebay. This will return more descriptive information about the knives, pulling from a container on the website that sellers must complete to post a listing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = pd.read_csv(\"listed_data/df_bench.csv\")\n",
    "df_buck = pd.read_csv(\"listed_data/df_buck.csv\")\n",
    "df_case = pd.read_csv(\"listed_data/df_case.csv\")\n",
    "df_caseXX = pd.read_csv(\"listed_data/df_CaseXX.csv\")\n",
    "df_crkt = pd.read_csv(\"listed_data/df_crkt.csv\")\n",
    "df_kersh = pd.read_csv(\"listed_data/df_kershaw.csv\")\n",
    "df_sog = pd.read_csv(\"listed_data/df_sog.csv\")\n",
    "df_spyd = pd.read_csv(\"listed_data/df_spyderco.csv\")\n",
    "df_vict = pd.read_csv(\"listed_data/df_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_bench,df_buck,\n",
    "           df_case,df_caseXX,\n",
    "           df_crkt,df_kersh,\n",
    "           df_sog,df_spyd,\n",
    "           df_vict]\n",
    "\n",
    "for dataframe in df_list:\n",
    "    dataframe.drop('galleryPlusPictureURL', axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchIds = df_bench.itemId.values.tolist()\n",
    "buckIds = df_buck.itemId.values.tolist()\n",
    "caseIds = df_case.itemId.values.tolist()\n",
    "caseXXIds = df_caseXX.itemId.values.tolist()\n",
    "crktIds = df_crkt.itemId.values.tolist()\n",
    "kershawIds = df_kersh.itemId.values.tolist()\n",
    "sogIds = df_sog.itemId.values.tolist()\n",
    "spydIds = df_spyd.itemId.values.tolist()\n",
    "victIds = df_vict.itemId.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return benchmade item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(benchIds), 20):\n",
    "    process_list(benchIds[i:i+20])\n",
    "\n",
    "bench = pd.DataFrame(full_dataset)\n",
    "bench.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "bench.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return buck item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(buckIds), 20):\n",
    "    process_list(buckIds[i:i+20])\n",
    "\n",
    "buck = pd.DataFrame(full_dataset)\n",
    "buck.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "buck.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return case brand item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(caseIds), 20):\n",
    "    process_list(caseIds[i:i+20])\n",
    "\n",
    "df_case = pd.DataFrame(full_dataset)\n",
    "df_case.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_case.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return caseXX brand item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(caseXXIds), 20):\n",
    "    process_list(caseXXIds[i:i+20])\n",
    "\n",
    "df_caseXX = pd.DataFrame(full_dataset)\n",
    "df_caseXX.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_caseXX.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return crkt item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(crktIds), 20):\n",
    "    process_list(crktIds[i:i+20])\n",
    "\n",
    "crkt = pd.DataFrame(full_dataset)\n",
    "crkt.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "crkt.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return kershaw item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(kershawIds), 20):\n",
    "    process_list(kershawIds[i:i+20])\n",
    "\n",
    "kershaw = pd.DataFrame(full_dataset)\n",
    "kershaw.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "kershaw.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return SOG item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(sogIds), 20):\n",
    "    process_list(sogIds[i:i+20])\n",
    "\n",
    "sog = pd.DataFrame(full_dataset)\n",
    "sog.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "sog.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ShoppingAPI call to return spyderco item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(spydIds), 20):\n",
    "    process_list(spydIds[i:i+20])\n",
    "spyd = pd.DataFrame(full_dataset)\n",
    "spyd.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "spyd.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return victorinox item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(victIds), 20):\n",
    "    process_list(victIds[i:i+20])\n",
    "    \n",
    "vict = pd.DataFrame(full_dataset)\n",
    "vict.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "vict.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bench.to_csv(\"listed_data/benchIds.csv\", index=False)\n",
    "buck.to_csv(\"listed_data/buckIds.csv\", index=False)\n",
    "df_case.to_csv(\"listed_data/caseIds.csv\", index=False)\n",
    "df_caseXX.to_csv(\"listed_data/caseXXIds.csv\", index=False)\n",
    "crkt.to_csv(\"listed_data/crktIds.csv\", index=False)\n",
    "kershaw.to_csv(\"listed_data/kershawIds.csv\", index=False)\n",
    "leath.to_csv(\"listed_data/leathIds.csv\", index=False)\n",
    "sog.to_csv(\"listed_data/sogIds.csv\", index=False)\n",
    "spyd.to_csv(\"listed_data/spydIds.csv\", index=False)\n",
    "vict.to_csv(\"listed_data/victIds.csv\", index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of prep to merge original listed data with item specific data requested using a seperate API for more complete details about all listings gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv(\"listed_data/benchIds.csv\")\n",
    "buck = pd.read_csv(\"listed_data/buckIds.csv\")\n",
    "case = pd.read_csv(\"listed_data/caseIds.csv\")\n",
    "caseXX = pd.read_csv(\"listed_data/caseXXIds.csv\")\n",
    "crkt = pd.read_csv(\"listed_data/crktIds.csv\")\n",
    "kershaw = pd.read_csv(\"listed_data/kershawIds.csv\")\n",
    "sog = pd.read_csv(\"listed_data/sogIds.csv\")\n",
    "spyd = pd.read_csv(\"listed_data/spydIds.csv\")\n",
    "vict = pd.read_csv(\"listed_data/victIds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ItemID', 'GalleryURL', 'PictureURL', 'Location', 'ConvertedCurrentPrice', 'Title', 'ItemSpecifics', 'Country', 'ConditionID'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['ItemID', 'GalleryURL', 'PictureURL', 'Location', 'ConvertedCurrentPrice', 'Title', 'ItemSpecifics', 'Country', 'ConditionID'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['ItemID', 'GalleryURL', 'PictureURL', 'Location', 'ConvertedCurrentPrice', 'Title', 'ItemSpecifics', 'Country', 'ConditionID'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['ItemID', 'GalleryURL', 'PictureURL', 'Location', 'ConvertedCurrentPrice', 'Title', 'ItemSpecifics', 'Country', 'ConditionID'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['ItemID', 'GalleryURL', 'PictureURL', 'Location', 'ConvertedCurrentPrice', 'Title', 'ItemSpecifics', 'Country', 'ConditionID'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['ItemID', 'GalleryURL', 'PictureURL', 'Location', 'ConvertedCurrentPrice', 'Title', 'ItemSpecifics', 'Country', 'ConditionID'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['ItemID', 'GalleryURL', 'PictureURL', 'Location', 'ConvertedCurrentPrice', 'Title', 'ItemSpecifics', 'Country', 'ConditionID'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['ItemID', 'GalleryURL', 'PictureURL', 'Location', 'ConvertedCurrentPrice', 'Title', 'ItemSpecifics', 'Country', 'ConditionID'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_list = [bench,buck,\n",
    "           case,caseXX,\n",
    "           crkt,kershaw,\n",
    "           sog,spyd]\n",
    "\n",
    "for dataframe in df_list:\n",
    "    display(dataframe.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [bench,buck,\n",
    "           case,caseXX,\n",
    "           crkt,kershaw,\n",
    "           sog,spyd,vict]\n",
    "\n",
    "for dataframe in df_list:\n",
    "    dataframe.rename({'Title': 'title',\n",
    "                      'ItemID': 'itemId'},\n",
    "                     axis=1,inplace=True)\n",
    "    \n",
    "    dataframe.drop(['ConditionID','ConvertedCurrentPrice'], \n",
    "                   axis=1, inplace=True)\n",
    "    dataframe['title'] = dataframe['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge Item Specific dataframes with original listed data using itemIds and title\n",
    "bench_merged = df_bench.merge(bench)\n",
    "buck_merged = df_buck.merge(buck)\n",
    "case_merged = df_case.merge(case)\n",
    "caseXX_merged = df_caseXX.merge(caseXX)\n",
    "crkt_merged = df_crkt.merge(crkt)\n",
    "kershaw_merged = df_kersh.merge(kershaw)\n",
    "spyd_merged = df_spyd.merge(spyd)\n",
    "sog_merged = df_sog.merge(sog)\n",
    "vict_merged = df_vict.merge(vict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_spec = transform_item_specifics(bench_merged, perc=55.0)\n",
    "buck_spec = transform_item_specifics(buck_merged, perc=70.0)\n",
    "case_spec = transform_item_specifics(case_merged, perc=70.0)\n",
    "caseXX_spec = transform_item_specifics(caseXX_merged, perc=70.0)\n",
    "crkt_spec = transform_item_specifics(crkt_merged, perc=70.0)\n",
    "kershaw_spec = transform_item_specifics(kershaw_merged, perc=70.0)\n",
    "sog_spec = transform_item_specifics(sog_merged, perc=70.0)\n",
    "spyd_spec = transform_item_specifics(spyd_merged, perc=60.0)\n",
    "vict_spec = transform_item_specifics(vict_merged, perc=75.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs_list = [bench_spec, buck_spec,\n",
    "              case_spec, caseXX_spec,\n",
    "              crkt_spec, kershaw_spec,\n",
    "              sog_spec, spyd_spec,\n",
    "              vict_spec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1979 entries, 0 to 1978\n",
      "Data columns (total 11 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   Blade Material                 1008 non-null   object\n",
      " 1   Model                          1696 non-null   object\n",
      " 2   Opening Mechanism              1105 non-null   object\n",
      " 3   Number of Blades               1180 non-null   object\n",
      " 4   Handle Material                1301 non-null   object\n",
      " 5   Blade Type                     1042 non-null   object\n",
      " 6   Brand                          1947 non-null   object\n",
      " 7   Color                          1431 non-null   object\n",
      " 8   Type                           1497 non-null   object\n",
      " 9   Country/Region of Manufacture  1166 non-null   object\n",
      " 10  Blade Edge                     1045 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 170.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2748 entries, 0 to 2747\n",
      "Data columns (total 16 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   Type                           2039 non-null   object\n",
      " 1   Model                          2286 non-null   object\n",
      " 2   Blade Material                 1380 non-null   object\n",
      " 3   Lock Type                      1431 non-null   object\n",
      " 4   Blade Type                     1171 non-null   object\n",
      " 5   Blade Edge                     1430 non-null   object\n",
      " 6   Color                          1751 non-null   object\n",
      " 7   Number of Blades               2010 non-null   object\n",
      " 8   Opening Mechanism              1679 non-null   object\n",
      " 9   Handle Material                1627 non-null   object\n",
      " 10  Brand                          2717 non-null   object\n",
      " 11  Original/Reproduction          1188 non-null   object\n",
      " 12  Blade Range                    1020 non-null   object\n",
      " 13  Dexterity                      1081 non-null   object\n",
      " 14  Country/Region of Manufacture  1618 non-null   object\n",
      " 15  Modified Item                  1019 non-null   object\n",
      "dtypes: object(16)\n",
      "memory usage: 343.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9191 entries, 0 to 9190\n",
      "Data columns (total 16 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   Type                           6676 non-null   object\n",
      " 1   Model                          6800 non-null   object\n",
      " 2   Dexterity                      4239 non-null   object\n",
      " 3   Blade Edge                     5159 non-null   object\n",
      " 4   Blade Material                 5495 non-null   object\n",
      " 5   Brand                          9014 non-null   object\n",
      " 6   MPN                            3083 non-null   object\n",
      " 7   Number of Blades               6020 non-null   object\n",
      " 8   Handle Material                6071 non-null   object\n",
      " 9   Blade Range                    4173 non-null   object\n",
      " 10  Color                          5670 non-null   object\n",
      " 11  Opening Mechanism              5327 non-null   object\n",
      " 12  Blade Type                     4995 non-null   object\n",
      " 13  Lock Type                      4593 non-null   object\n",
      " 14  Original/Reproduction          4481 non-null   object\n",
      " 15  Country/Region of Manufacture  4900 non-null   object\n",
      "dtypes: object(16)\n",
      "memory usage: 1.1+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7293 entries, 0 to 7292\n",
      "Data columns (total 15 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   Brand                          7229 non-null   object\n",
      " 1   Blade Material                 4923 non-null   object\n",
      " 2   Type                           6200 non-null   object\n",
      " 3   Lock Type                      2345 non-null   object\n",
      " 4   Dexterity                      2403 non-null   object\n",
      " 5   Color                          5867 non-null   object\n",
      " 6   Opening Mechanism              4535 non-null   object\n",
      " 7   Model                          6532 non-null   object\n",
      " 8   Blade Edge                     2918 non-null   object\n",
      " 9   Original/Reproduction          2584 non-null   object\n",
      " 10  Number of Blades               5834 non-null   object\n",
      " 11  Country/Region of Manufacture  5074 non-null   object\n",
      " 12  Blade Type                     2642 non-null   object\n",
      " 13  Year                           2646 non-null   object\n",
      " 14  Handle Material                5754 non-null   object\n",
      "dtypes: object(15)\n",
      "memory usage: 854.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1465 entries, 0 to 1464\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Brand                  1460 non-null   object\n",
      " 1   Type                   1181 non-null   object\n",
      " 2   Color                  1090 non-null   object\n",
      " 3   Model                  1341 non-null   object\n",
      " 4   Lock Type              918 non-null    object\n",
      " 5   Number of Blades       1100 non-null   object\n",
      " 6   Blade Type             870 non-null    object\n",
      " 7   Opening Mechanism      983 non-null    object\n",
      " 8   Handle Material        925 non-null    object\n",
      " 9   Blade Material         799 non-null    object\n",
      " 10  Blade Range            607 non-null    object\n",
      " 11  Dexterity              483 non-null    object\n",
      " 12  Blade Edge             938 non-null    object\n",
      " 13  Original/Reproduction  562 non-null    object\n",
      "dtypes: object(14)\n",
      "memory usage: 160.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5321 entries, 0 to 5320\n",
      "Data columns (total 17 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   Blade Material                 2873 non-null   object\n",
      " 1   Dexterity                      1950 non-null   object\n",
      " 2   Color                          3501 non-null   object\n",
      " 3   Opening Mechanism              3282 non-null   object\n",
      " 4   Blade Edge                     3023 non-null   object\n",
      " 5   Modified Item                  1771 non-null   object\n",
      " 6   Brand                          5242 non-null   object\n",
      " 7   Blade Type                     2563 non-null   object\n",
      " 8   Lock Type                      3009 non-null   object\n",
      " 9   Type                           4036 non-null   object\n",
      " 10  Model                          3906 non-null   object\n",
      " 11  Number of Blades               3358 non-null   object\n",
      " 12  Country/Region of Manufacture  2564 non-null   object\n",
      " 13  Handle Material                2977 non-null   object\n",
      " 14  Blade Range                    2166 non-null   object\n",
      " 15  Original/Reproduction          2211 non-null   object\n",
      " 16  MPN                            1666 non-null   object\n",
      "dtypes: object(17)\n",
      "memory usage: 706.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1285 entries, 0 to 1284\n",
      "Data columns (total 17 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   Brand                          1278 non-null   object\n",
      " 1   Blade Material                 842 non-null    object\n",
      " 2   Type                           1065 non-null   object\n",
      " 3   Color                          895 non-null    object\n",
      " 4   Model                          1093 non-null   object\n",
      " 5   Blade Edge                     898 non-null    object\n",
      " 6   Original/Reproduction          632 non-null    object\n",
      " 7   Number of Blades               1003 non-null   object\n",
      " 8   Handle Material                836 non-null    object\n",
      " 9   Blade Type                     751 non-null    object\n",
      " 10  Lock Type                      785 non-null    object\n",
      " 11  Opening Mechanism              914 non-null    object\n",
      " 12  MPN                            501 non-null    object\n",
      " 13  Modified Item                  524 non-null    object\n",
      " 14  Country/Region of Manufacture  480 non-null    object\n",
      " 15  Dexterity                      662 non-null    object\n",
      " 16  Blade Range                    571 non-null    object\n",
      "dtypes: object(17)\n",
      "memory usage: 170.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7218 entries, 0 to 7217\n",
      "Data columns (total 16 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   Model                          5651 non-null   object\n",
      " 1   Country/Region of Manufacture  4363 non-null   object\n",
      " 2   Blade Material                 4443 non-null   object\n",
      " 3   Lock Type                      4295 non-null   object\n",
      " 4   Blade Type                     3615 non-null   object\n",
      " 5   Blade Edge                     4702 non-null   object\n",
      " 6   Dexterity                      3644 non-null   object\n",
      " 7   Original/Reproduction          3262 non-null   object\n",
      " 8   Blade Range                    3673 non-null   object\n",
      " 9   Type                           5779 non-null   object\n",
      " 10  Color                          5560 non-null   object\n",
      " 11  Number of Blades               4600 non-null   object\n",
      " 12  Opening Mechanism              4399 non-null   object\n",
      " 13  MPN                            3083 non-null   object\n",
      " 14  Handle Material                5045 non-null   object\n",
      " 15  Brand                          7190 non-null   object\n",
      "dtypes: object(16)\n",
      "memory usage: 902.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4472 entries, 0 to 4471\n",
      "Data columns (total 17 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   Type                           3828 non-null   object\n",
      " 1   Brand                          4448 non-null   object\n",
      " 2   Country/Region of Manufacture  2833 non-null   object\n",
      " 3   Opening Mechanism              2734 non-null   object\n",
      " 4   Blade Material                 2532 non-null   object\n",
      " 5   Color                          3551 non-null   object\n",
      " 6   Model                          3612 non-null   object\n",
      " 7   Blade Edge                     2259 non-null   object\n",
      " 8   Original/Reproduction          1998 non-null   object\n",
      " 9   Handle Material                2831 non-null   object\n",
      " 10  Modified Item                  1627 non-null   object\n",
      " 11  Blade Range                    1682 non-null   object\n",
      " 12  Dexterity                      1475 non-null   object\n",
      " 13  Lock Type                      1619 non-null   object\n",
      " 14  Blade Type                     1856 non-null   object\n",
      " 15  Tools                          2110 non-null   object\n",
      " 16  Number of Blades               2625 non-null   object\n",
      "dtypes: object(17)\n",
      "memory usage: 594.1+ KB\n"
     ]
    }
   ],
   "source": [
    "for dataframe in specs_list:\n",
    "    dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in specs_list:\n",
    "    dataframe.rename({'Brand': 'specBrand'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_bench = bench_merged.join(bench_spec)\n",
    "tot_buck = buck_merged.join(buck_spec)\n",
    "tot_case = case_merged.join(case_spec)\n",
    "tot_caseXX = caseXX_merged.join(caseXX_spec)\n",
    "tot_crkt = crkt_merged.join(crkt_spec)\n",
    "tot_kershaw = kershaw_merged.join(kershaw_spec)\n",
    "tot_sog = sog_merged.join(sog_spec)\n",
    "tot_spyd = spyd_merged.join(spyd_spec)\n",
    "tot_vict = vict_merged.join(vict_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_bench.to_csv('listed_data/total_list_bench.csv', index=False)\n",
    "tot_buck.to_csv('listed_data/total_list_buck.csv', index=False)\n",
    "tot_case.to_csv('listed_data/total_list_case.csv', index=False)\n",
    "tot_caseXX.to_csv('listed_data/total_list_caseXX.csv', index=False)\n",
    "tot_crkt.to_csv('listed_data/total_list_crkt.csv', index=False)\n",
    "tot_kershaw.to_csv('listed_data/total_list_kershaw.csv', index=False)\n",
    "tot_sog.to_csv('listed_data/total_list_sog.csv', index=False)\n",
    "tot_spyd.to_csv('listed_data/total_list_spyd.csv', index=False)\n",
    "tot_vict.to_csv('listed_data/total_list_vict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tot_bench = pd.read_csv('listed_data/total_list_bench.csv')\n",
    "# tot_buck = pd.read_csv('listed_data/total_list_buck.csv')\n",
    "# tot_case = pd.read_csv('listed_data/total_list_case.csv')\n",
    "# tot_caseXX = pd.read_csv('listed_data/total_list_caseXX.csv')\n",
    "# tot_crkt = pd.read_csv('listed_data/total_list_crkt.csv')\n",
    "# tot_kershaw = pd.read_csv('listed_data/total_list_kershaw.csv')\n",
    "# tot_leath = pd.read_csv('listed_data/total_list_leath.csv')\n",
    "# tot_sog = pd.read_csv('listed_data/total_list_sog.csv')\n",
    "# tot_spyd = pd.read_csv('listed_data/total_list_spyd.csv')\n",
    "# tot_vict = pd.read_csv('listed_data/total_list_vict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives = pd.concat([tot_bench, tot_buck,\n",
    "                           tot_case, tot_caseXX,\n",
    "                           tot_crkt, tot_kershaw,\n",
    "                           tot_sog,tot_spyd, \n",
    "                           tot_vict])\n",
    "\n",
    "listed_knives = data_cleaner(listed_knives).copy()\n",
    "listed_knives.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives.drop(['sellingStatus', 'shippingInfo', \n",
    "                    'GalleryURL', 'ItemSpecifics', \n",
    "                    'item_list', 'listingInfo', \n",
    "                    'Modified Item'], \n",
    "                    axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33716 entries, 0 to 33715\n",
      "Data columns (total 39 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   itemId                         33716 non-null  int64  \n",
      " 1   title                          33716 non-null  object \n",
      " 2   galleryURL                     33689 non-null  object \n",
      " 3   viewItemURL                    33716 non-null  object \n",
      " 4   autoPay                        33716 non-null  bool   \n",
      " 5   postalCode                     32247 non-null  object \n",
      " 6   returnsAccepted                33716 non-null  bool   \n",
      " 7   condition                      33716 non-null  float64\n",
      " 8   topRatedListing                33716 non-null  bool   \n",
      " 9   pictureURLLarge                30809 non-null  object \n",
      " 10  pictureURLSuperSize            30574 non-null  object \n",
      " 11  shipping_cost                  33716 non-null  float64\n",
      " 12  price_in_US                    33716 non-null  float64\n",
      " 13  converted_price                33716 non-null  float64\n",
      " 14  brand                          33716 non-null  object \n",
      " 15  cost                           33716 non-null  float64\n",
      " 16  profit                         33716 non-null  float64\n",
      " 17  ROI                            33716 non-null  float64\n",
      " 18  PictureURL                     33713 non-null  object \n",
      " 19  Location                       33714 non-null  object \n",
      " 20  Country                        33716 non-null  object \n",
      " 21  Blade Material                 19579 non-null  object \n",
      " 22  Model                          26511 non-null  object \n",
      " 23  Opening Mechanism              20089 non-null  object \n",
      " 24  Number of Blades               22302 non-null  object \n",
      " 25  Handle Material                22101 non-null  object \n",
      " 26  Blade Type                     15709 non-null  object \n",
      " 27  specBrand                      32830 non-null  object \n",
      " 28  Color                          23677 non-null  object \n",
      " 29  Type                           26116 non-null  object \n",
      " 30  Country/Region of Manufacture  18608 non-null  object \n",
      " 31  Blade Edge                     18015 non-null  object \n",
      " 32  Lock Type                      15308 non-null  object \n",
      " 33  Original/Reproduction          13690 non-null  object \n",
      " 34  Blade Range                    11208 non-null  object \n",
      " 35  Dexterity                      12947 non-null  object \n",
      " 36  MPN                            7068 non-null   object \n",
      " 37  Year                           2173 non-null   object \n",
      " 38  Tools                          1643 non-null   object \n",
      "dtypes: bool(3), float64(7), int64(1), object(28)\n",
      "memory usage: 9.4+ MB\n"
     ]
    }
   ],
   "source": [
    "listed_knives.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives.drop(['Tools'],\n",
    "                    axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives.to_csv(\"listed_data/listed_knives_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of section for obtaining listed data from eBay APIs. Below is the start of processing scraped data from eBay's seller exclusive website. This data goes back 2 years and is filtered to include only used knives with final sale values. The listed data above only goes back 90 days and only shows listings currently up for sale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sold_bench = pd.read_csv(\"terapeak_data/bench_scraped2.csv\")\n",
    "# sold_buck1 = pd.read_csv(\"terapeak_data/buck_scraped2.csv\")\n",
    "# sold_buck2 = pd.read_csv(\"terapeak_data/buck_scraped2_reversed.csv\")\n",
    "# sold_case = pd.read_csv(\"terapeak_data/case_scraped2.csv\")\n",
    "# sold_caseXX1 = pd.read_csv(\"terapeak_data/caseXX_scraped2.csv\")\n",
    "# sold_caseXX2 = pd.read_csv(\"terapeak_data/caseXX2_reversed.csv\")\n",
    "# sold_crkt = pd.read_csv(\"terapeak_data/crkt_scraped.csv\")\n",
    "# sold_kershaw1 = pd.read_csv(\"terapeak_data/kershaw_scraped2.csv\")\n",
    "# sold_kershaw2 = pd.read_csv(\"terapeak_data/kershaw_scraped2_reversed.csv\")\n",
    "# sold_sog = pd.read_csv(\"terapeak_data/SOG_scraped2.csv\")\n",
    "# sold_spyd = pd.read_csv(\"terapeak_data/spyd_scraped2.csv\")\n",
    "# sold_vict1 = pd.read_csv(\"terapeak_data/vict_scraped.csv\")\n",
    "# sold_vict2 = pd.read_csv(\"terapeak_data/vict_reversed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_dict = {'benchmade': sold_bench, \n",
    "           'buck1': sold_buck1,\n",
    "           'buck2': sold_buck2,\n",
    "           'case':sold_case,\n",
    "           'caseXX1':sold_caseXX1,\n",
    "           'caseXX2':sold_caseXX2,\n",
    "           'crkt':sold_crkt,\n",
    "           'kershaw1':sold_kershaw1,\n",
    "           'kershaw2':sold_kershaw2,\n",
    "           'sog':sold_sog, \n",
    "           'spyderco':sold_spyd,\n",
    "           'vict1':sold_vict1,\n",
    "           'vict2':sold_vict2}\n",
    "          \n",
    "\n",
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in df_dict.values():\n",
    "    val.drop('title', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in df_dict.values():\n",
    "    val.rename({'Text': 'title',\n",
    "                'shipping_': 'shipping_cost'},\n",
    "               axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in df_dict.values():\n",
    "    val['date_sold'] = pd.to_datetime(val['date_sold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_buck = pd.concat([sold_buck1,sold_buck2])\n",
    "sold_caseXX = pd.concat([sold_caseXX1,sold_caseXX2])\n",
    "sold_kershaw = pd.concat([sold_kershaw1,sold_kershaw2])\n",
    "sold_vict = pd.concat([sold_vict1,sold_vict2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_buck.drop_duplicates(\n",
    "    subset = ['date_sold', 'price_in_US', 'shipping_cost'],\n",
    "    keep = 'last', inplace=True)\n",
    "\n",
    "sold_caseXX.drop_duplicates(\n",
    "    subset = ['date_sold', 'price_in_US', 'shipping_cost'],\n",
    "    keep = 'last', inplace=True)\n",
    "\n",
    "sold_kershaw.drop_duplicates(\n",
    "    subset = ['date_sold', 'price_in_US', 'shipping_cost'],\n",
    "    keep = 'last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_bench = prepare_tera_df(sold_bench, 0)\n",
    "sold_buck = prepare_tera_df(sold_buck, 1)\n",
    "sold_case = prepare_tera_df(sold_case, 2)\n",
    "sold_caseXX = prepare_tera_df(sold_caseXX, 2)\n",
    "sold_crkt = prepare_tera_df(sold_crkt, 3)\n",
    "sold_kershaw = prepare_tera_df(sold_kershaw, 4)\n",
    "sold_sog = prepare_tera_df(sold_sog, 5)\n",
    "sold_spyd = prepare_tera_df(sold_spyd, 6)\n",
    "sold_vict = prepare_tera_df(sold_vict, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in df_dict.values():\n",
    "    dataframe['title'] = dataframe['title'].str.lower()\n",
    "    dataframe['title'] = dataframe['title'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_df = pd.concat([sold_bench, sold_buck,\n",
    "                     sold_case, sold_caseXX, \n",
    "                     sold_crkt, sold_kershaw,\n",
    "                     sold_sog, sold_spyd,\n",
    "                     sold_vict]) \n",
    "\n",
    "sold_df['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_df.to_csv(\"teraform_data/terapeak_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_knives = data_cleaner(sold_df).copy()\n",
    "sold_knives.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_knives.brand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_knives.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sold_bench.to_csv(\"terapeak_data/tera_bench_prepared.csv\", index=False)\n",
    "# sold_buck.to_csv(\"terapeak_data/tera_buck_prepared.csv\", index=False)\n",
    "# sold_case.to_csv(\"terapeak_data/tera_case_prepared.csv\", index=False)\n",
    "# sold_caseXX.to_csv(\"terapeak_data/tera_caseXX_prepared.csv\", index=False)\n",
    "# sold_crkt.to_csv(\"terapeak_data/tera_crkt_prepared.csv\", index=False)\n",
    "# sold_kershaw.to_csv(\"terapeak_data/tera_kershaw_prepared.csv\", index=False)\n",
    "# sold_sog.to_csv(\"terapeak_data/tera_sog_prepared.csv\", index=False)\n",
    "# sold_spyd.to_csv(\"terapeak_data/tera_spyd_prepared.csv\", index=False)\n",
    "# sold_knives.to_csv(\"terapeak_data/sold_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below block of code merged all available teraform ebay itemIds with the appropriate data. This was done in order to call the ebay Shopping API that will only accept itemIds as input. However, much of the data is older than 90 days and can no longer be accessed using the ebay Shopping API. Therefore, the teraform data will unfortunatly lack additional item specific data.  \n",
    "\n",
    "```\n",
    "teradf_benchIDs = pd.read_csv(\"teraform_data/tera_benchmade_itemID.csv\")\n",
    "teradf_buckIDs = pd.read_csv(\"teraform_data/tera_buck_ItemIDs.csv\")\n",
    "teradf_caseIDs = pd.read_csv(\"teraform_data/tera_case_itemIDs.csv\")\n",
    "teradf_kershawIDs = pd.read_csv(\"teraform_data/tera_kershaw_ItemIDs.csv\")\n",
    "teradf_sogIDs = pd.read_csv(\"teraform_data/tera_sog_ItemIDs.csv\")\n",
    "teradf_spydIDs = pd.read_csv(\"teraform_data/tera_spyderco_ItemIDs.csv\")\n",
    "\n",
    "dfID_list = [teradf_benchIDs,teradf_buckIDs,\n",
    "             teradf_caseIDs, teradf_kershawIDs,\n",
    "             teradf_sogIDs, teradf_spydIDs]\n",
    "\n",
    "for dataframe in dfID_list:\n",
    "    dataframe.rename({'Field4': 'date_sold',\n",
    "                      'Data_field': 'itemID',\n",
    "                      'Title': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "    \n",
    "teradf_kershawIDs.rename({'item': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "                       \n",
    "for dataframe in dfID_list:\n",
    "    dataframe.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "for dataframe in dfID_list:\n",
    "    dataframe.rename({'Field4': 'date_sold',\n",
    "                      'Data_field': 'itemID',\n",
    "                      'Title': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "    dataframe.dropna(inplace=True)\n",
    "    dataframe['itemID'] = dataframe['itemID'].apply(int)\n",
    "\n",
    "teradf_kershawIDs.rename({'item': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "\n",
    "tera_benchIds = teradf_benchIDs.itemID.values.tolist()\n",
    "tera_buckIds = teradf_buckIDs.itemID.values.tolist()\n",
    "tera_caseIds = teradf_caseIDs.itemID.values.tolist()\n",
    "tera_kershawIds = teradf_kershawIDs.itemID.values.tolist()\n",
    "tera_sogIds = teradf_sogIDs.itemID.values.tolist()\n",
    "tera_spydIds = teradf_spydIDs.itemID.values.tolist()\n",
    "\n",
    "idMerge_bench = teradf_bench.merge(teradf_benchIDs, on='Image')\n",
    "idMerge_buck = teradf_buck.merge(teradf_buckIDs)\n",
    "idMerge_case = teradf_case.merge(teradf_caseIDs)\n",
    "idMerge_kershaw = teradf_kershaw.merge(teradf_kershawIDs)\n",
    "idMerge_spyd = teradf_spyd.merge(teradf_spydIDs)\n",
    "idMerge_sog = teradf_sog.merge(teradf_sogIDs)\n",
    "\n",
    "# idMerge_bench.to_csv('teraform_data/tera_bench_idMerge.csv', index=False)\n",
    "# idMerge_buck.to_csv('teraform_data/tera_buck_idMerge.csv', index=False)\n",
    "# idMerge_case.to_csv('teraform_data/tera_case_idMerge.csv', index=False)\n",
    "# idMerge_kershaw.to_csv('teraform_data/tera_kershaw_idMerge.csv', index=False)\n",
    "# idMerge_spyd.to_csv('teraform_data/tera_spyd_idMerge.csv', index=False)\n",
    "# idMerge_sog.to_csv('teraform_data/tera_sog_idMerge.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_color(line):\n",
    "#     pattern = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['color'] = df3.ItemSpecifics.apply(extract_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_type(line):\n",
    "#     pattern = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_manufacture_region(line):\n",
    "#     pattern = re.compile(\"Country/Region of Manufacture\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['region_of_Manufacture'] = df3.ItemSpecifics.apply(extract_manufacture_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['region_of_Manufacture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_handle_material(line):\n",
    "#     pattern = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['handle_material'] = df3.ItemSpecifics.apply(extract_handle_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['handle_material'].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_lock_type(line):\n",
    "#     pattern = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "\n",
    "# df3['lock_type'] = df3.ItemSpecifics.apply(extract_lock_type)\n",
    "\n",
    "# df3['lock_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_edge(line):\n",
    "#     pattern = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['blade_edge'] = df3.ItemSpecifics.apply(extract_blade_edge)\n",
    "\n",
    "# df3['blade_edge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_dexterity(line):\n",
    "#     pattern = re.compile(\"Dexterity\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['dexterity'] = df3.ItemSpecifics.apply(extract_dexterity)\n",
    "\n",
    "# df3['dexterity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.to_csv('data/item_specifics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root='C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pandas as pd\n",
    "# import matplotlib.pyplot  as plt\n",
    "# from PIL import Image\n",
    "# from pathlib import Path\n",
    "# # import imagesize\n",
    "# import numpy as np\n",
    "\n",
    "# # Get the Image Resolutions\n",
    "# imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "# img_meta = {}\n",
    "# for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# # Convert it to Dataframe and compute aspect ratio\n",
    "# img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "# img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "# img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "# print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "# img_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "    price_list.append(listed_price)\n",
    "    \n",
    "df['price_in_US'] = price_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#atttempt to pull shipping cost from json dict\n",
    "shipping_cost_list = []\n",
    "for row in full_dataset:\n",
    "    shipping_cost = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "    shipping_cost_list.append(shipping_cost)\n",
    "    \n",
    "df['shipping_price'] = shipping_cost_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#pull shipping cost from json dict with regex \n",
    "df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#create new feature 'converted price'\n",
    "df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "df = df.drop_duplicates(subset=['title', 'galleryURL'], keep='first')\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "\n",
    "df.to_csv('data/full_dataset.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwLbGNr0TbIj"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* Were there variables you dropped or created?\n",
    "* How did you address missing values or outliers?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "\n",
    "\n",
    "# here you run your code to clean the data\n",
    "\n",
    "```\n",
    "import code.data_cleaning as dc\n",
    "\n",
    "full_dataset = dc.full_clean()\n",
    "```\n",
    "\n",
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "# here you run your code to model the data\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do in the future to improve this project?\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_knife_dir = 'knife_images'\n",
    "# data_profit_dir = 'data/profit'\n",
    "# new_dir = 'split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_folder = os.path.join(new_dir, 'train')\n",
    "# train_profit = os.path.join(train_folder, 'profit')\n",
    "# os.mkdir(train_folder)\n",
    "# os.mkdir(train_profit)\n",
    "\n",
    "# test_folder = os.path.join(new_dir, 'test')\n",
    "# test_profit = os.path.join(test_folder, 'profit')\n",
    "# os.mkdir(test_folder)\n",
    "# os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# val_folder = os.path.join(new_dir, 'validation')\n",
    "# val_profit = os.path.join(val_folder, 'profit')\n",
    "# os.mkdir(val_folder)\n",
    "# os.mkdir(val_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train knife regression images\n",
    "# #80% of data\n",
    "# imgs = knife_images[:5620]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(train_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # test knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[5620:6322]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(test_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # validation knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[6322:]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(val, img)\n",
    "#     shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "plt.imshow(img_rgb)  # graph it\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_checker(index,):\n",
    "    img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "    plt.imshow(img_rgb)  # graph it\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_benchmade_index[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(2286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(1879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6094)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final processing steps for images\n",
    "\n",
    "image_list = []\n",
    "for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "    img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "    img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "    image_list.append(img_rgb)\n",
    "   \n",
    "    # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=  df_CNN_regression['mean_profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small batch\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256, 3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=30,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub['profit'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.2164 * 41.374303202846974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_batch32.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a train set of 60% and a val and test size of 20% each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model showed a lot of indication that it was overfit\n",
    "#need to retry how I split the data \n",
    "#Instead of manul indexing, will use \n",
    "# from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X[:4918]\n",
    "# y_train = y[:4918]\n",
    "\n",
    "# X_train = X[4918:5971]\n",
    "# y_train = y[4918:5971]\n",
    "\n",
    "# X_test = X[5971:]\n",
    "# y_test = y[5971:]\n",
    "\n",
    "\n",
    "# display(len(X_val)/len(X))\n",
    "# display(len(X_train)/len(X))\n",
    "# len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(224 ,224,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=32,\n",
    "#                     batch_size=300,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ## Data Preparation\n",
    "\n",
    "# Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * Were there variables you dropped or created?\n",
    "# * How did you address missing values or outliers?\n",
    "# * Why are these choices appropriate given the data and the business problem?\n",
    "# ***\n",
    "\n",
    "\n",
    "# # here you run your code to clean the data\n",
    "\n",
    "# ```\n",
    "# import code.data_cleaning as dc\n",
    "\n",
    "# full_dataset = dc.full_clean()\n",
    "# ```\n",
    "\n",
    "# ## Data Modeling\n",
    "# Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * How did you analyze or model the data?\n",
    "# * How did you iterate on your initial approach to make it better?\n",
    "# * Why are these choices appropriate given the data and the business problem?\n",
    "# ***\n",
    "# # here you run your code to model the data\n",
    "\n",
    "\n",
    "# ## Evaluation\n",
    "# Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * How do you interpret the results?\n",
    "# * How well does your model fit your data? How much better is this than your baseline model?\n",
    "# * How confident are you that your results would generalize beyond the data you have?\n",
    "# * How confident are you that this model would benefit the business if put into use?\n",
    "# ***\n",
    "\n",
    "\n",
    "# ## Conclusions\n",
    "# Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * What would you recommend the business do as a result of this work?\n",
    "# * What are some reasons why your analysis might not fully solve the business problem?\n",
    "# * What else could you do in the future to improve this project?\n",
    "# ***\n",
    "\n",
    "\n",
    "\n",
    "# # data_knife_dir = 'knife_images'\n",
    "# # data_profit_dir = 'data/profit'\n",
    "# # new_dir = 'split'\n",
    "\n",
    "# # os.mkdir(new_dir)\n",
    "\n",
    "# # train_folder = os.path.join(new_dir, 'train')\n",
    "# # train_profit = os.path.join(train_folder, 'profit')\n",
    "# # os.mkdir(train_folder)\n",
    "# # os.mkdir(train_profit)\n",
    "\n",
    "# # test_folder = os.path.join(new_dir, 'test')\n",
    "# # test_profit = os.path.join(test_folder, 'profit')\n",
    "# # os.mkdir(test_folder)\n",
    "# # os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# # val_folder = os.path.join(new_dir, 'validation')\n",
    "# # val_profit = os.path.join(val_folder, 'profit')\n",
    "# # os.mkdir(val_folder)\n",
    "# # os.mkdir(val_profit)\n",
    "\n",
    "# # val_profit\n",
    "\n",
    "# # # train knife regression images\n",
    "# # #80% of data\n",
    "# # imgs = knife_images[:5620]\n",
    "# # for img in imgs:\n",
    "# #     origin = os.path.join(data_knife_dir, img)\n",
    "# #     destination = os.path.join(train_profit, img)\n",
    "# #     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # # test knife regression images\n",
    "# # #10% of data\n",
    "# # imgs = knife_images[5620:6322]\n",
    "# # for img in imgs:\n",
    "# #     origin = os.path.join(data_knife_dir, img)\n",
    "# #     destination = os.path.join(test_profit, img)\n",
    "# #     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # # validation knife regression images\n",
    "# # #10% of data\n",
    "# # imgs = knife_images[6322:]\n",
    "# # for img in imgs:\n",
    "# #     origin = os.path.join(data_knife_dir, img)\n",
    "# #     destination = os.path.join(val, img)\n",
    "# #     shutil.copyfile(origin, destination)\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from keras.models import load_model\n",
    "# from keras.preprocessing import image\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "# img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "# img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "# plt.imshow(img_rgb)  # graph it\n",
    "# plt.show();\n",
    "\n",
    "\n",
    "# def image_checker(index,):\n",
    "#     img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "#     img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "#     plt.imshow(img_rgb)  # graph it\n",
    "#     plt.show();\n",
    "\n",
    "# top_benchmade_index[:50]\n",
    "\n",
    "# image_checker(6158)\n",
    "\n",
    "# image_checker(2286)\n",
    "\n",
    "# image_checker(1879)\n",
    "\n",
    "# image_checker(4326)\n",
    "\n",
    "# image_checker(6094)\n",
    "\n",
    "# #final processing steps for images\n",
    "\n",
    "# image_list = []\n",
    "# for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "#     img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "#     img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "#     img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "#     image_list.append(img_rgb)\n",
    "   \n",
    "#     # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n",
    "\n",
    "\n",
    "# df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())\n",
    "\n",
    "# df_CNN_regression['mean_profit'].describe()\n",
    "\n",
    "# X = np.array(image_list)\n",
    "\n",
    "# y=  df_CNN_regression['mean_profit']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets\n",
    "\n",
    "# X.shape\n",
    "\n",
    "# y.shape\n",
    "\n",
    "# print(\"Xtrain:\", X_train.shape)\n",
    "# print(\"y_train:\", y_train.shape)\n",
    "# print(\"X_test:\", X_test.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n",
    "\n",
    "# X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# print(\"Xtrain:\", X_train.shape)\n",
    "# print(\"y_train:\", y_train.shape)\n",
    "# print(\"X_test:\", X_test.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n",
    "# print(\"X_val:\", X_test.shape)\n",
    "# print(\"y_val:\", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #small batch\n",
    "\n",
    "# # model = models.Sequential()\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "# #                         input_shape=(256 ,256, 3)))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# # model.add(Dense(256, activation='relu'))\n",
    "# # model.add(Dense(128, activation='relu'))\n",
    "# # model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# # model.compile(loss='mean_squared_error',\n",
    "# #               optimizer='Adam',\n",
    "# #                metrics=['mse'])\n",
    "\n",
    "# # history = model.fit(X_train,\n",
    "# #                     y_train,\n",
    "# #                     epochs=30,\n",
    "# #                     batch_size=32,\n",
    "# #                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "# results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "# #model.summary()\n",
    "\n",
    "\n",
    "# df_scrub['profit'].mean()\n",
    "\n",
    "# 2.2164 * 41.374303202846974\n",
    "\n",
    "# df_scrub.head()\n",
    "\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# #The model learned patterns wells until epoch 20\n",
    "# #after that the loss spikes signifcantly before dropping again\n",
    "# fig = plt.figure(figsize=(12,8))\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.plot\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss( mean square error)')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "# plt.show();\n",
    "\n",
    "# model.save('my_model_batch32.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #a train set of 60% and a val and test size of 20% each \n",
    "\n",
    "# #this model showed a lot of indication that it was overfit\n",
    "# #need to retry how I split the data \n",
    "# #Instead of manul indexing, will use \n",
    "# # from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# # X_train = X[:4918]\n",
    "# # y_train = y[:4918]\n",
    "\n",
    "# # X_train = X[4918:5971]\n",
    "# # y_train = y[4918:5971]\n",
    "\n",
    "# # X_test = X[5971:]\n",
    "# # y_test = y[5971:]\n",
    "\n",
    "\n",
    "# # display(len(X_val)/len(X))\n",
    "# # display(len(X_train)/len(X))\n",
    "# # len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# # model = models.Sequential()\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "# #                         input_shape=(224 ,224,  3)))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Flatten())\n",
    "\n",
    "# # model.add(Dense(512, activation='relu'))\n",
    "# # model.add(Dropout(0.1))\n",
    "\n",
    "# # model.add(Dense(256, activation='relu'))\n",
    "# # model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# # model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# # model.compile(loss='mean_squared_error',\n",
    "# #               optimizer='Adam',\n",
    "# #                metrics=['mse'])\n",
    "# # history = model.fit(X_train,\n",
    "# #                     y_train,\n",
    "# #                     epochs=32,\n",
    "# #                     batch_size=300,\n",
    "# #                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "# #model.summary()\n",
    "\n",
    "\n",
    "# # model.save('my_model_batch500.h5')\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# # model.save('my_model_batch500.h5')\n",
    "\n",
    "# history.history.keys()\n",
    "\n",
    "# #The model is showing a lot of signs of overfitting \n",
    "# fig = plt.figure(figsize=(12,8))\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.plot\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss( mean square error)')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "# plt.show();\n",
    "\n",
    "# X_train.shape\n",
    "\n",
    "# # results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "# #model.summary()\n",
    "\n",
    "\n",
    "# # model.save('my_model_batch500.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
