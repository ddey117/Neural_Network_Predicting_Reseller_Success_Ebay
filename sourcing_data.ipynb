{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.finding import Connection\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.shopping import Connection as Shopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import  json\n",
    "import numpy as np\n",
    "import re\n",
    "# import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create function for organizing API call\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['itemId', 'title', 'galleryURL', \n",
    "#         'viewItemURL', 'autoPay', 'postalCode', \n",
    "#         'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "#         'returnsAccepted', 'condition', 'topRatedListing',\n",
    "#         'galleryPlusPictureURL']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         # Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_df(df):\n",
    "#     price_list = []\n",
    "#     for row in full_dataset:\n",
    "#         listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#         price_list.append(listed_price)\n",
    "\n",
    "#     df['price_in_US'] = price_list\n",
    "#     #pull shipping cost from json dict with regex \n",
    "#     df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "#     df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#     #create new feature 'converted price'\n",
    "#     df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#     df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     display(df.head())\n",
    "#     display(df.info())\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    condition = None\n",
    "    for row in full_dataset:\n",
    "        listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "        price_list.append(listed_price)\n",
    "     \n",
    "        try:\n",
    "            listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "\n",
    "        try:\n",
    "            condition = float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0,\n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "overhead_cost = 3\n",
    "def prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    " \n",
    "    #remove special characters\n",
    "#     df.title.apply(pp.remove_special_chars)\n",
    "    df['brand'] = str(list(bucket_dict.keys())[bucket_dict_position])\n",
    "    df['cost'] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    df['profit'] = (df['converted_price'] -  df['cost'] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/( df['cost'] + overhead_cost))*100.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "                'viewItemURL', 'autoPay', 'postalCode', \n",
    "                'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "                'returnsAccepted', 'condition', 'topRatedListing',\n",
    "                'galleryPlusPictureURL','pictureURLLarge', \n",
    "                'pictureURLSuperSize']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "\n",
    "def knife_request(Brand, dict_pos):\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': 1\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "    #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "\n",
    "    response_pages = response.dict()\n",
    "\n",
    "    full_dataset = []\n",
    "    \n",
    "    total_pages = int(response_pages['paginationOutput']['totalPages'])\n",
    "\n",
    "    if total_pages > 100:\n",
    "        pages_to_request = 100\n",
    "        \n",
    "    else:\n",
    "        pages_to_request = total_pages - 1\n",
    "        \n",
    "        \n",
    "\n",
    "    for page in range(1, pages_to_request):\n",
    "        # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "        # Make the query and get the response\n",
    "\n",
    "        api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "        request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "\n",
    "        response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "        #save the response as a json dict\n",
    "        response_dict = response.dict()\n",
    "\n",
    "\n",
    "        #index dict to appropriate index\n",
    "        results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "        # Call the prepare_data function to get a list of processed data\n",
    "        prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "        # Extend full_dataset with this list (don't append, or you'll get\n",
    "        # a list of lists instead of a flat list)\n",
    "        full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "    display(len(full_dataset))\n",
    "    \n",
    "    df = pd.DataFrame(full_dataset)\n",
    "    \n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    condition = None\n",
    "    for row in full_dataset:\n",
    "        try:\n",
    "            listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "            price_list.append(listed_price)\n",
    "        except:\n",
    "            listed_price = \"Na\"\n",
    "            price_list.append(listed_price)\n",
    "        try:\n",
    "            listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        try:\n",
    "            condition = float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df = prepare_brands(df, dict_pos)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_dataIds(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID','GalleryURL','PictureURL',\n",
    "                'Location','ConvertedCurrentPrice',\n",
    "                'Title','ItemSpecifics', \n",
    "                'Country','ConditionID']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_list(my_list):\n",
    " \n",
    "    api = Shopping(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "    request = {\n",
    "               'itemID': my_list,\n",
    "               'IncludeSelector': 'ItemSpecifics'\n",
    "              }\n",
    "    response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_dataIds(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "    return full_dataset\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "#x = position of bucket_dictionary\n",
    "def prepare_tera_df(df, x):\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\"$\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\",\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].apply(float)\n",
    "    \n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\"$\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\",\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].apply(float)\n",
    "    \n",
    "    df['converted_price'] = (df['price_in_US'] + df['shipping_cost'])\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - list(bucket_dict.values())[x])\n",
    "    df['ROI'] = (df['profit']/(list(bucket_dict.values())[x]))*100.0\n",
    "    \n",
    "    df['brand'] = list(bucket_dict.keys())[x]\n",
    "    df['cost'] = list(bucket_dict.values())[x]\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of API calls for listed data. To be merged with item specific data using ebay itemIds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running functions to call the Finding API and return datasets for cat () knives for sale listed on ebay in the last 90 days. (explain how ebay rules work)\n",
    "\n",
    "```\n",
    "bench_df = knife_request('Benchmade', 0)\n",
    "buck_df = knife_request('Buck', 1)\n",
    "case_df = knife_request('Case', 2)\n",
    "df_caseXX = knife_request('Case XX', 2)\n",
    "df_crkt = knife_request(\"CRKT\", 3)\n",
    "df_leatherman = knife_request('Leatherman', 5)\n",
    "df_sog = knife_request('SOG', 6)\n",
    "df_spyderco = knife_request('Spyderco', 7)\n",
    "\n",
    "\n",
    "bench_df.to_csv('data/df_bench1.csv', index=False)\n",
    "buck_df.to_csv('data/df_buck.csv', index=False)\n",
    "case_df.to_csv('data/df_case.csv', index=False)\n",
    "df_caseXX.to_csv('data/df_CaseXX.csv', index=False)\n",
    "df_crkt.to_csv('data/df_crkt.csv', index=False)\n",
    "df_leatherman.to_csv('data/df_leatherman.csv', index=False)\n",
    "df_sog.to_csv('data/df_sog.csv', index=False)\n",
    "df_spyderco.to_csv('data/df_spyderco.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kershaw and victorinox data was requested using the FindingAPI below after tweaking some pagination through trial and error to maximize data.\n",
    "\n",
    "```\n",
    "full_dataset = []\n",
    "for page in range(1, 57):\n",
    "#         # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "#         # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "        #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "\n",
    "    df = pd.DataFrame(full_dataset)\n",
    "    \n",
    "df_kershaw = prepare_df(df)\n",
    "df_kershaw = prepare_brands(df_kershaw, 4)\n",
    "df_kershaw.to_csv('data/df_kershaw.csv', index=False)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "full_dataset = []\n",
    "for page in range(1, 86):\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "df_victorinox = pd.DataFrame(full_dataset)\n",
    "df_victorinox = prepare_df(df_victorinox)\n",
    "df_victorinox = prepare_brands(df_victorinox, 8)\n",
    "df_victorinox.to_csv('data/df_victorinox.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end of API call for listed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start of API call section using IDs from preview listed datasets to get Item Specific data from ebay. This will return more descriptive information about the knives, pulling from a container on the website that sellers must complete to post a listing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = pd.read_csv(\"listed_data/df_bench1.csv\")\n",
    "df_buck = pd.read_csv(\"listed_data/df_buck.csv\")\n",
    "df_case = pd.read_csv(\"listed_data/df_case.csv\")\n",
    "df_caseXX = pd.read_csv(\"listed_data/df_CaseXX.csv\")\n",
    "df_crkt = pd.read_csv(\"listed_data/df_crkt.csv\")\n",
    "df_kershaw = pd.read_csv(\"listed_data/df_kershaw.csv\")\n",
    "df_leatherman = pd.read_csv(\"listed_data/df_leatherman.csv\")\n",
    "df_sog = pd.read_csv(\"listed_data/df_sog.csv\")\n",
    "df_spyderco = pd.read_csv(\"listed_data/df_spyderco.csv\")\n",
    "df_victorinox = pd.read_csv(\"listed_data/df_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchIds = df_bench.itemId.values.tolist()\n",
    "buckIds = df_buck.itemId.values.tolist()\n",
    "caseIds = df_case.itemId.values.tolist()\n",
    "caseXXIds = df_caseXX.itemId.values.tolist()\n",
    "crktIds = df_crkt.itemId.values.tolist()\n",
    "kershawIds = df_kershaw.itemId.values.tolist()\n",
    "leathIds = df_leatherman.itemId.values.tolist()\n",
    "sogIds = df_sog.itemId.values.tolist()\n",
    "spydIds = df_spyderco.itemId.values.tolist()\n",
    "victIds = df_victorinox.itemId.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return benchmade item specific data.\n",
    "\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(benchIds), 20):\n",
    "    process_list(benchIds[i:i+20])\n",
    "\n",
    "bench = pd.DataFrame(full_dataset)\n",
    "bench.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "bench.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return buck item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(buckIds), 20):\n",
    "    process_list(buckIds[i:i+20])\n",
    "\n",
    "buck = pd.DataFrame(full_dataset)\n",
    "buck.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "buck.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return case brand item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(caseIds), 20):\n",
    "    process_list(caseIds[i:i+20])\n",
    "\n",
    "df_case = pd.DataFrame(full_dataset)\n",
    "df_case.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_case.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return caseXX brand item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(caseXXIds), 20):\n",
    "    process_list(caseXXIds[i:i+20])\n",
    "\n",
    "df_caseXX = pd.DataFrame(full_dataset)\n",
    "df_caseXX.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_caseXX.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return crkt item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(crktIds), 20):\n",
    "    process_list(crktIds[i:i+20])\n",
    "\n",
    "crkt = pd.DataFrame(full_dataset)\n",
    "crkt.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "crkt.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return kershaw item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(kershawIds), 20):\n",
    "    process_list(kershawIds[i:i+20])\n",
    "\n",
    "kershaw = pd.DataFrame(full_dataset)\n",
    "kershaw.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "kershaw.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return leatherman item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(leathIds), 20):\n",
    "    process_list(leathIds[i:i+20])\n",
    "\n",
    "leath = pd.DataFrame(full_dataset)\n",
    "leath.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "leath.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return SOG item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(sogIds), 20):\n",
    "    process_list(sogIds[i:i+20])\n",
    "\n",
    "sog = pd.DataFrame(full_dataset)\n",
    "sog.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "sog.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return spyderco item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(spydIds), 20):\n",
    "    process_list(spydIds[i:i+20])\n",
    "\n",
    "spyd = pd.DataFrame(full_dataset)\n",
    "spyd.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "spyd.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return victorinox item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(victIds), 20):\n",
    "    process_list(victIds[i:i+20])\n",
    "    \n",
    "vict = pd.DataFrame(full_dataset)\n",
    "vict.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "vict.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bench.to_csv(\"listed_data/benchIds.csv\", index=False)\n",
    "buck.to_csv(\"listed_data/buckIds.csv\", index=False)\n",
    "case.to_csv(\"listed_data/caseIds.csv\", index=False)\n",
    "caseXX.to_csv(\"listed_data/caseXXIds.csv\", index=False)\n",
    "crkt.to_csv(\"listed_data/crktIds.csv\", index=False)\n",
    "kershaw.to_csv(\"listed_data/kershawIds.csv\", index=False)\n",
    "leath.to_csv(\"listed_data/leathIds.csv\", index=False)\n",
    "sog.to_csv(\"listed_data/sogIds.csv\", index=False)\n",
    "spyd.to_csv(\"listed_data/spydIds.csv\", index=False)\n",
    "vict.to_csv(\"listed_data/victIds.csv\", index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of prep to merge original listed data with item specific data requested using a seperate API for more complete details about all listings gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv(\"listed_data/benchIds.csv\")\n",
    "buck = pd.read_csv(\"listed_data/buckIds.csv\")\n",
    "case = pd.read_csv(\"listed_data/caseIds.csv\")\n",
    "caseXX = pd.read_csv(\"listed_data/caseXXIds.csv\")\n",
    "crkt = pd.read_csv(\"listed_data/crktIds.csv\")\n",
    "kershaw = pd.read_csv(\"listed_data/kershawIds.csv\")\n",
    "leath = pd.read_csv(\"listed_data/leathIds.csv\")\n",
    "sog = pd.read_csv(\"listed_data/sogIds.csv\")\n",
    "spyd = pd.read_csv(\"listed_data/spydIds.csv\")\n",
    "vict = pd.read_csv(\"listed_data/victIds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buck.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buck.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [bench,buck,\n",
    "           case,caseXX,\n",
    "           crkt,kershaw,\n",
    "           leath,sog,\n",
    "           spyd,vict]\n",
    "\n",
    "for dataframe in df_list:\n",
    "    dataframe.rename({'Title': 'title',\n",
    "                      'ItemID': 'itemId'},\n",
    "                     axis=1,inplace=True)\n",
    "    \n",
    "    dataframe.drop(['ConditionID','ConvertedCurrentPrice'], \n",
    "                   axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.title.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(df):\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    return df\n",
    "\n",
    "bench = lower(bench)\n",
    "buck = lower(buck)\n",
    "case = lower(case)\n",
    "caseXX = lower(caseXX)\n",
    "crkt = lower(crkt)\n",
    "kershaw = lower(kershaw)\n",
    "leath = lower(leath)\n",
    "spyd = lower(spyd)\n",
    "sog = lower(sog)\n",
    "vict = lower(vict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spyd.title.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge Item Specific dataframes with original listed data using itemIds and title\n",
    "bench_merged = df_bench.merge(bench)\n",
    "buck_merged = df_buck.merge(buck)\n",
    "case_merged = df_case.merge(case)\n",
    "caseXX_merged = df_caseXX.merge(caseXX)\n",
    "crkt_merged = df_crkt.merge(crkt)\n",
    "kershaw_merged = df_kershaw.merge(kershaw)\n",
    "leath_merged = df_leatherman.merge(leath)\n",
    "spyd_merged = df_spyderco.merge(spyd)\n",
    "sog_merged = df_sog.merge(sog)\n",
    "vict_merged = df_victorinox.merge(vict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_merged.to_csv('teraform_data/bench_merged.csv',index=False)\n",
    "buck_merged.to_csv('teraform_data/buck_merged.csv',index=False)\n",
    "case_merged.to_csv('teraform_data/case_merged.csv',index=False)\n",
    "caseXX_merged.to_csv('teraform_data/caseXX_merged.csv',index=False)\n",
    "crkt_merged.to_csv('teraform_data/crkt_merged.csv',index=False)\n",
    "kershaw_merged.to_csv('teraform_data/kershaw_merged.csv',index=False)\n",
    "sog_merged.to_csv('teraform_data/sog_merged.csv',index=False)\n",
    "spyd_merged.to_csv('teraform_data/spyd_merged.csv',index=False)\n",
    "vict_merged.to_csv('teraform_data/vict_merged.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of API calls for listed data on ebay website. Now will be using scraped data from ebay's proprietery Teraform webapp for researching data. This data shows used knives that were marked as sold and complete and goes back two years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench = pd.read_csv(\"teraform_data/tera_benchmade.csv\")\n",
    "teradf_buck = pd.read_csv(\"teraform_data/tera_buck.csv\")\n",
    "teradf_case = pd.read_csv(\"teraform_data/tera_case.csv\")\n",
    "teradf_crkt = pd.read_csv(\"teraform_data/tera_CRKT.csv\")\n",
    "teradf_kershaw = pd.read_csv(\"teraform_data/tera_kershaw.csv\")\n",
    "teradf_leath = pd.read_csv(\"teraform_data/tera_leatherman.csv\")\n",
    "teradf_sog = pd.read_csv(\"teraform_data/tera_SOG.csv\")\n",
    "teradf_spyd = pd.read_csv(\"teraform_data/tera_spyderco.csv\")\n",
    "teradf_vict = pd.read_csv(\"teraform_data/tera_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'benchmade': teradf_bench, \n",
    "           'buck': teradf_buck,\n",
    "           'case':teradf_case,\n",
    "           'crkt':teradf_crkt,\n",
    "           'kershaw':teradf_kershaw,\n",
    "           'leatherman':teradf_leath,\n",
    "           'sog':teradf_sog, \n",
    "           'spyderco':teradf_spyd,\n",
    "           'victorinox':teradf_vict}\n",
    "\n",
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench.rename({'Data_field':'title', \n",
    "                     'Data_field1':'price_in_US', \n",
    "                     'Data_field2':'shipping_cost', \n",
    "                     'Field2':'date_sold'},\n",
    "                     axis=1, inplace=True)\n",
    "\n",
    "teradf_buck.rename({'avg_price':'price_in_US', \n",
    "                    'avg_shipping':'shipping_cost'},\n",
    "                     axis=1, inplace=True)\n",
    "\n",
    "teradf_case.rename({'url':'title', \n",
    "                    'Field5':'title2', \n",
    "                    'Field4': 'date_sold',\n",
    "                    'Field': 'url',\n",
    "                    'avg_price': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "\n",
    "teradf_crkt.rename({'Field2':'date_sold', \n",
    "                    'title1':'title', \n",
    "                    'avg_cost': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "\n",
    "teradf_kershaw.rename({'Field':'url', \n",
    "                       'Field1':'title', \n",
    "                       'Field3':'date_sold',\n",
    "                       'avg_price': 'price_in_US',\n",
    "                       'avg_shipping': 'shipping_cost'\n",
    "                       }, axis=1, inplace=True)\n",
    "\n",
    "teradf_leath.rename({'Field2':'date_sold', \n",
    "                     'title1':'title2', \n",
    "                     'avg_price': 'price_in_US',\n",
    "                     'avg_shipping': 'shipping_cost'\n",
    "                     }, axis=1, inplace=True)\n",
    "\n",
    "teradf_sog.rename({'Field2':'date_sold', \n",
    "                   'title1':'title', \n",
    "                   'avg_price': 'price_in_US',\n",
    "                   'avg_shipping': 'shipping_cost'\n",
    "                   }, axis=1, inplace=True)\n",
    "\n",
    "teradf_spyd.rename({'Field2':'date_sold', \n",
    "                    'Field3':'title', \n",
    "                    'name':'title2',\n",
    "                    'avg_price': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "\n",
    "teradf_vict.rename({'FfImage':'Image', \n",
    "                    'Field3':'date_sold', \n",
    "                    'avg_price': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "# teradf_bench.drop(['Field', 'Price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_case.drop(['units_sold', 'Price'], axis=1, inplace=True)\n",
    "teradf_crkt.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_kershaw.drop(['Field2', 'Price'], axis=1, inplace=True)\n",
    "teradf_leath.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_sog.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_spyd.drop(['units_sold', 'Price'], axis=1, inplace=True)\n",
    "teradf_vict.drop(['Field2', 'Price'], axis=1, inplace=True)\n",
    "\n",
    "teradf_buck['title'].fillna(teradf_buck['title2'], inplace=True)\n",
    "teradf_case['title'].fillna(teradf_case['title2'], inplace=True)\n",
    "teradf_crkt['title'].fillna(teradf_crkt['title2'], inplace=True)\n",
    "teradf_leath['title'].fillna(teradf_leath['title2'], inplace=True)\n",
    "teradf_sog['title'].fillna(teradf_sog['title2'], inplace=True)\n",
    "teradf_spyd['title'].fillna(teradf_spyd['title2'], inplace=True)\n",
    "teradf_vict['title'].fillna(teradf_vict['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title2_list = [teradf_buck, teradf_case,\n",
    "               teradf_crkt, teradf_leath,\n",
    "               teradf_sog, teradf_spyd,\n",
    "               teradf_vict]\n",
    "               \n",
    "for dataframe in title2_list:\n",
    "    dataframe.drop('title2', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'benchmade': teradf_bench, \n",
    "           'buck': teradf_buck,\n",
    "           'case':teradf_case,\n",
    "           'crkt':teradf_crkt,\n",
    "           'kershaw':teradf_kershaw,\n",
    "           'leatherman':teradf_leath,\n",
    "           'sog':teradf_sog, \n",
    "           'spyderco':teradf_spyd,\n",
    "           'victorinox':teradf_vict}\n",
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench = prepare_tera_df(teradf_bench, 0)\n",
    "teradf_buck = prepare_tera_df(teradf_buck, 1)\n",
    "teradf_case = prepare_tera_df(teradf_case, 2)\n",
    "teradf_crkt = prepare_tera_df(teradf_crkt, 3)\n",
    "teradf_kershaw = prepare_tera_df(teradf_kershaw, 4)\n",
    "teradf_leath = prepare_tera_df(teradf_leath, 5)\n",
    "teradf_sog = prepare_tera_df(teradf_sog, 6)\n",
    "teradf_spyd = prepare_tera_df(teradf_spyd, 7)\n",
    "teradf_vict = prepare_tera_df(teradf_vict, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teradf_bench.to_csv(\"teraform_data/tera_bench_prepared.csv\", index=False)\n",
    "# teradf_buck.to_csv(\"teraform_data/tera_buck_prepared.csv\", index=False)\n",
    "# teradf_case.to_csv(\"teraform_data/tera_case_prepared.csv\", index=False)\n",
    "# teradf_crkt.to_csv(\"teraform_data/tera_crkt_prepared.csv\", index=False)\n",
    "# teradf_kershaw.to_csv(\"teraform_data/tera_kershaw_prepared.csv\", index=False)\n",
    "# teradf_leath.to_csv(\"teraform_data/tera_leatherman_prepared.csv\", index=False)\n",
    "# teradf_sog.to_csv(\"teraform_data/tera_sog_prepared.csv\", index=False)\n",
    "# teradf_spyd.to_csv(\"teraform_data/tera_spyd_prepared.csv\", index=False)\n",
    "# teradf_vict.to_csv(\"teraform_data/tera_victorinox_prepared.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench = pd.read_csv(\"teraform_data/tera_bench_prepared.csv\")\n",
    "teradf_buck = pd.read_csv(\"teraform_data/tera_buck_prepared.csv\")\n",
    "teradf_case = pd.read_csv(\"teraform_data/tera_case_prepared.csv\")\n",
    "teradf_crkt = pd.read_csv(\"teraform_data/tera_crkt_prepared.csv\")\n",
    "teradf_kershaw = pd.read_csv(\"teraform_data/tera_kershaw_prepared.csv\")\n",
    "teradf_leath = pd.read_csv(\"teraform_data/tera_leatherman_prepared.csv\")\n",
    "teradf_sog = pd.read_csv(\"teraform_data/tera_sog_prepared.csv\")\n",
    "teradf_spyd = pd.read_csv(\"teraform_data/tera_spyd_prepared.csv\")\n",
    "teradf_vict = pd.read_csv(\"teraform_data/tera_victorinox_prepared.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_benchIDs = pd.read_csv(\"teraform_data/tera_benchmade_itemID.csv\")\n",
    "teradf_buckIDs = pd.read_csv(\"teraform_data/tera_buck_ItemIDs.csv\")\n",
    "teradf_caseIDs = pd.read_csv(\"teraform_data/tera_case_itemIDs.csv\")\n",
    "teradf_kershawIDs = pd.read_csv(\"teraform_data/tera_kershaw_ItemIDs.csv\")\n",
    "teradf_sogIDs = pd.read_csv(\"teraform_data/tera_sog_ItemIDs.csv\")\n",
    "teradf_spydIDs = pd.read_csv(\"teraform_data/tera_spyderco_ItemIDs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfID_list = [teradf_benchIDs,teradf_buckIDs,\n",
    "             teradf_caseIDs, teradf_kershawIDs,\n",
    "             teradf_sogIDs, teradf_spydIDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dfID_list:\n",
    "    dataframe.rename({'Field4': 'date_sold',\n",
    "                      'Data_field': 'itemID',\n",
    "                      'Title': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "    \n",
    "tera_kershawIDs.rename({'item': 'title'}, \n",
    "                       axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dfID_list:\n",
    "    dataframe.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in dfID_list:\n",
    "    dataframe['itemID'] = dataframe['itemID'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_benchIds = teradf_benchIDs.itemID.values.tolist()\n",
    "tera_buckIds = teradf_buckIDs.itemID.values.tolist()\n",
    "tera_caseIds = teradf_caseIDs.itemID.values.tolist()\n",
    "tera_kershawIds = teradf_kershawIDs.itemID.values.tolist()\n",
    "tera_sogIds = teradf_sogIDs.itemID.values.tolist()\n",
    "tera_spydIds = teradf_spydIDs.itemID.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "'GetMultipleItems: Class: RequestError, Severity: Error, Code: 1.33, Token not available in request.Token not available in request. Please specify a valid token as HTTP header.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-4a6cb952ed4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtera_benchIds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprocess_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtera_benchIds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbench\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-14d505a0a54d>\u001b[0m in \u001b[0;36mprocess_list\u001b[0;34m(my_list)\u001b[0m\n\u001b[1;32m    267\u001b[0m                \u001b[0;34m'IncludeSelector'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ItemSpecifics'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m               }\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GetMultipleItems'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/ebaysdk/connection.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, verb, data, list_nodes, verb_attrs, files)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total time=%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/ebaysdk/connection.py\u001b[0m in \u001b[0;36merror_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mestr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresponse_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: 'GetMultipleItems: Class: RequestError, Severity: Error, Code: 1.33, Token not available in request.Token not available in request. Please specify a valid token as HTTP header.'"
     ]
    }
   ],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(tera_benchIds), 20):\n",
    "    process_list(tera_benchIds[i:i+20])\n",
    "\n",
    "bench = pd.DataFrame(full_dataset)\n",
    "bench.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "bench.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [teradf_bench,teradf_buck,\n",
    "           teradf_case, teradf_crkt,\n",
    "           teradf_kershaw, teradf_leath,\n",
    "           teradf_sog, teradf_spyd,\n",
    "           teradf_vict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in df_list:\n",
    "    dataframe['title'] = dataframe['title'].str.lower()\n",
    "    dataframe['title'] = dataframe['title'].str.strip()\n",
    "    \n",
    "for dataframe in dfID_list:\n",
    "    dataframe['title'] = dataframe['title'].str.lower()\n",
    "    dataframe['title'] = dataframe['title'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([df_bench,df_buck,df_case,df_crkt,df_kershaw,df_leatherman,df_spyderco,df_sog,df_victorinox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379    benchmade 13150-1 harley-davidson  mini hardta...\n",
       "146             benchmade - osborne barrage folding knife\n",
       "1669                     1990  one benchmade pocket knife\n",
       "1704    benchmade mini osborne 945bk-1 knife flytanium...\n",
       "486     benchmade 755bk mpr m390 shane sibert knife ge...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tera_benchIDs.title.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2095    benchmade griptilian knife gray/blue g-10 hand...\n",
       "137     benchmade 535bk-2 bugout axis lock knife black...\n",
       "6333    , preview full size image#831 black benchmade ...\n",
       "2305    benchmade 710 folding knife axis lock mchenry ...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teradf_bench.title.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"(, preview full size image)\")\n",
    "teradf_bench['title'] = teradf_bench['title'].apply(lambda x: re.sub(pattern, '', x))\n",
    "# pattern = re.compile(\"(, preview full size image)\")\n",
    "# teradf_bench['title'] = teradf_bench['title'].apply(lambda x: re.sub(pattern, '', x))\n",
    "# pattern = re.compile(\"^(benchmade)\")\n",
    "# teradf_bench['title'] = teradf_bench['title'].apply(lambda x: re.sub(pattern, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_merged_t = teradf_bench.merge(tera_benchIDs, on='Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bench_merged_t = teradf_bench.merge(tera_benchIDs, on='title')\n",
    "buck_merged_t = teradf_buck.merge(tera_buckIDs, on='title')\n",
    "case_merged_t = teradf_case.merge(tera_caseIDs, on='title')\n",
    "kershaw_merged_t = teradf_kershaw.merge(tera_kershawIDs, on='title')\n",
    "spyd_merged_t = teradf_spyd.merge(tera_spydIDs, on='title')\n",
    "sog_merged_t = teradf_sog.merge(tera_sogIDs, on='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_merged_t.to_csv('teraform_data/tera_bench_merged.csv', index=False)\n",
    "buck_merged_t.to_csv('teraform_data/tera_buck_merged.csv', index=False)\n",
    "case_merged_t.to_csv('teraform_data/tera_case_merged.csv', index=False)\n",
    "kershaw_merged_t.to_csv('teraform_data/tera_kershaw_merged.csv', index=False)\n",
    "spyd_merged_t.to_csv('teraform_data/tera_spyd_merged.csv', index=False)\n",
    "sog_merged_t.to_csv('teraform_data/tera_sog_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_type(line):\n",
    "#     pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.ItemSpecifics.sample(10).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_color(line):\n",
    "#     pattern = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['color'] = df3.ItemSpecifics.apply(extract_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_type(line):\n",
    "#     pattern = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_manufacture_region(line):\n",
    "#     pattern = re.compile(\"Country/Region of Manufacture\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['region_of_Manufacture'] = df3.ItemSpecifics.apply(extract_manufacture_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['region_of_Manufacture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_handle_material(line):\n",
    "#     pattern = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['handle_material'] = df3.ItemSpecifics.apply(extract_handle_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['handle_material'].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_lock_type(line):\n",
    "#     pattern = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "\n",
    "# df3['lock_type'] = df3.ItemSpecifics.apply(extract_lock_type)\n",
    "\n",
    "# df3['lock_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_edge(line):\n",
    "#     pattern = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['blade_edge'] = df3.ItemSpecifics.apply(extract_blade_edge)\n",
    "\n",
    "# df3['blade_edge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_dexterity(line):\n",
    "#     pattern = re.compile(\"Dexterity\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['dexterity'] = df3.ItemSpecifics.apply(extract_dexterity)\n",
    "\n",
    "# df3['dexterity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.to_csv('data/item_specifics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root='C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pandas as pd\n",
    "# import matplotlib.pyplot  as plt\n",
    "# from PIL import Image\n",
    "# from pathlib import Path\n",
    "# # import imagesize\n",
    "# import numpy as np\n",
    "\n",
    "# # Get the Image Resolutions\n",
    "# imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "# img_meta = {}\n",
    "# for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# # Convert it to Dataframe and compute aspect ratio\n",
    "# img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "# img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "# img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "# print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "# img_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "    price_list.append(listed_price)\n",
    "    \n",
    "df['price_in_US'] = price_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#atttempt to pull shipping cost from json dict\n",
    "shipping_cost_list = []\n",
    "for row in full_dataset:\n",
    "    shipping_cost = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "    shipping_cost_list.append(shipping_cost)\n",
    "    \n",
    "df['shipping_price'] = shipping_cost_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#pull shipping cost from json dict with regex \n",
    "df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#create new feature 'converted price'\n",
    "df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "df = df.drop_duplicates(subset=['title', 'galleryURL'], keep='first')\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "\n",
    "df.to_csv('data/full_dataset.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwLbGNr0TbIj"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* Were there variables you dropped or created?\n",
    "* How did you address missing values or outliers?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "\n",
    "\n",
    "# here you run your code to clean the data\n",
    "\n",
    "```\n",
    "import code.data_cleaning as dc\n",
    "\n",
    "full_dataset = dc.full_clean()\n",
    "```\n",
    "\n",
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "# here you run your code to model the data\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do in the future to improve this project?\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_knife_dir = 'knife_images'\n",
    "# data_profit_dir = 'data/profit'\n",
    "# new_dir = 'split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_folder = os.path.join(new_dir, 'train')\n",
    "# train_profit = os.path.join(train_folder, 'profit')\n",
    "# os.mkdir(train_folder)\n",
    "# os.mkdir(train_profit)\n",
    "\n",
    "# test_folder = os.path.join(new_dir, 'test')\n",
    "# test_profit = os.path.join(test_folder, 'profit')\n",
    "# os.mkdir(test_folder)\n",
    "# os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# val_folder = os.path.join(new_dir, 'validation')\n",
    "# val_profit = os.path.join(val_folder, 'profit')\n",
    "# os.mkdir(val_folder)\n",
    "# os.mkdir(val_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train knife regression images\n",
    "# #80% of data\n",
    "# imgs = knife_images[:5620]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(train_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # test knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[5620:6322]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(test_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # validation knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[6322:]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(val, img)\n",
    "#     shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "plt.imshow(img_rgb)  # graph it\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_checker(index,):\n",
    "    img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "    plt.imshow(img_rgb)  # graph it\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_benchmade_index[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(2286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(1879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6094)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final processing steps for images\n",
    "\n",
    "image_list = []\n",
    "for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "    img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "    img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "    image_list.append(img_rgb)\n",
    "   \n",
    "    # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=  df_CNN_regression['mean_profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small batch\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256, 3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=30,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub['profit'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.2164 * 41.374303202846974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_batch32.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a train set of 60% and a val and test size of 20% each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model showed a lot of indication that it was overfit\n",
    "#need to retry how I split the data \n",
    "#Instead of manul indexing, will use \n",
    "# from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X[:4918]\n",
    "# y_train = y[:4918]\n",
    "\n",
    "# X_train = X[4918:5971]\n",
    "# y_train = y[4918:5971]\n",
    "\n",
    "# X_test = X[5971:]\n",
    "# y_test = y[5971:]\n",
    "\n",
    "\n",
    "# display(len(X_val)/len(X))\n",
    "# display(len(X_train)/len(X))\n",
    "# len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(224 ,224,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=32,\n",
    "#                     batch_size=300,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model is showing a lot of signs of overfitting \n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# from ebaysdk.finding import Connection\n",
    "# import requests\n",
    "\n",
    "# from ebaysdk.shopping import Connection as Shopping\n",
    "\n",
    "# import pandas as pd \n",
    "# import  json\n",
    "# import numpy as np\n",
    "# import re\n",
    "# import preprocess_ddey117 as pp\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# import seaborn as sns \n",
    "\n",
    "# df = pd.read_csv('data/tera_df_prepared.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')\n",
    "\n",
    "# df.info()\n",
    "\n",
    "# api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "# request = {\n",
    "#            'itemID': list(itemIds),\n",
    "#            'IncludeSelector': 'ItemSpecifics'\n",
    "#           }\n",
    "# response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "# response_dict = response.dict()\n",
    "\n",
    "# results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "\n",
    "\n",
    "# response_dict = response.dict()\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "# # results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "# results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "# #create function for organizing API call\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "#                 'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "#                 'Country', 'Location', 'ConvertedCurrentPrice',\n",
    "#                 'PictureURL']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "# #         Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# #create function for organizing API call\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "#                 'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "\n",
    "#                 'PictureURL']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "# #         Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def process_list(my_list):\n",
    " \n",
    "#     api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "#     request = {\n",
    "#                'itemID': my_list,\n",
    "#                'IncludeSelector': 'ItemSpecifics'\n",
    "#               }\n",
    "#     response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "    \n",
    "#     return full_dataset\n",
    "\n",
    "# full_dataset = []\n",
    "# for i in range(0, len(itemIds), 20):\n",
    "#     process_list(itemIds[i:i+20])\n",
    "\n",
    "# itemIds = list(df.itemId)\n",
    "\n",
    "# full_dataset = []\n",
    "# for i in range(0, len(itemIds), 20):\n",
    "#     process_list(itemIds[i:i+20])\n",
    "\n",
    "# len(full_dataset)\n",
    "\n",
    "# full_dataset[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df2 = pd.DataFrame(full_dataset)\n",
    "\n",
    "# df3 = df2.drop_duplicates(subset='ItemID').copy()\n",
    "\n",
    "# df.info()\n",
    "\n",
    "# df3.ItemSpecifics.sample(5).apply(print)\n",
    "\n",
    "# import re\n",
    "\n",
    "# pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "# def extract_opening_mech(line):\n",
    "#     pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match\n",
    "\n",
    "# df3['opening_mechanism'] = df3.ItemSpecifics.apply(extract_opening_mech)\n",
    "\n",
    "# df3\n",
    "\n",
    "# pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "# def extract_blade_type(line):\n",
    "#     pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match\n",
    "\n",
    "# df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)\n",
    "\n",
    "# df3.ItemSpecifics.sample(10).apply(print)\n",
    "\n",
    "# def extract_color(line):\n",
    "#     pattern = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match\n",
    "\n",
    "# df3['color'] = df3.ItemSpecifics.apply(extract_color)\n",
    "\n",
    "# def extract_blade_type(line):\n",
    "#     pattern = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match\n",
    "\n",
    "# df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)\n",
    "\n",
    "# df3['blade_type'].value_counts()[:20]\n",
    "\n",
    "# def extract_manufacture_region(line):\n",
    "#     pattern = re.compile(\"Country/Region of Manufacture\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "\n",
    "\n",
    "# df3['region_of_Manufacture'] = df3.ItemSpecifics.apply(extract_manufacture_region)\n",
    "\n",
    "# df3['region_of_Manufacture'].value_counts()\n",
    "\n",
    "# def extract_handle_material(line):\n",
    "#     pattern = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "\n",
    "\n",
    "# df3['handle_material'] = df3.ItemSpecifics.apply(extract_handle_material)\n",
    "\n",
    "# df3['handle_material'].value_counts()[:50]\n",
    "\n",
    "# def extract_lock_type(line):\n",
    "#     pattern = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "\n",
    "# df3['lock_type'] = df3.ItemSpecifics.apply(extract_lock_type)\n",
    "\n",
    "# df3['lock_type'].value_counts()\n",
    "\n",
    "# def extract_blade_edge(line):\n",
    "#     pattern = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['blade_edge'] = df3.ItemSpecifics.apply(extract_blade_edge)\n",
    "\n",
    "# df3['blade_edge'].value_counts()\n",
    "\n",
    "# def extract_dexterity(line):\n",
    "#     pattern = re.compile(\"Dexterity\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['dexterity'] = df3.ItemSpecifics.apply(extract_dexterity)\n",
    "\n",
    "# df3['dexterity'].value_counts()\n",
    "\n",
    "# df3.to_csv('data/item_specifics_df.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "# # request = {\n",
    "# #            'itemID': list(itemIds),\n",
    "# #            'IncludeSelector': 'ItemSpecifics'\n",
    "# #           }\n",
    "# # response = api.execute('GetMultipleItems', request)\n",
    "# # print(response.dict())\n",
    "\n",
    "# ls\n",
    "\n",
    "# from os import listdir\n",
    "# from PIL import Image\n",
    "\n",
    "# for imageFolder in listdir('./nn_images2'):\n",
    "#     try:\n",
    "#         img = Image.open('./nn_images2/'+imageFolder)\n",
    "#         img.verify()     # to veify if its an img\n",
    "#         img.close()     #to close img and free memory space\n",
    "#     except (IOError, SyntaxError) as e:\n",
    "#         print('Bad file:', imageFolder)\n",
    "\n",
    "# listdir('/nnimages2')\n",
    "\n",
    "# root='C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images2/'\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot  as plt\n",
    "# from PIL import Image\n",
    "# from pathlib import Path\n",
    "# import imagesize\n",
    "# import numpy as np\n",
    "\n",
    "# # Get the Image Resolutions\n",
    "# imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "# img_meta = {}\n",
    "# for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# # Convert it to Dataframe and compute aspect ratio\n",
    "# img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "# img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "# img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "# print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "# img_meta_df.head()\n",
    "\n",
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)\n",
    "\n",
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #create function for organizing API call\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['itemId', 'title', 'galleryURL', \n",
    "#         'viewItemURL', 'autoPay', 'postalCode', \n",
    "#         'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "#         'returnsAccepted', 'condition', 'topRatedListing',\n",
    "#         'galleryPlusPictureURL', 'subtitle', 'discountPriceInfo',\n",
    "#         'secondaryCategory']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         # Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # def prepare_df(df):\n",
    "# #     price_list = []\n",
    "# #     for row in full_dataset:\n",
    "# #         listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "# #         price_list.append(listed_price)\n",
    "\n",
    "# #     df['price_in_US'] = price_list\n",
    "# #     #pull shipping cost from json dict with regex \n",
    "# #     df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "# #     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "# #     df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "# #     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "# #     #create new feature 'converted price'\n",
    "# #     df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "# #     df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "# #     df.reset_index(drop=True, inplace=True)\n",
    "# #     display(df.head())\n",
    "# #     display(df.info())\n",
    "# #     return df\n",
    "\n",
    "# def prepare_df(df):\n",
    "#     price_list = []\n",
    "#     ship_price_list = []\n",
    "#     condition_list = []\n",
    "#     for row in full_dataset:\n",
    "#         listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#         price_list.append(listed_price)\n",
    "\n",
    "#         try:\n",
    "#             listed_ship_price = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "#             ship_price_list.append(listed_ship_price)\n",
    "#         except: \n",
    "#             listed_ship_price = 0\n",
    "#             ship_price_list.append(listed_ship_price)\n",
    "\n",
    "#         try:\n",
    "#             condition = None\n",
    "#             condition = np.float(row['condition']['conditionId'])\n",
    "#             condition_list.append(condition)\n",
    "#         except: \n",
    "#             conditon = 0\n",
    "#             condition_list.append(condition)\n",
    "\n",
    "#     df['shipping_cost'] = ship_price_list\n",
    "#     df['price_in_US'] = price_list\n",
    "#     df['condition'] = condition_list\n",
    "    \n",
    "#     #create new feature 'converted price'\n",
    "#     df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#     df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     display(df.head())\n",
    "#     display(df.info())\n",
    "#     return df\n",
    "\n",
    "# overhead_cost = 3\n",
    "# def prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "#     df.title = df.title.apply(str.lower)\n",
    "\n",
    "#     #remove special characters\n",
    "#     df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "#     df[str(list(bucket_dict.keys())[bucket_dict_position])] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    \n",
    "#     df['profit'] = (df['converted_price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "#     df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 1000)\n",
    "\n",
    "# from base64 import b64encode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #this function was used at first to check accuracy of calls using ebay API\n",
    "# #the less complex \"prepare_brands\" function seems sufficient enough\n",
    "# #it is redudant to use regex with working ebay API call\n",
    "\n",
    "# def modified_prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "#     df.title = df.title.apply(str.lower)\n",
    "\n",
    "#     #remove special characters\n",
    "#     df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "#     pattern = re.compile(list(bucket_dict.keys())[bucket_dict_position])\n",
    "#     df[list(bucket_dict.keys())[bucket_dict_position]] = df_bench.title.apply(lambda x: re.sub(pattern, x, str(list(bucket_dict.values())[bucket_dict_position])))\n",
    "\n",
    "#     df[list(bucket_dict.keys())[bucket_dict_position]] = df[list(bucket_dict.keys())[bucket_dict_position]].apply(lambda x: np.float(x))\n",
    "    \n",
    "#     df['profit'] = (df['converted_price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "#     df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "\n",
    "#     return df\n",
    "\n",
    "# request = {\n",
    "#            'categoryId': 48818,\n",
    "#             'itemFilter': [\n",
    "#                             {'name': 'Condition', 'value': 'Used'},\n",
    "#                             {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                           ],\n",
    "#             'aspectFilter': [\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Benchmade'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Buck'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Case'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'CRKT'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Leatherman'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Spyderco'},\n",
    "#                              ],\n",
    "#             'paginationInput': {\n",
    "#                                 'entriesPerPage': 100,\n",
    "#                                 'pageNumber': 1\n",
    "\n",
    "#                                 }}\n",
    "\n",
    "# # Create an empty list for the full prepared dataset\n",
    "# full_dataset = []\n",
    "# #debugger error at page 23\n",
    "\n",
    "\n",
    "# for page in range(1, 22):\n",
    "#     # Add or update the \"offset\" key-value pair in url_params\n",
    "#     request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "#     # Make the query and get the response\n",
    "\n",
    "#     api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "#     request = {\n",
    "#                 'categoryId': 48818,\n",
    "#                 'itemFilter': [\n",
    "# #                                 {'name': 'Condition', 'value': 'Used'},\n",
    "#                                 {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                               ],\n",
    "#                 'aspectFilter': [\n",
    "#                                   {'aspectName': 'Brand', 'aspectValueName': 'Benchmade'}],\n",
    "\n",
    "                    \n",
    "#                 'paginationInput': {\n",
    "#                                     'entriesPerPage': 100,\n",
    "#                                     'pageNumber': page\n",
    "                    \n",
    "#                                     },\n",
    "               \n",
    "#                 }\n",
    "\n",
    "#     response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "\n",
    "# # Check the length of the full dataset. It will be up to `total`,\n",
    "# # potentially less if there were missing values\n",
    "# display(len(full_dataset))\n",
    "\n",
    "\n",
    "# df_bench = pd.DataFrame(full_dataset)\n",
    "\n",
    "# bucket_dict = {'benchmade': 45.0,\n",
    "#                'buck': 20.0,\n",
    "#                'case': 20.0,\n",
    "#                'crkt': 15.0,\n",
    "#                'kershaw': 15.0,\n",
    "#                'leatherman': 30.0, \n",
    "#                'spyderco': 30.0,\n",
    "#                'victorinox': 20.0\n",
    "#               }\n",
    "\n",
    "# df_bench = prepare_df(df_bench)\n",
    "\n",
    "# df_bench = prepare_brands(df_bench, 0)\n",
    "\n",
    "# df_bench['benchmade'].value_counts()\n",
    "\n",
    "# df_bench.info()\n",
    "\n",
    "# df_bench[df_bench['galleryPlusPictureURL'].notna()]['galleryPlusPictureURL'].sample(30).apply(print)\n",
    "\n",
    "# df_bench['profit'].describe()\n",
    "\n",
    "# df_bench['ROI'].describe()\n",
    "\n",
    "# df_bench.to_csv('data/df_bench.csv', index=False)\n",
    "\n",
    "# ### Domain Understading: Cost Breakdown\n",
    "# - padded envelopes: \\$0.50 per knife\n",
    "# - flatrate shipping: \\$4.45 per knife\n",
    "# - brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "# - overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $7\n",
    "\n",
    "# # Create an empty list for the full prepared dataset\n",
    "# full_dataset = []\n",
    "# #debugger error at page 42\n",
    "\n",
    "\n",
    "# for page in range(1, 41):\n",
    "#     # Add or update the \"offset\" key-value pair in url_params\n",
    "#     request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "#     # Make the query and get the response\n",
    "\n",
    "#     api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "#     request = {\n",
    "#                 'keywords': 'knife',\n",
    "#                 'itemFilter': [\n",
    "#                                 {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                               ],\n",
    "#                 'aspectFilter': [\n",
    "#                                  {'aspectName': 'Brand', 'aspectValueName': 'Buck'}],\n",
    "\n",
    "                    \n",
    "                    \n",
    "#                 'paginationInput': {\n",
    "#                                     'entriesPerPage': 100,\n",
    "#                                     'pageNumber': page\n",
    "                    \n",
    "#                                     },\n",
    "               \n",
    "#                 }\n",
    "\n",
    "#     response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "\n",
    "# # Check the length of the full dataset. It will be up to `total`,\n",
    "# # potentially less if there were missing values\n",
    "# display(len(full_dataset))\n",
    "\n",
    "\n",
    "# df_buck = pd.DataFrame(full_dataset)\n",
    "\n",
    "# df_buck = prepare_df(df_buck)\n",
    "\n",
    "# df_buck = prepare_brands(df_buck, 1)\n",
    "\n",
    "# df_buck.info()\n",
    "\n",
    "# df_buck['buck'].value_counts()\n",
    "\n",
    "# df_buck['profit'].describe()\n",
    "\n",
    "# df_buck['ROI'].describe()\n",
    "\n",
    "# df_buck.to_csv('data/df_buck.csv', index=False)\n",
    "\n",
    "# # Create an empty list for the full prepared dataset\n",
    "# full_dataset = []\n",
    "\n",
    "\n",
    "\n",
    "# for page in range(1, 100):\n",
    "#     # Add or update the \"offset\" key-value pair in url_params\n",
    "#     request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "#     # Make the query and get the response\n",
    "\n",
    "#     api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "#     request = {\n",
    "#                 'keywords': 'knife',\n",
    "#                 'itemFilter': [\n",
    "#                                 {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                               ],\n",
    "#                 'aspectFilter': [\n",
    "# #                               \n",
    "#                                    {'aspectName': 'Brand', 'aspectValueName': 'Case XX'},\n",
    "#                                    {'aspectName': 'Brand', 'aspectValueName': 'Case'}\n",
    "#                                  ],\n",
    "\n",
    "                    \n",
    "                    \n",
    "#                 'paginationInput': {\n",
    "#                                     'entriesPerPage': 100,\n",
    "#                                     'pageNumber': page\n",
    "                    \n",
    "#                                     },\n",
    "               \n",
    "#                 }\n",
    "\n",
    "#     response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "\n",
    "# # Check the length of the full dataset. It will be up to `total`,\n",
    "# # potentially less if there were missing values\n",
    "# display(len(full_dataset))\n",
    "\n",
    "\n",
    "# df_case = pd.DataFrame(full_dataset)\n",
    "\n",
    "# df_case = prepare_df(df_case)\n",
    "\n",
    "# df_case = prepare_brands(df_case, 2)\n",
    "\n",
    "# df_case.info()\n",
    "\n",
    "# df_case.to_csv('data/df_case.csv', index=False)\n",
    "\n",
    "# # Create an empty list for the full prepared dataset\n",
    "# full_dataset = []\n",
    "# #debug error page 18\n",
    "\n",
    "\n",
    "# for page in range(1, 17):\n",
    "#     # Add or update the \"offset\" key-value pair in url_params\n",
    "#     request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "#     # Make the query and get the response\n",
    "\n",
    "#     api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "#     request = {\n",
    "#                 'keywords': 'knife',\n",
    "#                 'itemFilter': [\n",
    "#                                 {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                               ],\n",
    "#                 'aspectFilter': [\n",
    "\n",
    "#                                     {'aspectName': 'Brand', 'aspectValueName': 'CRKT'}],\n",
    "                 \n",
    "                    \n",
    "#                 'paginationInput': {\n",
    "#                                     'entriesPerPage': 100,\n",
    "#                                     'pageNumber': page\n",
    "                    \n",
    "#                                     },\n",
    "               \n",
    "#                 }\n",
    "\n",
    "#     response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "\n",
    "# # Check the length of the full dataset. It will be up to `total`,\n",
    "# # potentially less if there were missing values\n",
    "# display(len(full_dataset))\n",
    "\n",
    "\n",
    "# df_crkt = pd.DataFrame(full_dataset)\n",
    "\n",
    "# df_crkt = prepare_df(df_crkt)\n",
    "\n",
    "# bucket_dict \n",
    "\n",
    "# df_crkt = prepare_brands(df_crkt, 3)\n",
    "\n",
    "# display(df_crkt.head())\n",
    "# display(df_crkt.info())\n",
    "\n",
    "# df_crkt.to_csv('data/df_crkt.csv', index=False)\n",
    "\n",
    "# # Create an empty list for the full prepared dataset\n",
    "# full_dataset = []\n",
    "# #debugger threw error at page 70\n",
    "\n",
    "\n",
    "# for page in range(1, 69):\n",
    "#     # Add or update the \"offset\" key-value pair in url_params\n",
    "#     request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "#     # Make the query and get the response\n",
    "\n",
    "#     api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "#     request = {\n",
    "#                 'keywords': 'knife',\n",
    "#                 'itemFilter': [\n",
    "#                                 {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                               ],\n",
    "#                 'aspectFilter': [\n",
    "\n",
    "#                                   {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'}\n",
    "#                                 ],\n",
    "\n",
    "#                 'paginationInput': {\n",
    "#                                     'entriesPerPage': 100,\n",
    "#                                     'pageNumber': page\n",
    "                    \n",
    "#                                     },\n",
    "               \n",
    "#                 }\n",
    "\n",
    "#     response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "\n",
    "# # Check the length of the full dataset. It will be up to `total`,\n",
    "# # potentially less if there were missing values\n",
    "# display(len(full_dataset))\n",
    "\n",
    "\n",
    "# df_kershaw = pd.DataFrame(full_dataset)\n",
    "\n",
    "# bucket_dict\n",
    "\n",
    "# df_kershaw = prepare_df(df_kershaw)\n",
    "\n",
    "# df_kershaw = prepare_brands(df_kershaw, 4)\n",
    "\n",
    "# df_kershaw.info()\n",
    "\n",
    "# df_kershaw.to_csv('data/df_kershaw.csv', index=False)\n",
    "\n",
    "# # Create an empty list for the full prepared dataset\n",
    "# full_dataset = []\n",
    "# #debug error page 18\n",
    "\n",
    "\n",
    "# for page in range(1, 17):\n",
    "#     # Add or update the \"offset\" key-value pair in url_params\n",
    "#     request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "#     # Make the query and get the response\n",
    "\n",
    "#     api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "#     request = {\n",
    "#                 'keywords': 'knife',\n",
    "#                 'itemFilter': [\n",
    "#                                 {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                               ],\n",
    "#                 'aspectFilter': [\n",
    "\n",
    "#                                    {'aspectName': 'Brand', 'aspectValueName': 'Leatherman'}],\n",
    "                    \n",
    "                    \n",
    "#                 'paginationInput': {\n",
    "#                                     'entriesPerPage': 100,\n",
    "#                                     'pageNumber': page\n",
    "                    \n",
    "#                                     },\n",
    "               \n",
    "#                 }\n",
    "\n",
    "#     response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "\n",
    "# # Check the length of the full dataset. It will be up to `total`,\n",
    "# # potentially less if there were missing values\n",
    "# display(len(full_dataset))\n",
    "\n",
    "\n",
    "# df_leatherman = pd.DataFrame(full_dataset)\n",
    "\n",
    "# bucket_dict\n",
    "\n",
    "# df_leatherman = prepare_df(df_leatherman)\n",
    "\n",
    "# df_leatherman = prepare_brands(df_leatherman, 5)\n",
    "\n",
    "# df_leatherman.info()\n",
    "\n",
    "# df_leatherman.to_csv('data/df_leatherman.csv', index=False)\n",
    "\n",
    "# # Create an empty list for the full prepared dataset\n",
    "# full_dataset = []\n",
    "# #debug error page 72\n",
    "\n",
    "# for page in range(1, 71):\n",
    "#     # Add or update the \"offset\" key-value pair in url_params\n",
    "#     request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "#     # Make the query and get the response\n",
    "\n",
    "#     api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "#     request = {\n",
    "#                 'keywords': 'knife',\n",
    "#                 'itemFilter': [\n",
    "#                                 {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                               ],\n",
    "#                 'aspectFilter': [\n",
    "\n",
    "#                                    {'aspectName': 'Brand', 'aspectValueName': 'Spyderco'}\n",
    "#                               ],\n",
    "                    \n",
    "                    \n",
    "#                 'paginationInput': {\n",
    "#                                     'entriesPerPage': 100,\n",
    "#                                     'pageNumber': page\n",
    "                    \n",
    "#                                     },\n",
    "               \n",
    "#                 }\n",
    "\n",
    "#     response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "\n",
    "# # Check the length of the full dataset. It will be up to `total`,\n",
    "# # potentially less if there were missing values\n",
    "# display(len(full_dataset))\n",
    "\n",
    "# df_spyderco = pd.DataFrame(full_dataset)\n",
    "\n",
    "# df_spyderco = prepare_df(df_spyderco)\n",
    "\n",
    "# bucket_dict \n",
    "\n",
    "# df_spyderco = prepare_brands(df_spyderco, 6)\n",
    "\n",
    "# df_spyderco.info()\n",
    "\n",
    "# df_spyderco.to_csv('data/df_spyderco.csv', index=False)\n",
    "\n",
    "# # Create an empty list for the full prepared dataset\n",
    "# full_dataset = []\n",
    "\n",
    "\n",
    "# for page in range(1, 21):\n",
    "#     # Add or update the \"offset\" key-value pair in url_params\n",
    "#     request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "#     # Make the query and get the response\n",
    "\n",
    "#     api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "#     request = {\n",
    "#                 'keywords': 'knife',\n",
    "#                 'itemFilter': [\n",
    "#                                 {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                               ],\n",
    "#                 'aspectFilter': [\n",
    "\n",
    "#                                    {'aspectName': 'Brand', 'aspectValueName': 'SOG'}],\n",
    "\n",
    "                    \n",
    "#                 'paginationInput': {\n",
    "#                                     'entriesPerPage': 100,\n",
    "#                                     'pageNumber': page\n",
    "                    \n",
    "#                                     },\n",
    "               \n",
    "#                 }\n",
    "\n",
    "#     response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "\n",
    "# # Check the length of the full dataset. It will be up to `total`,\n",
    "# # potentially less if there were missing values\n",
    "# display(len(full_dataset))\n",
    "\n",
    "\n",
    "# df_sog = pd.DataFrame(full_dataset)\n",
    "\n",
    "# df_sog = prepare_df(df_sog)\n",
    "\n",
    "# bucket_dict['sog'] = 15.0\n",
    "\n",
    "# df_sog = prepare_brands(df_sog, 8)\n",
    "\n",
    "# df_sog.info()\n",
    "\n",
    "# df_sog.to_csv('data/df_sog.csv', index=False)\n",
    "\n",
    "# # Create an empty list for the full prepared dataset\n",
    "# full_dataset = []\n",
    "\n",
    "\n",
    "# for page in range(1, 100):\n",
    "#     # Add or update the \"offset\" key-value pair in url_params\n",
    "#     request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "#     # Make the query and get the response\n",
    "\n",
    "#     api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "#     request = {\n",
    "#                 'keywords': 'knife',\n",
    "#                 'itemFilter': [\n",
    "#                                 {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                               ],\n",
    "#                 'aspectFilter': [\n",
    "\n",
    "#                                    {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "                    \n",
    "#                 'paginationInput': {\n",
    "#                                     'entriesPerPage': 100,\n",
    "#                                     'pageNumber': page\n",
    "                    \n",
    "#                                     },\n",
    "               \n",
    "#                 }\n",
    "\n",
    "#     response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "\n",
    "# # Check the length of the full dataset. It will be up to `total`,\n",
    "# # potentially less if there were missing values\n",
    "# display(len(full_dataset))\n",
    "\n",
    "\n",
    "# df_victorinox = pd.DataFrame(full_dataset)\n",
    "\n",
    "# df_victorinox = prepare_df(df_victorinox)\n",
    "\n",
    "# bucket_dict\n",
    "\n",
    "# df_victorinox = prepare_brands(df_victorinox, 7)\n",
    "\n",
    "# df_victorinox.info()\n",
    "\n",
    "# df_victorinox.to_csv('data/df_victorinox.csv', index=False)\n",
    "\n",
    "# # df_bench = pd.read_csv(\"data/df_bench.csv\")\n",
    "# # df_buck = pd.read_csv(\"data/df_buck.csv\")\n",
    "# # df_case = pd.read_csv(\"data/df_case.csv\")\n",
    "# # df_crkt = pd.read_csv(\"data/df_crkt.csv\")\n",
    "# # df_kershaw = pd.read_csv(\"data/df_kershaw.csv\")\n",
    "# # df_leatherman = pd.read_csv(\"data/df_leatherman.csv\")\n",
    "# # df_spyderco = pd.read_csv(\"data/df_spyderco.csv\")\n",
    "# # df_victorinox = pd.read_csv(\"data/df_victorinox.csv\")\n",
    "\n",
    "# df = pd.concat([df_bench,df_buck,df_case,df_crkt,df_kershaw,df_leatherman,df_spyderco,df_sog,df_victorinox])\n",
    "\n",
    "# df.info()\n",
    "\n",
    "# df.isna().sum()\n",
    "\n",
    "# df.to_csv(\"data/full_dataset2.csv\", index=False)\n",
    "\n",
    "# After succesfully going through 10,000 items on ebay's website and extracting everything possible, there is still a little bit more extracting to do from the json dictionary before saving the dataframe again. We need to get the price of the knives out of the nested dictionary in the dataframe as well as the shipping cost. After that, I would like to create a new feature called\n",
    "# **\"converted price,\" which is simply the price of the knife listed on the ebay's website plus shipping.**\n",
    "\n",
    "# ```\n",
    "# #Create row for converted Price of Knives in US dollars\n",
    "# price_list = []\n",
    "# for row in full_dataset:\n",
    "#     listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#     price_list.append(listed_price)\n",
    "    \n",
    "# df['price_in_US'] = price_list\n",
    "# ```\n",
    "\n",
    "# ```\n",
    "# #atttempt to pull shipping cost from json dict\n",
    "# shipping_cost_list = []\n",
    "# for row in full_dataset:\n",
    "#     shipping_cost = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "#     shipping_cost_list.append(shipping_cost)\n",
    "    \n",
    "# df['shipping_price'] = shipping_cost_list\n",
    "# ```\n",
    "\n",
    "# ```\n",
    "# #pull shipping cost from json dict with regex \n",
    "# df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "# df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "# df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "# df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "# #create new feature 'converted price'\n",
    "# df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "# df = df.drop_duplicates(subset=['title', 'galleryURL'], keep='first')\n",
    "# display(df.head())\n",
    "# display(df.info())\n",
    "\n",
    "# df.to_csv('data/full_dataset.csv', index=False)\n",
    "# ```\n",
    "\n",
    "# ## Data Preparation\n",
    "\n",
    "# Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * Were there variables you dropped or created?\n",
    "# * How did you address missing values or outliers?\n",
    "# * Why are these choices appropriate given the data and the business problem?\n",
    "# ***\n",
    "\n",
    "\n",
    "# # here you run your code to clean the data\n",
    "\n",
    "# ```\n",
    "# import code.data_cleaning as dc\n",
    "\n",
    "# full_dataset = dc.full_clean()\n",
    "# ```\n",
    "\n",
    "# ## Data Modeling\n",
    "# Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * How did you analyze or model the data?\n",
    "# * How did you iterate on your initial approach to make it better?\n",
    "# * Why are these choices appropriate given the data and the business problem?\n",
    "# ***\n",
    "# # here you run your code to model the data\n",
    "\n",
    "\n",
    "# ## Evaluation\n",
    "# Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * How do you interpret the results?\n",
    "# * How well does your model fit your data? How much better is this than your baseline model?\n",
    "# * How confident are you that your results would generalize beyond the data you have?\n",
    "# * How confident are you that this model would benefit the business if put into use?\n",
    "# ***\n",
    "\n",
    "\n",
    "# ## Conclusions\n",
    "# Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * What would you recommend the business do as a result of this work?\n",
    "# * What are some reasons why your analysis might not fully solve the business problem?\n",
    "# * What else could you do in the future to improve this project?\n",
    "# ***\n",
    "\n",
    "\n",
    "\n",
    "# # data_knife_dir = 'knife_images'\n",
    "# # data_profit_dir = 'data/profit'\n",
    "# # new_dir = 'split'\n",
    "\n",
    "# # os.mkdir(new_dir)\n",
    "\n",
    "# # train_folder = os.path.join(new_dir, 'train')\n",
    "# # train_profit = os.path.join(train_folder, 'profit')\n",
    "# # os.mkdir(train_folder)\n",
    "# # os.mkdir(train_profit)\n",
    "\n",
    "# # test_folder = os.path.join(new_dir, 'test')\n",
    "# # test_profit = os.path.join(test_folder, 'profit')\n",
    "# # os.mkdir(test_folder)\n",
    "# # os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# # val_folder = os.path.join(new_dir, 'validation')\n",
    "# # val_profit = os.path.join(val_folder, 'profit')\n",
    "# # os.mkdir(val_folder)\n",
    "# # os.mkdir(val_profit)\n",
    "\n",
    "# # val_profit\n",
    "\n",
    "# # # train knife regression images\n",
    "# # #80% of data\n",
    "# # imgs = knife_images[:5620]\n",
    "# # for img in imgs:\n",
    "# #     origin = os.path.join(data_knife_dir, img)\n",
    "# #     destination = os.path.join(train_profit, img)\n",
    "# #     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # # test knife regression images\n",
    "# # #10% of data\n",
    "# # imgs = knife_images[5620:6322]\n",
    "# # for img in imgs:\n",
    "# #     origin = os.path.join(data_knife_dir, img)\n",
    "# #     destination = os.path.join(test_profit, img)\n",
    "# #     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # # validation knife regression images\n",
    "# # #10% of data\n",
    "# # imgs = knife_images[6322:]\n",
    "# # for img in imgs:\n",
    "# #     origin = os.path.join(data_knife_dir, img)\n",
    "# #     destination = os.path.join(val, img)\n",
    "# #     shutil.copyfile(origin, destination)\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from keras.models import load_model\n",
    "# from keras.preprocessing import image\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "# img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "# img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "# plt.imshow(img_rgb)  # graph it\n",
    "# plt.show();\n",
    "\n",
    "\n",
    "# def image_checker(index,):\n",
    "#     img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "#     img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "#     plt.imshow(img_rgb)  # graph it\n",
    "#     plt.show();\n",
    "\n",
    "# top_benchmade_index[:50]\n",
    "\n",
    "# image_checker(6158)\n",
    "\n",
    "# image_checker(2286)\n",
    "\n",
    "# image_checker(1879)\n",
    "\n",
    "# image_checker(4326)\n",
    "\n",
    "# image_checker(6094)\n",
    "\n",
    "# #final processing steps for images\n",
    "\n",
    "# image_list = []\n",
    "# for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "#     img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "#     img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "#     img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "#     image_list.append(img_rgb)\n",
    "   \n",
    "#     # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n",
    "\n",
    "\n",
    "# df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())\n",
    "\n",
    "# df_CNN_regression['mean_profit'].describe()\n",
    "\n",
    "# X = np.array(image_list)\n",
    "\n",
    "# y=  df_CNN_regression['mean_profit']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets\n",
    "\n",
    "# X.shape\n",
    "\n",
    "# y.shape\n",
    "\n",
    "# print(\"Xtrain:\", X_train.shape)\n",
    "# print(\"y_train:\", y_train.shape)\n",
    "# print(\"X_test:\", X_test.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n",
    "\n",
    "# X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# print(\"Xtrain:\", X_train.shape)\n",
    "# print(\"y_train:\", y_train.shape)\n",
    "# print(\"X_test:\", X_test.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n",
    "# print(\"X_val:\", X_test.shape)\n",
    "# print(\"y_val:\", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #small batch\n",
    "\n",
    "# # model = models.Sequential()\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "# #                         input_shape=(256 ,256, 3)))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# # model.add(Dense(256, activation='relu'))\n",
    "# # model.add(Dense(128, activation='relu'))\n",
    "# # model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# # model.compile(loss='mean_squared_error',\n",
    "# #               optimizer='Adam',\n",
    "# #                metrics=['mse'])\n",
    "\n",
    "# # history = model.fit(X_train,\n",
    "# #                     y_train,\n",
    "# #                     epochs=30,\n",
    "# #                     batch_size=32,\n",
    "# #                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "# results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "# #model.summary()\n",
    "\n",
    "\n",
    "# df_scrub['profit'].mean()\n",
    "\n",
    "# 2.2164 * 41.374303202846974\n",
    "\n",
    "# df_scrub.head()\n",
    "\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# #The model learned patterns wells until epoch 20\n",
    "# #after that the loss spikes signifcantly before dropping again\n",
    "# fig = plt.figure(figsize=(12,8))\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.plot\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss( mean square error)')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "# plt.show();\n",
    "\n",
    "# model.save('my_model_batch32.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #a train set of 60% and a val and test size of 20% each \n",
    "\n",
    "# #this model showed a lot of indication that it was overfit\n",
    "# #need to retry how I split the data \n",
    "# #Instead of manul indexing, will use \n",
    "# # from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# # X_train = X[:4918]\n",
    "# # y_train = y[:4918]\n",
    "\n",
    "# # X_train = X[4918:5971]\n",
    "# # y_train = y[4918:5971]\n",
    "\n",
    "# # X_test = X[5971:]\n",
    "# # y_test = y[5971:]\n",
    "\n",
    "\n",
    "# # display(len(X_val)/len(X))\n",
    "# # display(len(X_train)/len(X))\n",
    "# # len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# # model = models.Sequential()\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "# #                         input_shape=(224 ,224,  3)))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Flatten())\n",
    "\n",
    "# # model.add(Dense(512, activation='relu'))\n",
    "# # model.add(Dropout(0.1))\n",
    "\n",
    "# # model.add(Dense(256, activation='relu'))\n",
    "# # model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# # model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# # model.compile(loss='mean_squared_error',\n",
    "# #               optimizer='Adam',\n",
    "# #                metrics=['mse'])\n",
    "# # history = model.fit(X_train,\n",
    "# #                     y_train,\n",
    "# #                     epochs=32,\n",
    "# #                     batch_size=300,\n",
    "# #                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "# #model.summary()\n",
    "\n",
    "\n",
    "# # model.save('my_model_batch500.h5')\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# # model.save('my_model_batch500.h5')\n",
    "\n",
    "# history.history.keys()\n",
    "\n",
    "# #The model is showing a lot of signs of overfitting \n",
    "# fig = plt.figure(figsize=(12,8))\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.plot\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss( mean square error)')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "# plt.show();\n",
    "\n",
    "# X_train.shape\n",
    "\n",
    "# # results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "# #model.summary()\n",
    "\n",
    "\n",
    "# # model.save('my_model_batch500.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket_dict = {'benchmade': 45.0,\n",
    "#                'buck': 20.0,\n",
    "#                'case': 20.0,\n",
    "#                'crkt': 15.0,\n",
    "#                'kershaw': 15.0,\n",
    "#                'leatherman': 30.0,\n",
    "#                'sog': 15.0,\n",
    "#                'spyderco': 30.0,\n",
    "#                'victorinox': 20.0\n",
    "#               }\n",
    "\n",
    "# overhead_cost = 3\n",
    "# def prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "#     df.title = df.title.apply(str.lower)\n",
    " \n",
    "#     #remove special characters\n",
    "# #     df.title.apply(pp.remove_special_chars)\n",
    "#     df['brand'] = str(list(bucket_dict.keys())[bucket_dict_position])\n",
    "#     df['cost'] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "#     df['profit'] = (df['converted_price'] -  df['cost'] - overhead_cost)\n",
    "#     df['ROI'] = (df['profit']/( df['cost'] + overhead_cost))*100.0\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['itemId', 'title', 'galleryURL', \n",
    "#         'viewItemURL', 'autoPay', 'postalCode', \n",
    "#         'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "#         'returnsAccepted', 'condition', 'topRatedListing',\n",
    "#         'galleryPlusPictureURL']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         # Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def knife_request(Brand, dict_pos, Lots=False):\n",
    "#     api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "#     full_dataset = []\n",
    "#     price_list = []\n",
    "#     ship_price_list = []\n",
    "#     condition_list = []\n",
    "#     condition = None\n",
    "#     if Lots == True:\n",
    "#         request = {\n",
    "#                     'categoryId': 48818,\n",
    "#                     'itemFilter': [\n",
    "#                                     {'name': 'LotsOnly', 'value': 'True'},\n",
    "#                                     {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                                   ],\n",
    "#                     'aspectFilter': [\n",
    "#                                       {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "\n",
    "#                     'paginationInput': {\n",
    "#                                         'entriesPerPage': 100,\n",
    "#                                         'pageNumber': 1\n",
    "\n",
    "#                                         },\n",
    "\n",
    "#                     }\n",
    "\n",
    "\n",
    "#         response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "\n",
    "#         response_pages = response.dict()\n",
    "\n",
    "    \n",
    "#         total_pages = int(response_pages['paginationOutput']['totalPages'])\n",
    "        \n",
    "#         if total_pages > 100:\n",
    "#             pages_to_request = 100\n",
    "        \n",
    "#         else:\n",
    "#             pages_to_request = total_pages - 1\n",
    "\n",
    "\n",
    "#         for page in range(1, pages_to_request):\n",
    "#         # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "#         # Make the query and get the response\n",
    "\n",
    "#             api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "#             request = {\n",
    "#                         'categoryId': 48818,\n",
    "#                         'itemFilter': [\n",
    "#                                         {'name': 'LotsOnly', 'value': 'True'},\n",
    "#                                         {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                                       ],\n",
    "#                         'aspectFilter': [\n",
    "#                                           {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "\n",
    "#                         'paginationInput': {\n",
    "#                                             'entriesPerPage': 100,\n",
    "#                                             'pageNumber': page\n",
    "\n",
    "#                                             },\n",
    "\n",
    "#                         }\n",
    "\n",
    "\n",
    "#             response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#             #save the response as a json dict\n",
    "#             response_dict = response.dict()\n",
    "\n",
    "\n",
    "#             #index dict to appropriate index\n",
    "#             results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#             # Call the prepare_data function to get a list of processed data\n",
    "#             prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#             # Extend full_dataset with this list (don't append, or you'll get\n",
    "#             # a list of lists instead of a flat list)\n",
    "#             full_dataset.extend(prepared_knives)\n",
    "            \n",
    "#         display(len(full_dataset))\n",
    "    \n",
    "#         df = pd.DataFrame(full_dataset)\n",
    "        \n",
    "#         for row in full_dataset:\n",
    "#             try:\n",
    "#                 listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#                 price_list.append(listed_price)\n",
    "#             except:\n",
    "#                 listed_price = \"Na\"\n",
    "#                 price_list.append(listed_price)\n",
    "#             try:\n",
    "#                 listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "#                 ship_price_list.append(listed_ship_price)\n",
    "#             except: \n",
    "#                 listed_ship_price = 0\n",
    "#                 ship_price_list.append(listed_ship_price)\n",
    "#             try:\n",
    "#                 condition = float(row['condition']['conditionId'])\n",
    "#                 condition_list.append(condition)\n",
    "#             except: \n",
    "#                 conditon = 0\n",
    "#                 condition_list.append(condition)\n",
    "\n",
    "#         df['shipping_cost'] = ship_price_list\n",
    "#         df['price_in_US'] = price_list\n",
    "#         df['condition'] = condition_list\n",
    "    \n",
    "#     #create new feature 'converted price'\n",
    "#         df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#         df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#         df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#         df = prepare_brands(df, dict_pos)\n",
    "        \n",
    "        \n",
    "#     else: \n",
    "#         request = {\n",
    "#                     'categoryId': 48818,\n",
    "#                     'itemFilter': [\n",
    "#                                     {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                                   ],\n",
    "#                     'aspectFilter': [\n",
    "#                                       {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "\n",
    "#                     'paginationInput': {\n",
    "#                                         'entriesPerPage': 100,\n",
    "#                                         'pageNumber': 1\n",
    "\n",
    "#                                         },\n",
    "\n",
    "#                     }\n",
    "\n",
    "\n",
    "#         response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "\n",
    "#         response_pages = response.dict()\n",
    "\n",
    "    \n",
    "#         total_pages = int(response_pages['paginationOutput']['totalPages'])\n",
    "#         if total_pages > 100:\n",
    "#             pages_to_request = 100\n",
    "        \n",
    "#         else:\n",
    "#             pages_to_request = total_pages - 1\n",
    "        \n",
    "        \n",
    "#         for page in range(1, pages_to_request):\n",
    "#         # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "#         # Make the query and get the response\n",
    "\n",
    "#             api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "#             request = {\n",
    "#                         'categoryId': 48818,\n",
    "#                         'itemFilter': [\n",
    "#                                         {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                                       ],\n",
    "#                         'aspectFilter': [\n",
    "#                                           {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "\n",
    "#                         'paginationInput': {\n",
    "#                                             'entriesPerPage': 100,\n",
    "#                                             'pageNumber': page\n",
    "\n",
    "#                                             },\n",
    "\n",
    "#                         }\n",
    "\n",
    "\n",
    "#             response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#             #save the response as a json dict\n",
    "#             response_dict = response.dict()\n",
    "\n",
    "\n",
    "#             #index dict to appropriate index\n",
    "#             results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "#             # Call the prepare_data function to get a list of processed data\n",
    "#             prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#             # Extend full_dataset with this list (don't append, or you'll get\n",
    "#             # a list of lists instead of a flat list)\n",
    "#         full_dataset.extend(prepared_knives)\n",
    "#         display(len(full_dataset))\n",
    "    \n",
    "#         df = pd.DataFrame(full_dataset)\n",
    "        \n",
    "#         for row in full_dataset:\n",
    "#             try:\n",
    "#                 listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#                 price_list.append(listed_price)\n",
    "#             except:\n",
    "#                 listed_price = \"Na\"\n",
    "#                 price_list.append(listed_price)\n",
    "#             try:\n",
    "#                 listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "#                 ship_price_list.append(listed_ship_price)\n",
    "#             except: \n",
    "#                 listed_ship_price = 0\n",
    "#                 ship_price_list.append(listed_ship_price)\n",
    "#             try:\n",
    "#                 condition = float(row['condition']['conditionId'])\n",
    "#                 condition_list.append(condition)\n",
    "#             except: \n",
    "#                 conditon = 0\n",
    "#                 condition_list.append(condition)\n",
    "\n",
    "#         df['shipping_cost'] = ship_price_list\n",
    "#         df['price_in_US'] = price_list\n",
    "#         df['condition'] = condition_list\n",
    "    \n",
    "#     #create new feature 'converted price'\n",
    "#         df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#         df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#         df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#         df = prepare_brands(df, dict_pos)\n",
    "        \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
