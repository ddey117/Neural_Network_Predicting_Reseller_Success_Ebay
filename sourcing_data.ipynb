{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.finding import Connection\n",
    "import requests\n",
    "from ebaysdk.shopping import Connection as Shopping\n",
    "import pandas as pd \n",
    "import  json\n",
    "import numpy as np\n",
    "import re\n",
    "# import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import ast\n",
    "\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    condition = None\n",
    "    for row in full_dataset:\n",
    "        listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "        price_list.append(listed_price)\n",
    "     \n",
    "        try:\n",
    "            listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "\n",
    "        try:\n",
    "            condition = float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0,\n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "\n",
    "def prepare_brands(df, bucket_dict_position, overhead_cost=3):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    " \n",
    "    #remove special characters\n",
    "#     df.title.apply(pp.remove_special_chars)\n",
    "    df['brand'] = str(list(bucket_dict.keys())[bucket_dict_position])\n",
    "    df['cost'] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    df['profit'] = (df['converted_price'] -  df['cost'] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/( df['cost'] + overhead_cost))*100.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "                'viewItemURL', 'autoPay', 'postalCode', \n",
    "                'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "                'returnsAccepted', 'condition', 'topRatedListing',\n",
    "                'galleryPlusPictureURL','pictureURLLarge', \n",
    "                'pictureURLSuperSize']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "\n",
    "def knife_request(Brand, dict_pos):\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': 1\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "    #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "\n",
    "    response_pages = response.dict()\n",
    "\n",
    "    full_dataset = []\n",
    "    \n",
    "    total_pages = int(response_pages['paginationOutput']['totalPages'])\n",
    "\n",
    "    if total_pages > 100:\n",
    "        pages_to_request = 100\n",
    "        \n",
    "    else:\n",
    "        pages_to_request = total_pages - 1\n",
    "        \n",
    "        \n",
    "\n",
    "    for page in range(1, pages_to_request):\n",
    "        # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "        # Make the query and get the response\n",
    "\n",
    "        api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "        request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "\n",
    "        response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "        #save the response as a json dict\n",
    "        response_dict = response.dict()\n",
    "\n",
    "\n",
    "        #index dict to appropriate index\n",
    "        results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "        # Call the prepare_data function to get a list of processed data\n",
    "        prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "        # Extend full_dataset with this list (don't append, or you'll get\n",
    "        # a list of lists instead of a flat list)\n",
    "        full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "    display(len(full_dataset))\n",
    "    \n",
    "    df = pd.DataFrame(full_dataset)\n",
    "    \n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    condition = None\n",
    "    for row in full_dataset:\n",
    "        try:\n",
    "            listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "            price_list.append(listed_price)\n",
    "        except:\n",
    "            listed_price = \"Na\"\n",
    "            price_list.append(listed_price)\n",
    "        try:\n",
    "            listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        try:\n",
    "            condition = float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df = prepare_brands(df, dict_pos)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_dataIds(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID','GalleryURL','PictureURL',\n",
    "                'Location','ConvertedCurrentPrice',\n",
    "                'Title','ItemSpecifics', \n",
    "                'Country','ConditionID']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_list(my_list):\n",
    " \n",
    "    api = Shopping(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "    request = {\n",
    "               'itemID': my_list,\n",
    "               'IncludeSelector': 'ItemSpecifics'\n",
    "              }\n",
    "    response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_dataIds(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "    return full_dataset\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "#x = position of bucket_dictionary\n",
    "def prepare_tera_df(df, x, overhead_cost=3):\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\"$\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\",\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].apply(float)\n",
    "    \n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\"$\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\",\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].apply(float)\n",
    "    \n",
    "    df['converted_price'] = (df['price_in_US'] + df['shipping_cost'])\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - list(bucket_dict.values())[x] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/(list(bucket_dict.values())[x]))*100.0\n",
    "    \n",
    "    df['brand'] = list(bucket_dict.keys())[x]\n",
    "    df['cost'] = list(bucket_dict.values())[x]\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fix(col):\n",
    "    dd = dict()\n",
    "    for d in col:\n",
    "        values = list(d.values())\n",
    "        if len(values) == 2:\n",
    "            dd[values[0]] = values[1]\n",
    "    return dd\n",
    "\n",
    "\n",
    "def transform_item_specifics(df, perc=90.0):\n",
    "\n",
    "    df.dropna(subset=['ItemSpecifics'], inplace=True)\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(lambda x: ast.literal_eval(x))\n",
    "    df['item_list'] = df['ItemSpecifics'].apply(lambda x: x['NameValueList'])\n",
    "\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(lambda x: [x['NameValueList']] if isinstance(x['NameValueList'], dict) else x['NameValueList'])\n",
    "\n",
    "    df['ItemSpecifics'] = df['ItemSpecifics'].apply(fix)\n",
    "\n",
    "    df = pd.json_normalize(df['ItemSpecifics'])\n",
    "\n",
    "    min_count =  int(((100-perc)/100)*df.shape[0] + 1)\n",
    "    mod_df = df.dropna(axis=1, \n",
    "                       thresh=min_count)\n",
    "\n",
    "    return mod_df\n",
    "\n",
    "\n",
    "def data_cleaner(df):\n",
    "    lot = re.compile('(?<!-\\S)lot(?![^\\s.,:?!])')\n",
    "    disp = re.compile('(display)')\n",
    "    box = re.compile('(box)')\n",
    "    group = re.compile('(group)')\n",
    "    is_set = re.compile('(?<!-\\S)set(?![^\\s.,?!])')\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    trim_list = [lot,disp,box,group,is_set]\n",
    "    for item in trim_list:\n",
    "        df.loc[df['title'].apply(lambda x: re.search(item, x)).notnull(), 'trim'] = 1 \n",
    "    to_drop = df.loc[df['trim'] == 1].index\n",
    "    df.drop(to_drop, inplace=True)\n",
    "    df.drop('trim', axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bench_df = knife_request('Benchmade', 0)\n",
    "# buck_df = knife_request('Buck', 1)\n",
    "# case_df = knife_request('Case', 2)\n",
    "# df_caseXX = knife_request('Case XX', 2)\n",
    "# df_crkt = knife_request(\"CRKT\", 3)\n",
    "# # df_leatherman = knife_request('Leatherman', 5)\n",
    "# df_sog = knife_request('SOG', 6)\n",
    "# df_spyderco = knife_request('Spyderco', 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of API calls for listed data. To be merged with item specific data using ebay itemIds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running functions to call the Finding API and return datasets for cat () knives for sale listed on ebay in the last 90 days. (explain how ebay rules work)\n",
    "\n",
    "```\n",
    "bench_df = knife_request('Benchmade', 0)\n",
    "buck_df = knife_request('Buck', 1)\n",
    "case_df = knife_request('Case', 2)\n",
    "df_caseXX = knife_request('Case XX', 2)\n",
    "df_crkt = knife_request(\"CRKT\", 3)\n",
    "df_leatherman = knife_request('Leatherman', 5)\n",
    "df_sog = knife_request('SOG', 6)\n",
    "df_spyderco = knife_request('Spyderco', 7)\n",
    "\n",
    "\n",
    "bench_df.to_csv('data/df_bench1.csv', index=False)\n",
    "buck_df.to_csv('data/df_buck.csv', index=False)\n",
    "case_df.to_csv('data/df_case.csv', index=False)\n",
    "df_caseXX.to_csv('data/df_CaseXX.csv', index=False)\n",
    "df_crkt.to_csv('data/df_crkt.csv', index=False)\n",
    "df_leatherman.to_csv('data/df_leatherman.csv', index=False)\n",
    "df_sog.to_csv('data/df_sog.csv', index=False)\n",
    "df_spyderco.to_csv('data/df_spyderco.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kershaw and victorinox data was requested using the FindingAPI below after tweaking some pagination through trial and error to maximize data.\n",
    "\n",
    "```\n",
    "full_dataset = []\n",
    "for page in range(1, 57):\n",
    "#         # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "#         # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "        #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "\n",
    "    df = pd.DataFrame(full_dataset)\n",
    "    \n",
    "df_kershaw = prepare_df(df)\n",
    "df_kershaw = prepare_brands(df_kershaw, 4)\n",
    "df_kershaw.to_csv('data/df_kershaw.csv', index=False)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "full_dataset = []\n",
    "for page in range(1, 86):\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "                'outputSelector': ['PictureURLLarge', 'PictureURLSuperSize'],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "df_victorinox = pd.DataFrame(full_dataset)\n",
    "df_victorinox = prepare_df(df_victorinox)\n",
    "df_victorinox = prepare_brands(df_victorinox, 8)\n",
    "df_victorinox.to_csv('data/df_victorinox.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end of API call for listed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start of API call section using IDs from preview listed datasets to get Item Specific data from ebay. This will return more descriptive information about the knives, pulling from a container on the website that sellers must complete to post a listing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = pd.read_csv(\"listed_data/df_bench.csv\")\n",
    "df_buck = pd.read_csv(\"listed_data/df_buck.csv\")\n",
    "df_case = pd.read_csv(\"listed_data/df_case.csv\")\n",
    "df_caseXX = pd.read_csv(\"listed_data/df_CaseXX.csv\")\n",
    "df_crkt = pd.read_csv(\"listed_data/df_crkt.csv\")\n",
    "df_kershaw = pd.read_csv(\"listed_data/df_kershaw.csv\")\n",
    "# df_leatherman = pd.read_csv(\"listed_data/df_leatherman.csv\")\n",
    "df_sog = pd.read_csv(\"listed_data/df_sog.csv\")\n",
    "df_spyderco = pd.read_csv(\"listed_data/df_spyderco.csv\")\n",
    "# df_victorinox = pd.read_csv(\"listed_data/df_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchIds = df_bench.itemId.values.tolist()\n",
    "buckIds = df_buck.itemId.values.tolist()\n",
    "caseIds = df_case.itemId.values.tolist()\n",
    "caseXXIds = df_caseXX.itemId.values.tolist()\n",
    "crktIds = df_crkt.itemId.values.tolist()\n",
    "kershawIds = df_kershaw.itemId.values.tolist()\n",
    "# leathIds = df_leatherman.itemId.values.tolist()\n",
    "sogIds = df_sog.itemId.values.tolist()\n",
    "spydIds = df_spyderco.itemId.values.tolist()\n",
    "# victIds = df_victorinox.itemId.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "'GetMultipleItems: Class: RequestError, Severity: Error, Code: 1.33, Token not available in request.Token not available in request. Please specify a valid token as HTTP header.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-549c34a37633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchIds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprocess_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchIds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbench\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2a8325a0d72d>\u001b[0m in \u001b[0;36mprocess_list\u001b[0;34m(my_list)\u001b[0m\n\u001b[1;32m    267\u001b[0m                \u001b[0;34m'IncludeSelector'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ItemSpecifics'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m               }\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GetMultipleItems'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/ebaysdk/connection.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, verb, data, list_nodes, verb_attrs, files)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total time=%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/ebaysdk/connection.py\u001b[0m in \u001b[0;36merror_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mestr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresponse_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: 'GetMultipleItems: Class: RequestError, Severity: Error, Code: 1.33, Token not available in request.Token not available in request. Please specify a valid token as HTTP header.'"
     ]
    }
   ],
   "source": [
    "#ShoppingAPI call to return benchmade item specific data.\n",
    "\n",
    "# full_dataset = []\n",
    "# for i in range(0, len(benchIds), 20):\n",
    "#     process_list(benchIds[i:i+20])\n",
    "\n",
    "# bench = pd.DataFrame(full_dataset)\n",
    "# bench.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "# bench.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return buck item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(buckIds), 20):\n",
    "    process_list(buckIds[i:i+20])\n",
    "\n",
    "buck = pd.DataFrame(full_dataset)\n",
    "buck.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "buck.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return case brand item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(caseIds), 20):\n",
    "    process_list(caseIds[i:i+20])\n",
    "\n",
    "df_case = pd.DataFrame(full_dataset)\n",
    "df_case.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_case.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return caseXX brand item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(caseXXIds), 20):\n",
    "    process_list(caseXXIds[i:i+20])\n",
    "\n",
    "df_caseXX = pd.DataFrame(full_dataset)\n",
    "df_caseXX.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_caseXX.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return crkt item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(crktIds), 20):\n",
    "    process_list(crktIds[i:i+20])\n",
    "\n",
    "crkt = pd.DataFrame(full_dataset)\n",
    "crkt.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "crkt.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return kershaw item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(kershawIds), 20):\n",
    "    process_list(kershawIds[i:i+20])\n",
    "\n",
    "kershaw = pd.DataFrame(full_dataset)\n",
    "kershaw.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "kershaw.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return leatherman item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(leathIds), 20):\n",
    "    process_list(leathIds[i:i+20])\n",
    "\n",
    "leath = pd.DataFrame(full_dataset)\n",
    "leath.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "leath.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return SOG item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(sogIds), 20):\n",
    "    process_list(sogIds[i:i+20])\n",
    "\n",
    "sog = pd.DataFrame(full_dataset)\n",
    "sog.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "sog.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return spyderco item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(spydIds), 20):\n",
    "    process_list(spydIds[i:i+20])\n",
    "\n",
    "spyd = pd.DataFrame(full_dataset)\n",
    "spyd.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "spyd.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return victorinox item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(victIds), 20):\n",
    "    process_list(victIds[i:i+20])\n",
    "    \n",
    "vict = pd.DataFrame(full_dataset)\n",
    "vict.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "vict.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bench.to_csv(\"listed_data/benchIds.csv\", index=False)\n",
    "buck.to_csv(\"listed_data/buckIds.csv\", index=False)\n",
    "case.to_csv(\"listed_data/caseIds.csv\", index=False)\n",
    "caseXX.to_csv(\"listed_data/caseXXIds.csv\", index=False)\n",
    "crkt.to_csv(\"listed_data/crktIds.csv\", index=False)\n",
    "kershaw.to_csv(\"listed_data/kershawIds.csv\", index=False)\n",
    "leath.to_csv(\"listed_data/leathIds.csv\", index=False)\n",
    "sog.to_csv(\"listed_data/sogIds.csv\", index=False)\n",
    "spyd.to_csv(\"listed_data/spydIds.csv\", index=False)\n",
    "vict.to_csv(\"listed_data/victIds.csv\", index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of prep to merge original listed data with item specific data requested using a seperate API for more complete details about all listings gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv(\"listed_data/benchIds.csv\")\n",
    "buck = pd.read_csv(\"listed_data/buckIds.csv\")\n",
    "case = pd.read_csv(\"listed_data/caseIds.csv\")\n",
    "caseXX = pd.read_csv(\"listed_data/caseXXIds.csv\")\n",
    "crkt = pd.read_csv(\"listed_data/crktIds.csv\")\n",
    "kershaw = pd.read_csv(\"listed_data/kershawIds.csv\")\n",
    "leath = pd.read_csv(\"listed_data/leathIds.csv\")\n",
    "sog = pd.read_csv(\"listed_data/sogIds.csv\")\n",
    "spyd = pd.read_csv(\"listed_data/spydIds.csv\")\n",
    "vict = pd.read_csv(\"listed_data/victIds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [bench,buck,\n",
    "           case,caseXX,\n",
    "           crkt,kershaw,\n",
    "           leath,sog,\n",
    "           spyd,vict]\n",
    "\n",
    "for dataframe in df_list:\n",
    "    dataframe.rename({'Title': 'title',\n",
    "                      'ItemID': 'itemId'},\n",
    "                     axis=1,inplace=True)\n",
    "    \n",
    "    dataframe.drop(['ConditionID','ConvertedCurrentPrice'], \n",
    "                   axis=1, inplace=True)\n",
    "    dataframe['title'] = dataframe['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge Item Specific dataframes with original listed data using itemIds and title\n",
    "bench_merged = df_bench.merge(bench)\n",
    "buck_merged = df_buck.merge(buck)\n",
    "case_merged = df_case.merge(case)\n",
    "caseXX_merged = df_caseXX.merge(caseXX)\n",
    "crkt_merged = df_crkt.merge(crkt)\n",
    "kershaw_merged = df_kershaw.merge(kershaw)\n",
    "leath_merged = df_leatherman.merge(leath)\n",
    "spyd_merged = df_spyderco.merge(spyd)\n",
    "sog_merged = df_sog.merge(sog)\n",
    "vict_merged = df_victorinox.merge(vict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_spec = transform_item_specifics(bench_merged, perc=75.0)\n",
    "buck_spec = transform_item_specifics(buck_merged, perc=75.0)\n",
    "case_spec = transform_item_specifics(case_merged, perc=75.0)\n",
    "caseXX_spec = transform_item_specifics(caseXX_merged, perc=75.0)\n",
    "crkt_spec = transform_item_specifics(crkt_merged, perc=75.0)\n",
    "kershaw_spec = transform_item_specifics(kershaw_merged, perc=75.0)\n",
    "leath_spec = transform_item_specifics(leath_merged, perc=75.0)\n",
    "sog_spec = transform_item_specifics(sog_merged, perc=75.0)\n",
    "spyd_spec = transform_item_specifics(spyd_merged, perc=75.0)\n",
    "vict_spec = transform_item_specifics(vict_merged, perc=75.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs_list = [bench_spec, buck_spec,\n",
    "              case_spec, caseXX_spec,\n",
    "              crkt_spec, kershaw_spec,\n",
    "              leath_spec, sog_spec,\n",
    "              spyd_spec, vict_spec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in specs_list:\n",
    "    dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataframe in specs_list:\n",
    "    dataframe.rename({'Brand': 'specBrand'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_bench = bench_merged.join(bench_spec)\n",
    "tot_buck = buck_merged.join(buck_spec)\n",
    "tot_case = case_merged.join(case_spec)\n",
    "tot_caseXX = caseXX_merged.join(caseXX_spec)\n",
    "tot_crkt = crkt_merged.join(crkt_spec)\n",
    "tot_kershaw = kershaw_merged.join(kershaw_spec)\n",
    "tot_leath = leath_merged.join(leath_spec)\n",
    "tot_sog = sog_merged.join(sog_spec)\n",
    "tot_spyd = spyd_merged.join(spyd_spec)\n",
    "tot_vict = vict_merged.join(vict_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_bench.to_csv('listed_data/total_list_bench.csv', index=False)\n",
    "tot_buck.to_csv('listed_data/total_list_buck.csv', index=False)\n",
    "tot_case.to_csv('listed_data/total_list_case.csv', index=False)\n",
    "tot_caseXX.to_csv('listed_data/total_list_caseXX.csv', index=False)\n",
    "tot_crkt.to_csv('listed_data/total_list_crkt.csv', index=False)\n",
    "tot_kershaw.to_csv('listed_data/total_list_kershaw.csv', index=False)\n",
    "tot_leath.to_csv('listed_data/total_list_leath.csv', index=False)\n",
    "tot_sog.to_csv('listed_data/total_list_sog.csv', index=False)\n",
    "tot_spyd.to_csv('listed_data/total_list_spyd.csv', index=False)\n",
    "tot_vict.to_csv('listed_data/total_list_vict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tot_bench = pd.read_csv('listed_data/total_list_bench.csv')\n",
    "# tot_buck = pd.read_csv('listed_data/total_list_buck.csv')\n",
    "# tot_case = pd.read_csv('listed_data/total_list_case.csv')\n",
    "# tot_caseXX = pd.read_csv('listed_data/total_list_caseXX.csv')\n",
    "# tot_crkt = pd.read_csv('listed_data/total_list_crkt.csv')\n",
    "# tot_kershaw = pd.read_csv('listed_data/total_list_kershaw.csv')\n",
    "# tot_leath = pd.read_csv('listed_data/total_list_leath.csv')\n",
    "# tot_sog = pd.read_csv('listed_data/total_list_sog.csv')\n",
    "# tot_spyd = pd.read_csv('listed_data/total_list_spyd.csv')\n",
    "# tot_vict = pd.read_csv('listed_data/total_list_vict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_df = pd.concat([tot_bench, tot_buck,\n",
    "            tot_case, tot_caseXX,\n",
    "            tot_crkt, tot_kershaw,\n",
    "            tot_leath, tot_sog,\n",
    "            tot_spyd, tot_vict])\n",
    "\n",
    "listed_df = data_cleaner(listed_df).copy()\n",
    "listed_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_df.drop(['sellingStatus', 'shippingInfo', \n",
    "                'galleryPlusPictureURL', \n",
    "                'GalleryURL', 'ItemSpecifics', \n",
    "                'item_list', 'listingInfo', \n",
    "                'Vintage', 'Modified Item'], \n",
    "                axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_df.to_csv(\"listed_data/listed_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives = pd.concat([tot_bench, tot_buck,\n",
    "                           tot_case, tot_caseXX,\n",
    "                           tot_crkt, tot_kershaw,\n",
    "                           tot_sog,tot_spyd])\n",
    "\n",
    "listed_knives = data_cleaner(listed_knives).copy()\n",
    "listed_knives.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives.drop(['sellingStatus', 'shippingInfo', \n",
    "                    'galleryPlusPictureURL', \n",
    "                    'GalleryURL', 'ItemSpecifics', \n",
    "                    'item_list', 'listingInfo', \n",
    "                    'Vintage', 'Modified Item'], \n",
    "                    axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives.to_csv(\"listed_data/listed_knives_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench = pd.read_csv(\"teraform_data/bench_scraped.csv\")\n",
    "teradf_buck1 = pd.read_csv(\"teraform_data/buck_scraped.csv\")\n",
    "teradf_buck2 = pd.read_csv(\"teraform_data/buck_reversed.csv\")\n",
    "teradf_case = pd.read_csv(\"teraform_data/case_scraped_Copy.csv\")\n",
    "teradf_caseXX1 = pd.read_csv(\"teraform_data/caseXX_scraped.csv\")\n",
    "teradf_caseXX2 = pd.read_csv(\"teraform_data/caseXX_reversed.csv\")\n",
    "teradf_crkt = pd.read_csv(\"teraform_data/crkt_scraped.csv\")\n",
    "teradf_kershaw = pd.read_csv(\"teraform_data/Kershaw_scraped.csv\")\n",
    "teradf_sog = pd.read_csv(\"teraform_data/SOG_scraped.csv\")\n",
    "teradf_spyd = pd.read_csv(\"teraform_data/spyd_scraped.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in [teradf_buck2,teradf_caseXX2]:\n",
    "    dataframe.drop(['Field4', 'shipping_'], \n",
    "                   axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>Field4</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>shipping_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.ebayimg.com/images/g/DYkAAOSwhJ5jty0...</td>\n",
       "      <td>https://www.ebay.com/itm/394408654612?nordt=tr...</td>\n",
       "      <td>Benchmade 560BK1 Freek Folding Knife</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$140.00</td>\n",
       "      <td>$13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.ebayimg.com/images/g/KW4AAOSwIpBjs6i...</td>\n",
       "      <td>https://www.ebay.com/itm/266067848542?nordt=tr...</td>\n",
       "      <td>Benchmade Pink Griptilian 556 AXIS Assist 154C...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$99.00</td>\n",
       "      <td>$6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.ebayimg.com/images/g/PvoAAOSwEPhjs6Z...</td>\n",
       "      <td>https://www.ebay.com/itm/266067841410?nordt=tr...</td>\n",
       "      <td>Benchmade Black Barrage 585 AXIS Assist Drop P...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$88.00</td>\n",
       "      <td>$6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.ebayimg.com/images/g/oEkAAOSwR91jsXo...</td>\n",
       "      <td>https://www.ebay.com/itm/155337381120?nordt=tr...</td>\n",
       "      <td>Benchmade 560BK-1 Freek Gray Black CPM M4 3.60...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$179.00</td>\n",
       "      <td>$5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.ebayimg.com/images/g/DLcAAOSwc4Fjrpq...</td>\n",
       "      <td>https://www.ebay.com/itm/314297176821?nordt=tr...</td>\n",
       "      <td>Manual BENCHMADE 150802 crooked river wood kni...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$157.50</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image                                                url                                              title       Field4 price_in_US shipping_\n",
       "0  https://i.ebayimg.com/images/g/DYkAAOSwhJ5jty0...  https://www.ebay.com/itm/394408654612?nordt=tr...               Benchmade 560BK1 Freek Folding Knife  Jan 5, 2023     $140.00    $13.27\n",
       "1  https://i.ebayimg.com/images/g/KW4AAOSwIpBjs6i...  https://www.ebay.com/itm/266067848542?nordt=tr...  Benchmade Pink Griptilian 556 AXIS Assist 154C...  Jan 5, 2023      $99.00     $6.50\n",
       "2  https://i.ebayimg.com/images/g/PvoAAOSwEPhjs6Z...  https://www.ebay.com/itm/266067841410?nordt=tr...  Benchmade Black Barrage 585 AXIS Assist Drop P...  Jan 5, 2023      $88.00     $6.50\n",
       "3  https://i.ebayimg.com/images/g/oEkAAOSwR91jsXo...  https://www.ebay.com/itm/155337381120?nordt=tr...  Benchmade 560BK-1 Freek Gray Black CPM M4 3.60...  Jan 5, 2023     $179.00     $5.90\n",
       "4  https://i.ebayimg.com/images/g/DLcAAOSwc4Fjrpq...  https://www.ebay.com/itm/314297176821?nordt=tr...  Manual BENCHMADE 150802 crooked river wood kni...  Jan 5, 2023     $157.50     $0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buck1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>Field4</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>shipping_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.ebayimg.com/images/g/itEAAOSwe~Rjtvz...</td>\n",
       "      <td>https://www.ebay.com/itm/334691682388?nordt=tr...</td>\n",
       "      <td>Buck 419 Knife Folding Kalinga S30V Rare.    E...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$212.00</td>\n",
       "      <td>$10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.ebayimg.com/images/g/Hl8AAOSwTEVjtI~...</td>\n",
       "      <td>https://www.ebay.com/itm/225330553218?nordt=tr...</td>\n",
       "      <td>Vintage Buck Knife Holder Holster 110</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$15.99</td>\n",
       "      <td>$5.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.ebayimg.com/images/g/vxwAAOSwQoxjrlK...</td>\n",
       "      <td>https://www.ebay.com/itm/295445473778?nordt=tr...</td>\n",
       "      <td>Buck USA 112 Lockback Pocket Knife 2019 Model</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$56.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.ebayimg.com/images/g/TAYAAOSwMk1jrjS...</td>\n",
       "      <td>https://www.ebay.com/itm/295445439931?nordt=tr...</td>\n",
       "      <td>Buck USA 303 Cadet Pocket Knife 2019 Model</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$28.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.ebayimg.com/images/g/GQ8AAOSwvm1jj1h...</td>\n",
       "      <td>https://www.ebay.com/itm/285065681742?nordt=tr...</td>\n",
       "      <td>Vintage 1994 Buck Knives 110 Folding Knife</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$32.90</td>\n",
       "      <td>$12.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image                                                url                                              title       Field4 price_in_US shipping_\n",
       "0  https://i.ebayimg.com/images/g/itEAAOSwe~Rjtvz...  https://www.ebay.com/itm/334691682388?nordt=tr...  Buck 419 Knife Folding Kalinga S30V Rare.    E...  Jan 5, 2023     $212.00    $10.00\n",
       "1  https://i.ebayimg.com/images/g/Hl8AAOSwTEVjtI~...  https://www.ebay.com/itm/225330553218?nordt=tr...              Vintage Buck Knife Holder Holster 110  Jan 5, 2023      $15.99     $5.85\n",
       "2  https://i.ebayimg.com/images/g/vxwAAOSwQoxjrlK...  https://www.ebay.com/itm/295445473778?nordt=tr...      Buck USA 112 Lockback Pocket Knife 2019 Model  Jan 5, 2023      $56.00     $0.00\n",
       "3  https://i.ebayimg.com/images/g/TAYAAOSwMk1jrjS...  https://www.ebay.com/itm/295445439931?nordt=tr...         Buck USA 303 Cadet Pocket Knife 2019 Model  Jan 5, 2023      $28.00     $0.00\n",
       "4  https://i.ebayimg.com/images/g/GQ8AAOSwvm1jj1h...  https://www.ebay.com/itm/285065681742?nordt=tr...         Vintage 1994 Buck Knives 110 Folding Knife  Jan 5, 2023      $32.90    $12.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buck2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>title</th>\n",
       "      <th>date_sold</th>\n",
       "      <th>shipping_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.ebayimg.com/images/g/Nh0AAOSwr0Rd2~n...</td>\n",
       "      <td>$21.95</td>\n",
       "      <td>Buck 345 OR 346 Vantage Avid Knife, Black OR D...</td>\n",
       "      <td>Jan 23, 2021</td>\n",
       "      <td>$4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.ebayimg.com/images/g/Nh0AAOSwr0Rd2~n...</td>\n",
       "      <td>$37.49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Feb 21, 2021</td>\n",
       "      <td>$4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.ebayimg.com/images/g/Nh0AAOSwr0Rd2~n...</td>\n",
       "      <td>$17.99</td>\n",
       "      <td>Buck 870 Camouflage Tactical Knife Combo Frame...</td>\n",
       "      <td>Mar 15, 2021</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.ebayimg.com/images/g/Nh0AAOSwr0Rd2~n...</td>\n",
       "      <td>$99.99</td>\n",
       "      <td>Buck 770 FlashPoint orange Folding Knife with ...</td>\n",
       "      <td>Mar 22, 2021</td>\n",
       "      <td>$4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.ebayimg.com/images/g/Nh0AAOSwr0Rd2~n...</td>\n",
       "      <td>$16.99</td>\n",
       "      <td>Buck Knives 736 Trekker XLT Partially Serrated...</td>\n",
       "      <td>Apr 7, 2021</td>\n",
       "      <td>$4.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image price_in_US                                              title     date_sold shipping_cost\n",
       "0  https://i.ebayimg.com/images/g/Nh0AAOSwr0Rd2~n...      $21.95  Buck 345 OR 346 Vantage Avid Knife, Black OR D...  Jan 23, 2021         $4.25\n",
       "1  https://i.ebayimg.com/images/g/Nh0AAOSwr0Rd2~n...      $37.49                                                NaN  Feb 21, 2021         $4.99\n",
       "2  https://i.ebayimg.com/images/g/Nh0AAOSwr0Rd2~n...      $17.99  Buck 870 Camouflage Tactical Knife Combo Frame...  Mar 15, 2021         $0.00\n",
       "3  https://i.ebayimg.com/images/g/Nh0AAOSwr0Rd2~n...      $99.99  Buck 770 FlashPoint orange Folding Knife with ...  Mar 22, 2021         $4.99\n",
       "4  https://i.ebayimg.com/images/g/Nh0AAOSwr0Rd2~n...      $16.99  Buck Knives 736 Trekker XLT Partially Serrated...   Apr 7, 2021         $4.99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>Field4</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>shipping_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.ebayimg.com/images/g/yv4AAOSwxHVjqj4...</td>\n",
       "      <td>https://www.ebay.com/itm/266058575726?nordt=tr...</td>\n",
       "      <td>VTG CASE XX USA P158 LSSP MAKO SHARK LOCK BACK...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$95.00</td>\n",
       "      <td>$6.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.ebayimg.com/images/g/or4AAOSw43ZjreO...</td>\n",
       "      <td>https://www.ebay.com/itm/404080796638?nordt=tr...</td>\n",
       "      <td>Case XX Muskrat Knife 5 Dot 1975 Genuine Bone ...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$144.50</td>\n",
       "      <td>$6.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.ebayimg.com/images/g/F8YAAOSw4Tljrkq...</td>\n",
       "      <td>https://www.ebay.com/itm/364094383451?nordt=tr...</td>\n",
       "      <td>Case Rut Season Red Stag Copperlock Knife 1999...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$184.46</td>\n",
       "      <td>$6.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.ebayimg.com/images/g/SN4AAOSwBoNjomq...</td>\n",
       "      <td>https://www.ebay.com/itm/134376462817?nordt=tr...</td>\n",
       "      <td>CASE 6.54052 SS Medium Congress Knife, Bonesta...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$50.00</td>\n",
       "      <td>$5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.ebayimg.com/images/g/q0sAAOSwXYBjriX...</td>\n",
       "      <td>https://www.ebay.com/itm/404081171674?nordt=tr...</td>\n",
       "      <td>Case Canoe Knife 2008 Red &amp; White CASE XX Embe...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$183.50</td>\n",
       "      <td>$6.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image                                                url                                              title       Field4 price_in_US shipping_\n",
       "0  https://i.ebayimg.com/images/g/yv4AAOSwxHVjqj4...  https://www.ebay.com/itm/266058575726?nordt=tr...  VTG CASE XX USA P158 LSSP MAKO SHARK LOCK BACK...  Jan 5, 2023      $95.00     $6.85\n",
       "1  https://i.ebayimg.com/images/g/or4AAOSw43ZjreO...  https://www.ebay.com/itm/404080796638?nordt=tr...  Case XX Muskrat Knife 5 Dot 1975 Genuine Bone ...  Jan 5, 2023     $144.50     $6.95\n",
       "2  https://i.ebayimg.com/images/g/F8YAAOSw4Tljrkq...  https://www.ebay.com/itm/364094383451?nordt=tr...  Case Rut Season Red Stag Copperlock Knife 1999...  Jan 5, 2023     $184.46     $6.95\n",
       "3  https://i.ebayimg.com/images/g/SN4AAOSwBoNjomq...  https://www.ebay.com/itm/134376462817?nordt=tr...  CASE 6.54052 SS Medium Congress Knife, Bonesta...  Jan 5, 2023      $50.00     $5.00\n",
       "4  https://i.ebayimg.com/images/g/q0sAAOSwXYBjriX...  https://www.ebay.com/itm/404081171674?nordt=tr...  Case Canoe Knife 2008 Red & White CASE XX Embe...  Jan 5, 2023     $183.50     $6.95"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caseXX1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>Field4</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>shipping_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.ebayimg.com/images/g/U4kAAOSwBmFjrmN...</td>\n",
       "      <td>https://www.ebay.com/itm/185717561861?nordt=tr...</td>\n",
       "      <td>CASE XX KNIFE DISPLAY 13X9X24 (KNIVES NOT INCL...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$222.50</td>\n",
       "      <td>$26.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.ebayimg.com/images/g/6vgAAOSwfNRjrmC...</td>\n",
       "      <td>https://www.ebay.com/itm/185717549205?nordt=tr...</td>\n",
       "      <td>CASE XX LARGE SOD BUSTER 90S MODEL KNIFE NEVER...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$25.55</td>\n",
       "      <td>$5.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.ebayimg.com/images/g/faQAAOSwxxBjrl3...</td>\n",
       "      <td>https://www.ebay.com/itm/185717541631?nordt=tr...</td>\n",
       "      <td>CASE XX 62031 Hawkbill LHR ELC KNIFE 90s model...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$60.00</td>\n",
       "      <td>$4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.ebayimg.com/images/g/qkwAAOSwanliebu...</td>\n",
       "      <td>https://www.ebay.com/itm/295445514988?nordt=tr...</td>\n",
       "      <td>Case XX USA 3254 SS Trapper Pocket Knife Yellow</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$39.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.ebayimg.com/images/g/iWUAAOSwtx1js54...</td>\n",
       "      <td>https://www.ebay.com/itm/314302110640?nordt=tr...</td>\n",
       "      <td>Case XX 6207 SS Mini Trapper Independence Decl...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$44.65</td>\n",
       "      <td>$5.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image                                                url                                              title       Field4 price_in_US shipping_\n",
       "0  https://i.ebayimg.com/images/g/U4kAAOSwBmFjrmN...  https://www.ebay.com/itm/185717561861?nordt=tr...  CASE XX KNIFE DISPLAY 13X9X24 (KNIVES NOT INCL...  Jan 5, 2023     $222.50    $26.60\n",
       "1  https://i.ebayimg.com/images/g/6vgAAOSwfNRjrmC...  https://www.ebay.com/itm/185717549205?nordt=tr...  CASE XX LARGE SOD BUSTER 90S MODEL KNIFE NEVER...  Jan 5, 2023      $25.55     $5.85\n",
       "2  https://i.ebayimg.com/images/g/faQAAOSwxxBjrl3...  https://www.ebay.com/itm/185717541631?nordt=tr...  CASE XX 62031 Hawkbill LHR ELC KNIFE 90s model...  Jan 5, 2023      $60.00     $4.95\n",
       "3  https://i.ebayimg.com/images/g/qkwAAOSwanliebu...  https://www.ebay.com/itm/295445514988?nordt=tr...    Case XX USA 3254 SS Trapper Pocket Knife Yellow  Jan 5, 2023      $39.00     $0.00\n",
       "4  https://i.ebayimg.com/images/g/iWUAAOSwtx1js54...  https://www.ebay.com/itm/314302110640?nordt=tr...  Case XX 6207 SS Mini Trapper Independence Decl...  Jan 5, 2023      $44.65     $5.85"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caseXX2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>title</th>\n",
       "      <th>date_sold</th>\n",
       "      <th>shipping_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.ebayimg.com/images/g/e1gAAOSwaqFec6u...</td>\n",
       "      <td>$29.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 18, 2021</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.ebayimg.com/images/g/e1gAAOSwaqFec6u...</td>\n",
       "      <td>$13.78</td>\n",
       "      <td>Case XX 059L SS USA Lockback Knife Plastic Han...</td>\n",
       "      <td>May 6, 2021</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.ebayimg.com/images/g/e1gAAOSwaqFec6u...</td>\n",
       "      <td>$6.00</td>\n",
       "      <td>CASE XX Knives White Irridescent Knife Shooter...</td>\n",
       "      <td>May 10, 2021</td>\n",
       "      <td>$3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.ebayimg.com/images/g/e1gAAOSwaqFec6u...</td>\n",
       "      <td>$18.99</td>\n",
       "      <td>Case XX USA 059L SS Stainless Steel Folding Lo...</td>\n",
       "      <td>May 11, 2021</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.ebayimg.com/images/g/e1gAAOSwaqFec6u...</td>\n",
       "      <td>$95.00</td>\n",
       "      <td>CASE XX Knives 6354 HB SS Bone Handle Hobo Kni...</td>\n",
       "      <td>Jun 10, 2021</td>\n",
       "      <td>$7.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image price_in_US                                              title     date_sold shipping_cost\n",
       "0  https://i.ebayimg.com/images/g/e1gAAOSwaqFec6u...      $29.99                                                NaN  Apr 18, 2021         $0.00\n",
       "1  https://i.ebayimg.com/images/g/e1gAAOSwaqFec6u...      $13.78  Case XX 059L SS USA Lockback Knife Plastic Han...   May 6, 2021         $0.00\n",
       "2  https://i.ebayimg.com/images/g/e1gAAOSwaqFec6u...       $6.00  CASE XX Knives White Irridescent Knife Shooter...  May 10, 2021         $3.95\n",
       "3  https://i.ebayimg.com/images/g/e1gAAOSwaqFec6u...      $18.99  Case XX USA 059L SS Stainless Steel Folding Lo...  May 11, 2021         $0.00\n",
       "4  https://i.ebayimg.com/images/g/e1gAAOSwaqFec6u...      $95.00  CASE XX Knives 6354 HB SS Bone Handle Hobo Kni...  Jun 10, 2021         $7.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crkt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date_sold</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>shipping_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.ebayimg.com/images/g/6jIAAOSwRE1jqvL...</td>\n",
       "      <td>https://www.ebay.com/itm/134393899363?nordt=tr...</td>\n",
       "      <td>#557 Black CRKT K300KXP Hootenanny Onion Desig...</td>\n",
       "      <td>Jan 6, 2023</td>\n",
       "      <td>$35.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.ebayimg.com/images/g/V7MAAOSw3lVjrge...</td>\n",
       "      <td>https://www.ebay.com/itm/144877707003?nordt=tr...</td>\n",
       "      <td>CRKT Carson Design M21-10KSF Pocket Knife</td>\n",
       "      <td>Jan 6, 2023</td>\n",
       "      <td>$20.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.ebayimg.com/images/g/-xcAAOSweP1jtY5...</td>\n",
       "      <td>https://www.ebay.com/itm/144886094623?nordt=tr...</td>\n",
       "      <td>CRKT Carson Design M16-10KSF Pocket Knife</td>\n",
       "      <td>Jan 6, 2023</td>\n",
       "      <td>$20.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.ebayimg.com/images/g/ZN0AAOSwoXBjsZ1...</td>\n",
       "      <td>https://www.ebay.com/itm/255906523693?nordt=tr...</td>\n",
       "      <td>Used CRKT M16-14DSFG Special Forces Desert Tan...</td>\n",
       "      <td>Jan 6, 2023</td>\n",
       "      <td>$33.00</td>\n",
       "      <td>$4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.ebayimg.com/images/g/NgEAAOSws8xjeXG...</td>\n",
       "      <td>https://www.ebay.com/itm/394341893404?nordt=tr...</td>\n",
       "      <td>USED CRKT Drifter 6450K Pocket Knife b</td>\n",
       "      <td>Jan 6, 2023</td>\n",
       "      <td>$21.24</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image                                                url                                              title    date_sold price_in_US shipping_\n",
       "0  https://i.ebayimg.com/images/g/6jIAAOSwRE1jqvL...  https://www.ebay.com/itm/134393899363?nordt=tr...  #557 Black CRKT K300KXP Hootenanny Onion Desig...  Jan 6, 2023      $35.00     $0.00\n",
       "1  https://i.ebayimg.com/images/g/V7MAAOSw3lVjrge...  https://www.ebay.com/itm/144877707003?nordt=tr...          CRKT Carson Design M21-10KSF Pocket Knife  Jan 6, 2023      $20.00     $0.00\n",
       "2  https://i.ebayimg.com/images/g/-xcAAOSweP1jtY5...  https://www.ebay.com/itm/144886094623?nordt=tr...          CRKT Carson Design M16-10KSF Pocket Knife  Jan 6, 2023      $20.00     $0.00\n",
       "3  https://i.ebayimg.com/images/g/ZN0AAOSwoXBjsZ1...  https://www.ebay.com/itm/255906523693?nordt=tr...  Used CRKT M16-14DSFG Special Forces Desert Tan...  Jan 6, 2023      $33.00     $4.75\n",
       "4  https://i.ebayimg.com/images/g/NgEAAOSws8xjeXG...  https://www.ebay.com/itm/394341893404?nordt=tr...             USED CRKT Drifter 6450K Pocket Knife b  Jan 6, 2023      $21.24     $0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kershaw\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>Field4</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>shipping_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.ebayimg.com/images/g/mysAAOSwefpjtv8...</td>\n",
       "      <td>https://www.ebay.com/itm/175562212665?nordt=tr...</td>\n",
       "      <td>1660 Kershaw Leek Knife silver plain Blade  as...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$20.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.ebayimg.com/images/g/SNkAAOSwCbtjsko...</td>\n",
       "      <td>https://www.ebay.com/itm/185446319788?nordt=tr...</td>\n",
       "      <td>Kershaw 1660 Leek Pocket Knife Stainless Plain...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$31.92</td>\n",
       "      <td>$5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.ebayimg.com/images/g/hxoAAOSwyBtjn23...</td>\n",
       "      <td>https://www.ebay.com/itm/275585219880?nordt=tr...</td>\n",
       "      <td>Kershaw Volt II 3650 Assisted Open Knife Plain...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$16.00</td>\n",
       "      <td>$5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.ebayimg.com/images/g/lpsAAOSwG5tjrk2...</td>\n",
       "      <td>https://www.ebay.com/itm/295445451063?nordt=tr...</td>\n",
       "      <td>Kershaw USA 1620 Scallion Assisted Opening Knife</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$24.99</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.ebayimg.com/images/g/Zy8AAOSwopljri1...</td>\n",
       "      <td>https://www.ebay.com/itm/295445437316?nordt=tr...</td>\n",
       "      <td>Kershaw USA 1660PUR Leek Assisted Opening Knife</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$36.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image                                                url                                              title       Field4 price_in_US shipping_\n",
       "0  https://i.ebayimg.com/images/g/mysAAOSwefpjtv8...  https://www.ebay.com/itm/175562212665?nordt=tr...  1660 Kershaw Leek Knife silver plain Blade  as...  Jan 5, 2023      $20.00     $0.00\n",
       "1  https://i.ebayimg.com/images/g/SNkAAOSwCbtjsko...  https://www.ebay.com/itm/185446319788?nordt=tr...  Kershaw 1660 Leek Pocket Knife Stainless Plain...  Jan 5, 2023      $31.92     $5.25\n",
       "2  https://i.ebayimg.com/images/g/hxoAAOSwyBtjn23...  https://www.ebay.com/itm/275585219880?nordt=tr...  Kershaw Volt II 3650 Assisted Open Knife Plain...  Jan 5, 2023      $16.00     $5.00\n",
       "3  https://i.ebayimg.com/images/g/lpsAAOSwG5tjrk2...  https://www.ebay.com/itm/295445451063?nordt=tr...   Kershaw USA 1620 Scallion Assisted Opening Knife  Jan 5, 2023      $24.99     $0.00\n",
       "4  https://i.ebayimg.com/images/g/Zy8AAOSwopljri1...  https://www.ebay.com/itm/295445437316?nordt=tr...    Kershaw USA 1660PUR Leek Assisted Opening Knife  Jan 5, 2023      $36.00     $0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sog\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>Field4</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>shipping_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.ebayimg.com/images/g/YM4AAOSwUMFjsOw...</td>\n",
       "      <td>https://www.ebay.com/itm/314299721324?nordt=tr...</td>\n",
       "      <td>SOG SPECIALTY KNIVES TWITCH II 2 BLACK LOCKBAC...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$26.00</td>\n",
       "      <td>$5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.ebayimg.com/images/g/6VMAAOSwNhJjsdi...</td>\n",
       "      <td>https://www.ebay.com/itm/185720746351?nordt=tr...</td>\n",
       "      <td>SOG Kiku Folding Knife 3.50\", Limited Edition ...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$77.00</td>\n",
       "      <td>$17.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.ebayimg.com/images/g/v-IAAOSwkoNjm4m...</td>\n",
       "      <td>https://www.ebay.com/itm/155333349954?nordt=tr...</td>\n",
       "      <td>SOG M-46 Topo Meridian Folding Knife 3.25\" Pla...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$124.99</td>\n",
       "      <td>$5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.ebayimg.com/images/g/RBIAAOSwDa9jq6r...</td>\n",
       "      <td>https://www.ebay.com/itm/354371547188?nordt=tr...</td>\n",
       "      <td>SOG Micron II Pocket Keychain Knife Mid Lock P...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$12.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.ebayimg.com/images/g/gsEAAOSwjSVjdo8...</td>\n",
       "      <td>https://www.ebay.com/itm/234784143973?nordt=tr...</td>\n",
       "      <td>SOG FLASH 2 Combo Edge ASSISTED OPEN TACTICAL ...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$39.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image                                                url                                              title       Field4 price_in_US shipping_\n",
       "0  https://i.ebayimg.com/images/g/YM4AAOSwUMFjsOw...  https://www.ebay.com/itm/314299721324?nordt=tr...  SOG SPECIALTY KNIVES TWITCH II 2 BLACK LOCKBAC...  Jan 5, 2023      $26.00     $5.90\n",
       "1  https://i.ebayimg.com/images/g/6VMAAOSwNhJjsdi...  https://www.ebay.com/itm/185720746351?nordt=tr...  SOG Kiku Folding Knife 3.50\", Limited Edition ...  Jan 5, 2023      $77.00    $17.05\n",
       "2  https://i.ebayimg.com/images/g/v-IAAOSwkoNjm4m...  https://www.ebay.com/itm/155333349954?nordt=tr...  SOG M-46 Topo Meridian Folding Knife 3.25\" Pla...  Jan 5, 2023     $124.99     $5.90\n",
       "3  https://i.ebayimg.com/images/g/RBIAAOSwDa9jq6r...  https://www.ebay.com/itm/354371547188?nordt=tr...  SOG Micron II Pocket Keychain Knife Mid Lock P...  Jan 5, 2023      $12.00     $0.00\n",
       "4  https://i.ebayimg.com/images/g/gsEAAOSwjSVjdo8...  https://www.ebay.com/itm/234784143973?nordt=tr...  SOG FLASH 2 Combo Edge ASSISTED OPEN TACTICAL ...  Jan 5, 2023      $39.00     $0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spyderco\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>Field4</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>shipping_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.ebayimg.com/images/g/9hMAAOSw2RpjcAO...</td>\n",
       "      <td>https://www.ebay.com/itm/275537703449?nordt=tr...</td>\n",
       "      <td>Rare PINK Spyderco Ladybug 1 Original Folding ...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$119.00</td>\n",
       "      <td>$5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.ebayimg.com/images/g/4XkAAOSwxxBjs52...</td>\n",
       "      <td>https://www.ebay.com/itm/266067805694?nordt=tr...</td>\n",
       "      <td>Spyderco Black on Black Delica 4 Half Serrated...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$35.00</td>\n",
       "      <td>$6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.ebayimg.com/images/g/t7EAAOSwMqpjs5c...</td>\n",
       "      <td>https://www.ebay.com/itm/266067757103?nordt=tr...</td>\n",
       "      <td>Spyderco Endura H-1 Black Plain Edge H-1 Blade...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$35.00</td>\n",
       "      <td>$6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.ebayimg.com/images/g/HUoAAOSw1NNjrNH...</td>\n",
       "      <td>https://www.ebay.com/itm/325487034577?nordt=tr...</td>\n",
       "      <td>Spyderco Endura 6 inch Endura 4 Folder Knife -...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$60.00</td>\n",
       "      <td>$5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.ebayimg.com/images/g/~bsAAOSw9dFjtaf...</td>\n",
       "      <td>https://www.ebay.com/itm/144886211394?nordt=tr...</td>\n",
       "      <td>Spyderco Byrd Lightweight Folding Knife, Hollo...</td>\n",
       "      <td>Jan 5, 2023</td>\n",
       "      <td>$24.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image                                                url                                              title       Field4 price_in_US shipping_\n",
       "0  https://i.ebayimg.com/images/g/9hMAAOSw2RpjcAO...  https://www.ebay.com/itm/275537703449?nordt=tr...  Rare PINK Spyderco Ladybug 1 Original Folding ...  Jan 5, 2023     $119.00     $5.00\n",
       "1  https://i.ebayimg.com/images/g/4XkAAOSwxxBjs52...  https://www.ebay.com/itm/266067805694?nordt=tr...  Spyderco Black on Black Delica 4 Half Serrated...  Jan 5, 2023      $35.00     $6.50\n",
       "2  https://i.ebayimg.com/images/g/t7EAAOSwMqpjs5c...  https://www.ebay.com/itm/266067757103?nordt=tr...  Spyderco Endura H-1 Black Plain Edge H-1 Blade...  Jan 5, 2023      $35.00     $6.50\n",
       "3  https://i.ebayimg.com/images/g/HUoAAOSw1NNjrNH...  https://www.ebay.com/itm/325487034577?nordt=tr...  Spyderco Endura 6 inch Endura 4 Folder Knife -...  Jan 5, 2023      $60.00     $5.50\n",
       "4  https://i.ebayimg.com/images/g/~bsAAOSw9dFjtaf...  https://www.ebay.com/itm/144886211394?nordt=tr...  Spyderco Byrd Lightweight Folding Knife, Hollo...  Jan 5, 2023      $24.00     $0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_dict = {'benchmade': teradf_bench, \n",
    "           'buck1': teradf_buck1,\n",
    "           'buck2': teradf_buck2,\n",
    "           'case':teradf_case,\n",
    "           'caseXX1':teradf_caseXX1,\n",
    "           'caseXX2':teradf_caseXX2,\n",
    "           'crkt':teradf_crkt,\n",
    "           'kershaw':teradf_kershaw,\n",
    "           'sog':teradf_sog, \n",
    "           'spyderco':teradf_spyd}\n",
    "          \n",
    "\n",
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in df_dict.values():\n",
    "    val.rename({'Field4': 'date_sold',\n",
    "                'shipping_': 'shipping_cost'},\n",
    "               axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmade\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'title', 'date_sold', 'price_in_US', 'shipping_cost'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buck1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'title', 'date_sold', 'price_in_US', 'shipping_cost'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buck2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'price_in_US', 'title', 'date_sold', 'shipping_cost'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'title', 'date_sold', 'price_in_US', 'shipping_cost'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caseXX1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'title', 'date_sold', 'price_in_US', 'shipping_cost'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caseXX2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'price_in_US', 'title', 'date_sold', 'shipping_cost'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crkt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'title', 'date_sold', 'price_in_US', 'shipping_cost'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kershaw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'title', 'date_sold', 'price_in_US', 'shipping_cost'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'title', 'date_sold', 'price_in_US', 'shipping_cost'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spyderco\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Image', 'url', 'title', 'date_sold', 'price_in_US', 'shipping_cost'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in df_dict.values():\n",
    "    val['date_sold'] = pd.to_datetime(val['date_sold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_buck = pd.concat([teradf_buck1,teradf_buck2])\n",
    "teradf_caseXX = pd.concat([teradf_caseXX1,teradf_caseXX2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_buck.drop_duplicates(\n",
    "    subset = ['date_sold', 'price_in_US', 'shipping_cost'],\n",
    "    keep = 'last', inplace=True)\n",
    "\n",
    "teradf_case.drop_duplicates(\n",
    "    subset = ['date_sold', 'price_in_US', 'shipping_cost'],\n",
    "    keep = 'last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmade': 45.0,\n",
       " 'buck': 20.0,\n",
       " 'case': 20.0,\n",
       " 'crkt': 15.0,\n",
       " 'kershaw': 15.0,\n",
       " 'leatherman': 30.0,\n",
       " 'sog': 15.0,\n",
       " 'spyderco': 30.0,\n",
       " 'victorinox': 20.0}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench =  prepare_tera_df(teradf_bench, 0)\n",
    "teradf_buck =  prepare_tera_df(teradf_buck, 1)\n",
    "teradf_case =  prepare_tera_df(teradf_case, 2)\n",
    "teradf_caseXX =  prepare_tera_df(teradf_caseXX, 2)\n",
    "teradf_crkt =  prepare_tera_df(teradf_crkt, 3)\n",
    "teradf_kershaw =  prepare_tera_df(teradf_kershaw, 4)\n",
    "teradf_sog =  prepare_tera_df(teradf_sog, 6)\n",
    "teradf_spyd =  prepare_tera_df(teradf_spyd, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teradf_bench = prepare_tera_df(teradf_bench, 0)\n",
    "# teradf_buck = prepare_tera_df(teradf_buck, 1)\n",
    "# teradf_case = prepare_tera_df(teradf_case, 2)\n",
    "# teradf_crkt = prepare_tera_df(teradf_crkt, 3)\n",
    "# teradf_kershaw = prepare_tera_df(teradf_kershaw, 4)\n",
    "# teradf_leath = prepare_tera_df(teradf_leath, 5)\n",
    "# teradf_sog = prepare_tera_df(teradf_sog, 6)\n",
    "# teradf_spyd = prepare_tera_df(teradf_spyd, 7)\n",
    "# teradf_vict = prepare_tera_df(teradf_vict, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in df_dict.values():\n",
    "    dataframe['title'] = dataframe['title'].str.lower()\n",
    "    dataframe['title'] = dataframe['title'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df = pd.concat([teradf_bench, teradf_buck,\n",
    "                     teradf_case, teradf_caseXX, \n",
    "                     teradf_crkt, teradf_kershaw,\n",
    "                     teradf_sog, teradf_spyd])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 84182 entries, 0 to 9123\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   Image            84182 non-null  object        \n",
      " 1   url              18633 non-null  object        \n",
      " 2   title            37267 non-null  object        \n",
      " 3   date_sold        84182 non-null  datetime64[ns]\n",
      " 4   price_in_US      84182 non-null  float64       \n",
      " 5   shipping_cost    84182 non-null  float64       \n",
      " 6   converted_price  84182 non-null  float64       \n",
      " 7   profit           84182 non-null  float64       \n",
      " 8   ROI              84182 non-null  float64       \n",
      " 9   brand            84182 non-null  object        \n",
      " 10  cost             84182 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(6), object(4)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "tera_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case         27805\n",
       "buck         17071\n",
       "kershaw       9900\n",
       "spyderco      9124\n",
       "benchmade     8707\n",
       "crkt          6730\n",
       "sog           4845\n",
       "Name: brand, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tera_df['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of API calls for listed data. To be merged with item specific data using ebay itemIds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29505"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "141000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "22500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "19500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "111000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running functions to call the Finding API and return datasets for cat () knives for sale listed on ebay in the last 90 days. (explain how ebay rules work)\n",
    "\n",
    "\n",
    "bench_df = knife_request('Benchmade', 0)\n",
    "buck_df = knife_request('Buck', 1)\n",
    "case_df = knife_request('Case', 2)\n",
    "df_caseXX = knife_request('Case XX', 2)\n",
    "df_crkt = knife_request(\"CRKT\", 3)\n",
    "# df_leatherman = knife_request('Leatherman', 5)\n",
    "df_sog = knife_request('SOG', 6)\n",
    "df_spyderco = knife_request('Spyderco', 7)\n",
    "\n",
    "\n",
    "# bench_df.to_csv('data/df_bench1.csv', index=False)\n",
    "# buck_df.to_csv('data/df_buck.csv', index=False)\n",
    "# case_df.to_csv('data/df_case.csv', index=False)\n",
    "# df_caseXX.to_csv('data/df_CaseXX.csv', index=False)\n",
    "# df_crkt.to_csv('data/df_crkt.csv', index=False)\n",
    "# df_leatherman.to_csv('data/df_leatherman.csv', index=False)\n",
    "# df_sog.to_csv('data/df_sog.csv', index=False)\n",
    "# df_spyderco.to_csv('data/df_spyderco.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = pd.read_csv(\"listed_data/df_bench1.csv\")\n",
    "df_buck = pd.read_csv(\"listed_data/df_buck.csv\")\n",
    "df_case = pd.read_csv(\"listed_data/df_case.csv\")\n",
    "df_caseXX = pd.read_csv(\"listed_data/df_CaseXX.csv\")\n",
    "df_crkt = pd.read_csv(\"listed_data/df_crkt.csv\")\n",
    "df_kershaw = pd.read_csv(\"listed_data/df_kershaw.csv\")\n",
    "df_leatherman = pd.read_csv(\"listed_data/df_leatherman.csv\")\n",
    "df_sog = pd.read_csv(\"listed_data/df_sog.csv\")\n",
    "df_spyderco = pd.read_csv(\"listed_data/df_spyderco.csv\")\n",
    "df_victorinox = pd.read_csv(\"listed_data/df_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchIds = df_bench.itemId.values.tolist()\n",
    "buckIds = df_buck.itemId.values.tolist()\n",
    "caseIds = df_case.itemId.values.tolist()\n",
    "caseXXIds = df_caseXX.itemId.values.tolist()\n",
    "crktIds = df_crkt.itemId.values.tolist()\n",
    "kershawIds = df_kershaw.itemId.values.tolist()\n",
    "leathIds = df_leatherman.itemId.values.tolist()\n",
    "sogIds = df_sog.itemId.values.tolist()\n",
    "spydIds = df_spyderco.itemId.values.tolist()\n",
    "victIds = df_victorinox.itemId.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return benchmade item specific data.\n",
    "\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(benchIds), 20):\n",
    "    process_list(benchIds[i:i+20])\n",
    "\n",
    "bench = pd.DataFrame(full_dataset)\n",
    "bench.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "bench.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return buck item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(buckIds), 20):\n",
    "    process_list(buckIds[i:i+20])\n",
    "\n",
    "buck = pd.DataFrame(full_dataset)\n",
    "buck.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "buck.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return case brand item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(caseIds), 20):\n",
    "    process_list(caseIds[i:i+20])\n",
    "\n",
    "df_case = pd.DataFrame(full_dataset)\n",
    "df_case.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_case.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return caseXX brand item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(caseXXIds), 20):\n",
    "    process_list(caseXXIds[i:i+20])\n",
    "\n",
    "df_caseXX = pd.DataFrame(full_dataset)\n",
    "df_caseXX.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "df_caseXX.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return crkt item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(crktIds), 20):\n",
    "    process_list(crktIds[i:i+20])\n",
    "\n",
    "crkt = pd.DataFrame(full_dataset)\n",
    "crkt.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "crkt.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return kershaw item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(kershawIds), 20):\n",
    "    process_list(kershawIds[i:i+20])\n",
    "\n",
    "kershaw = pd.DataFrame(full_dataset)\n",
    "kershaw.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "kershaw.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return leatherman item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(leathIds), 20):\n",
    "    process_list(leathIds[i:i+20])\n",
    "\n",
    "leath = pd.DataFrame(full_dataset)\n",
    "leath.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "leath.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return SOG item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(sogIds), 20):\n",
    "    process_list(sogIds[i:i+20])\n",
    "\n",
    "sog = pd.DataFrame(full_dataset)\n",
    "sog.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "sog.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return spyderco item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(spydIds), 20):\n",
    "    process_list(spydIds[i:i+20])\n",
    "\n",
    "spyd = pd.DataFrame(full_dataset)\n",
    "spyd.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "spyd.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShoppingAPI call to return victorinox item specific data.\n",
    "```\n",
    "full_dataset = []\n",
    "for i in range(0, len(victIds), 20):\n",
    "    process_list(victIds[i:i+20])\n",
    "    \n",
    "vict = pd.DataFrame(full_dataset)\n",
    "vict.drop_duplicates(subset=['ItemID'], inplace=True)\n",
    "vict.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bench.to_csv(\"listed_data/benchIds.csv\", index=False)\n",
    "buck.to_csv(\"listed_data/buckIds.csv\", index=False)\n",
    "case.to_csv(\"listed_data/caseIds.csv\", index=False)\n",
    "caseXX.to_csv(\"listed_data/caseXXIds.csv\", index=False)\n",
    "crkt.to_csv(\"listed_data/crktIds.csv\", index=False)\n",
    "kershaw.to_csv(\"listed_data/kershawIds.csv\", index=False)\n",
    "leath.to_csv(\"listed_data/leathIds.csv\", index=False)\n",
    "sog.to_csv(\"listed_data/sogIds.csv\", index=False)\n",
    "spyd.to_csv(\"listed_data/spydIds.csv\", index=False)\n",
    "vict.to_csv(\"listed_data/victIds.csv\", index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of prep to merge original listed data with item specific data requested using a seperate API for more complete details about all listings gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv(\"listed_data/benchIds.csv\")\n",
    "buck = pd.read_csv(\"listed_data/buckIds.csv\")\n",
    "case = pd.read_csv(\"listed_data/caseIds.csv\")\n",
    "caseXX = pd.read_csv(\"listed_data/caseXXIds.csv\")\n",
    "crkt = pd.read_csv(\"listed_data/crktIds.csv\")\n",
    "kershaw = pd.read_csv(\"listed_data/kershawIds.csv\")\n",
    "leath = pd.read_csv(\"listed_data/leathIds.csv\")\n",
    "sog = pd.read_csv(\"listed_data/sogIds.csv\")\n",
    "spyd = pd.read_csv(\"listed_data/spydIds.csv\")\n",
    "vict = pd.read_csv(\"listed_data/victIds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [bench,buck,\n",
    "           case,caseXX,\n",
    "           crkt,kershaw,\n",
    "           leath,sog,\n",
    "           spyd,vict]\n",
    "\n",
    "for dataframe in df_list:\n",
    "    dataframe.rename({'Title': 'title',\n",
    "                      'ItemID': 'itemId'},\n",
    "                     axis=1,inplace=True)\n",
    "    \n",
    "    dataframe.drop(['ConditionID','ConvertedCurrentPrice'], \n",
    "                   axis=1, inplace=True)\n",
    "    dataframe['title'] = dataframe['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge Item Specific dataframes with original listed data using itemIds and title\n",
    "bench_merged = df_bench.merge(bench)\n",
    "buck_merged = df_buck.merge(buck)\n",
    "case_merged = df_case.merge(case)\n",
    "caseXX_merged = df_caseXX.merge(caseXX)\n",
    "crkt_merged = df_crkt.merge(crkt)\n",
    "kershaw_merged = df_kershaw.merge(kershaw)\n",
    "leath_merged = df_leatherman.merge(leath)\n",
    "spyd_merged = df_spyderco.merge(spyd)\n",
    "sog_merged = df_sog.merge(sog)\n",
    "vict_merged = df_victorinox.merge(vict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_spec = transform_item_specifics(bench_merged, perc=75.0)\n",
    "buck_spec = transform_item_specifics(buck_merged, perc=75.0)\n",
    "case_spec = transform_item_specifics(case_merged, perc=75.0)\n",
    "caseXX_spec = transform_item_specifics(caseXX_merged, perc=75.0)\n",
    "crkt_spec = transform_item_specifics(crkt_merged, perc=75.0)\n",
    "kershaw_spec = transform_item_specifics(kershaw_merged, perc=75.0)\n",
    "leath_spec = transform_item_specifics(leath_merged, perc=75.0)\n",
    "sog_spec = transform_item_specifics(sog_merged, perc=75.0)\n",
    "spyd_spec = transform_item_specifics(spyd_merged, perc=75.0)\n",
    "vict_spec = transform_item_specifics(vict_merged, perc=75.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs_list = [bench_spec, buck_spec,\n",
    "              case_spec, caseXX_spec,\n",
    "              crkt_spec, kershaw_spec,\n",
    "              leath_spec, sog_spec,\n",
    "              spyd_spec, vict_spec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in specs_list:\n",
    "    dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataframe in specs_list:\n",
    "    dataframe.rename({'Brand': 'specBrand'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_bench = bench_merged.join(bench_spec)\n",
    "tot_buck = buck_merged.join(buck_spec)\n",
    "tot_case = case_merged.join(case_spec)\n",
    "tot_caseXX = caseXX_merged.join(caseXX_spec)\n",
    "tot_crkt = crkt_merged.join(crkt_spec)\n",
    "tot_kershaw = kershaw_merged.join(kershaw_spec)\n",
    "tot_leath = leath_merged.join(leath_spec)\n",
    "tot_sog = sog_merged.join(sog_spec)\n",
    "tot_spyd = spyd_merged.join(spyd_spec)\n",
    "tot_vict = vict_merged.join(vict_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_bench.to_csv('listed_data/total_list_bench.csv', index=False)\n",
    "tot_buck.to_csv('listed_data/total_list_buck.csv', index=False)\n",
    "tot_case.to_csv('listed_data/total_list_case.csv', index=False)\n",
    "tot_caseXX.to_csv('listed_data/total_list_caseXX.csv', index=False)\n",
    "tot_crkt.to_csv('listed_data/total_list_crkt.csv', index=False)\n",
    "tot_kershaw.to_csv('listed_data/total_list_kershaw.csv', index=False)\n",
    "tot_leath.to_csv('listed_data/total_list_leath.csv', index=False)\n",
    "tot_sog.to_csv('listed_data/total_list_sog.csv', index=False)\n",
    "tot_spyd.to_csv('listed_data/total_list_spyd.csv', index=False)\n",
    "tot_vict.to_csv('listed_data/total_list_vict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tot_bench = pd.read_csv('listed_data/total_list_bench.csv')\n",
    "# tot_buck = pd.read_csv('listed_data/total_list_buck.csv')\n",
    "# tot_case = pd.read_csv('listed_data/total_list_case.csv')\n",
    "# tot_caseXX = pd.read_csv('listed_data/total_list_caseXX.csv')\n",
    "# tot_crkt = pd.read_csv('listed_data/total_list_crkt.csv')\n",
    "# tot_kershaw = pd.read_csv('listed_data/total_list_kershaw.csv')\n",
    "# tot_leath = pd.read_csv('listed_data/total_list_leath.csv')\n",
    "# tot_sog = pd.read_csv('listed_data/total_list_sog.csv')\n",
    "# tot_spyd = pd.read_csv('listed_data/total_list_spyd.csv')\n",
    "# tot_vict = pd.read_csv('listed_data/total_list_vict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_df = pd.concat([tot_bench, tot_buck,\n",
    "            tot_case, tot_caseXX,\n",
    "            tot_crkt, tot_kershaw,\n",
    "            tot_leath, tot_sog,\n",
    "            tot_spyd, tot_vict])\n",
    "\n",
    "listed_df = data_cleaner(listed_df).copy()\n",
    "listed_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_df.drop(['sellingStatus', 'shippingInfo', \n",
    "                'galleryPlusPictureURL', \n",
    "                'GalleryURL', 'ItemSpecifics', \n",
    "                'item_list', 'listingInfo', \n",
    "                'Vintage', 'Modified Item'], \n",
    "                axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_df.to_csv(\"listed_data/listed_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives = pd.concat([tot_bench, tot_buck,\n",
    "                           tot_case, tot_caseXX,\n",
    "                           tot_crkt, tot_kershaw,\n",
    "                           tot_sog,tot_spyd])\n",
    "\n",
    "listed_knives = data_cleaner(listed_knives).copy()\n",
    "listed_knives.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives.drop(['sellingStatus', 'shippingInfo', \n",
    "                    'galleryPlusPictureURL', \n",
    "                    'GalleryURL', 'ItemSpecifics', \n",
    "                    'item_list', 'listingInfo', \n",
    "                    'Vintage', 'Modified Item'], \n",
    "                    axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_knives.to_csv(\"listed_data/listed_knives_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench = pd.read_csv(\"teraform_data/tera_benchmade.csv\")\n",
    "teradf_buck = pd.read_csv(\"teraform_data/tera_buck.csv\")\n",
    "teradf_case = pd.read_csv(\"teraform_data/tera_case.csv\")\n",
    "teradf_crkt = pd.read_csv(\"teraform_data/tera_CRKT.csv\")\n",
    "teradf_kershaw = pd.read_csv(\"teraform_data/tera_kershaw.csv\")\n",
    "teradf_leath = pd.read_csv(\"teraform_data/tera_leatherman.csv\")\n",
    "teradf_sog = pd.read_csv(\"teraform_data/tera_SOG.csv\")\n",
    "teradf_spyd = pd.read_csv(\"teraform_data/tera_spyderco.csv\")\n",
    "teradf_vict = pd.read_csv(\"teraform_data/tera_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'benchmade': teradf_bench, \n",
    "           'buck': teradf_buck,\n",
    "           'case':teradf_case,\n",
    "           'crkt':teradf_crkt,\n",
    "           'kershaw':teradf_kershaw,\n",
    "           'leatherman':teradf_leath,\n",
    "           'sog':teradf_sog, \n",
    "           'spyderco':teradf_spyd,\n",
    "           'victorinox':teradf_vict}\n",
    "\n",
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench.rename({'Data_field':'title', \n",
    "                     'Data_field1':'price_in_US', \n",
    "                     'Data_field2':'shipping_cost', \n",
    "                     'Field2':'date_sold'},\n",
    "                     axis=1, inplace=True)\n",
    "\n",
    "teradf_buck.rename({'avg_price':'price_in_US', \n",
    "                    'avg_shipping':'shipping_cost'},\n",
    "                     axis=1, inplace=True)\n",
    "\n",
    "teradf_case.rename({'url':'title', \n",
    "                    'Field5':'title2', \n",
    "                    'Field4': 'date_sold',\n",
    "                    'Field': 'url',\n",
    "                    'avg_price': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "\n",
    "teradf_crkt.rename({'Field2':'date_sold', \n",
    "                    'title1':'title', \n",
    "                    'avg_cost': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "\n",
    "teradf_kershaw.rename({'Field':'url', \n",
    "                       'Field1':'title', \n",
    "                       'Field3':'date_sold',\n",
    "                       'avg_price': 'price_in_US',\n",
    "                       'avg_shipping': 'shipping_cost'\n",
    "                       }, axis=1, inplace=True)\n",
    "\n",
    "teradf_leath.rename({'Field2':'date_sold', \n",
    "                     'title1':'title2', \n",
    "                     'avg_price': 'price_in_US',\n",
    "                     'avg_shipping': 'shipping_cost'\n",
    "                     }, axis=1, inplace=True)\n",
    "\n",
    "teradf_sog.rename({'Field2':'date_sold', \n",
    "                   'title1':'title', \n",
    "                   'avg_price': 'price_in_US',\n",
    "                   'avg_shipping': 'shipping_cost'\n",
    "                   }, axis=1, inplace=True)\n",
    "\n",
    "teradf_spyd.rename({'Field2':'date_sold', \n",
    "                    'Field3':'title', \n",
    "                    'name':'title2',\n",
    "                    'avg_price': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "\n",
    "teradf_vict.rename({'FfImage':'Image', \n",
    "                    'Field3':'date_sold', \n",
    "                    'avg_price': 'price_in_US',\n",
    "                    'avg_shipping': 'shipping_cost'\n",
    "                    }, axis=1, inplace=True)\n",
    "# teradf_bench.drop(['Field', 'Price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_case.drop(['units_sold', 'Price'], axis=1, inplace=True)\n",
    "teradf_crkt.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_kershaw.drop(['Field2', 'Price'], axis=1, inplace=True)\n",
    "teradf_leath.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_sog.drop(['Field', 'Price'], axis=1, inplace=True)\n",
    "teradf_spyd.drop(['units_sold', 'Price'], axis=1, inplace=True)\n",
    "teradf_vict.drop(['Field2', 'Price'], axis=1, inplace=True)\n",
    "\n",
    "teradf_buck['title'].fillna(teradf_buck['title2'], inplace=True)\n",
    "teradf_case['title'].fillna(teradf_case['title2'], inplace=True)\n",
    "teradf_crkt['title'].fillna(teradf_crkt['title2'], inplace=True)\n",
    "teradf_leath['title'].fillna(teradf_leath['title2'], inplace=True)\n",
    "teradf_sog['title'].fillna(teradf_sog['title2'], inplace=True)\n",
    "teradf_spyd['title'].fillna(teradf_spyd['title2'], inplace=True)\n",
    "teradf_vict['title'].fillna(teradf_vict['title2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title2_list = [teradf_buck, teradf_case,\n",
    "               teradf_crkt, teradf_leath,\n",
    "               teradf_sog, teradf_spyd,\n",
    "               teradf_vict]\n",
    "               \n",
    "for dataframe in title2_list:\n",
    "    dataframe.drop('title2', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'benchmade': teradf_bench, \n",
    "           'buck': teradf_buck,\n",
    "           'case':teradf_case,\n",
    "           'crkt':teradf_crkt,\n",
    "           'kershaw':teradf_kershaw,\n",
    "           'leatherman':teradf_leath,\n",
    "           'sog':teradf_sog, \n",
    "           'spyderco':teradf_spyd,\n",
    "           'victorinox':teradf_vict}\n",
    "for key,val in df_dict.items():\n",
    "    print(key)\n",
    "    display(val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradf_bench = prepare_tera_df(teradf_bench, 0)\n",
    "teradf_buck = prepare_tera_df(teradf_buck, 1)\n",
    "teradf_case = prepare_tera_df(teradf_case, 2)\n",
    "teradf_crkt = prepare_tera_df(teradf_crkt, 3)\n",
    "teradf_kershaw = prepare_tera_df(teradf_kershaw, 4)\n",
    "teradf_leath = prepare_tera_df(teradf_leath, 5)\n",
    "teradf_sog = prepare_tera_df(teradf_sog, 6)\n",
    "teradf_spyd = prepare_tera_df(teradf_spyd, 7)\n",
    "teradf_vict = prepare_tera_df(teradf_vict, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [teradf_bench,teradf_buck,\n",
    "           teradf_case, teradf_crkt,\n",
    "           teradf_kershaw, teradf_leath,\n",
    "           teradf_sog, teradf_spyd,\n",
    "           teradf_vict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in df_list:\n",
    "    dataframe['title'] = dataframe['title'].str.lower()\n",
    "    dataframe['title'] = dataframe['title'].str.strip()\n",
    "    \n",
    "pattern = re.compile(\"(, preview full size image)\")\n",
    "teradf_bench['title'] = teradf_bench['title'].apply(lambda x: re.sub(pattern, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera_df = pd.concat([teradf_bench, teradf_buck,\n",
    "                     teradf_case, teradf_crkt,\n",
    "                     teradf_kershaw, teradf_leath,\n",
    "                     teradf_sog, teradf_spyd,\n",
    "                    teradf_vict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teradf_bench.to_csv(\"teraform_data/tera_bench_prepared.csv\", index=False)\n",
    "# teradf_buck.to_csv(\"teraform_data/tera_buck_prepared.csv\", index=False)\n",
    "# teradf_case.to_csv(\"teraform_data/tera_case_prepared.csv\", index=False)\n",
    "# teradf_crkt.to_csv(\"teraform_data/tera_crkt_prepared.csv\", index=False)\n",
    "# teradf_kershaw.to_csv(\"teraform_data/tera_kershaw_prepared.csv\", index=False)\n",
    "# teradf_leath.to_csv(\"teraform_data/tera_leatherman_prepared.csv\", index=False)\n",
    "# teradf_sog.to_csv(\"teraform_data/tera_sog_prepared.csv\", index=False)\n",
    "# teradf_spyd.to_csv(\"teraform_data/tera_spyd_prepared.csv\", index=False)\n",
    "# teradf_vict.to_csv(\"teraform_data/tera_victorinox_prepared.csv\", index=False)\n",
    "# tera_df.to_csv(\"teraform_data/teraform_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teradf_bench = pd.read_csv(\"teraform_data/tera_bench_prepared.csv\")\n",
    "# teradf_buck = pd.read_csv(\"teraform_data/tera_buck_prepared.csv\")\n",
    "# teradf_case = pd.read_csv(\"teraform_data/tera_case_prepared.csv\")\n",
    "# teradf_crkt = pd.read_csv(\"teraform_data/tera_crkt_prepared.csv\")\n",
    "# teradf_kershaw = pd.read_csv(\"teraform_data/tera_kershaw_prepared.csv\")\n",
    "# teradf_leath = pd.read_csv(\"teraform_data/tera_leatherman_prepared.csv\")\n",
    "# teradf_sog = pd.read_csv(\"teraform_data/tera_sog_prepared.csv\")\n",
    "# teradf_spyd = pd.read_csv(\"teraform_data/tera_spyd_prepared.csv\")\n",
    "# teradf_vict = pd.read_csv(\"teraform_data/tera_victorinox_prepared.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below block of code merged all available teraform ebay itemIds with the appropriate data. This was done in order to call the ebay Shopping API that will only accept itemIds as input. However, much of the data is older than 90 days and can no longer be accessed using the ebay Shopping API. Therefore, the teraform data will unfortunatly lack additional item specific data.  \n",
    "\n",
    "```\n",
    "teradf_benchIDs = pd.read_csv(\"teraform_data/tera_benchmade_itemID.csv\")\n",
    "teradf_buckIDs = pd.read_csv(\"teraform_data/tera_buck_ItemIDs.csv\")\n",
    "teradf_caseIDs = pd.read_csv(\"teraform_data/tera_case_itemIDs.csv\")\n",
    "teradf_kershawIDs = pd.read_csv(\"teraform_data/tera_kershaw_ItemIDs.csv\")\n",
    "teradf_sogIDs = pd.read_csv(\"teraform_data/tera_sog_ItemIDs.csv\")\n",
    "teradf_spydIDs = pd.read_csv(\"teraform_data/tera_spyderco_ItemIDs.csv\")\n",
    "\n",
    "dfID_list = [teradf_benchIDs,teradf_buckIDs,\n",
    "             teradf_caseIDs, teradf_kershawIDs,\n",
    "             teradf_sogIDs, teradf_spydIDs]\n",
    "\n",
    "for dataframe in dfID_list:\n",
    "    dataframe.rename({'Field4': 'date_sold',\n",
    "                      'Data_field': 'itemID',\n",
    "                      'Title': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "    \n",
    "teradf_kershawIDs.rename({'item': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "                       \n",
    "for dataframe in dfID_list:\n",
    "    dataframe.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "for dataframe in dfID_list:\n",
    "    dataframe.rename({'Field4': 'date_sold',\n",
    "                      'Data_field': 'itemID',\n",
    "                      'Title': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "    dataframe.dropna(inplace=True)\n",
    "    dataframe['itemID'] = dataframe['itemID'].apply(int)\n",
    "\n",
    "teradf_kershawIDs.rename({'item': 'title'}, \n",
    "                       axis=1, inplace=True)\n",
    "\n",
    "tera_benchIds = teradf_benchIDs.itemID.values.tolist()\n",
    "tera_buckIds = teradf_buckIDs.itemID.values.tolist()\n",
    "tera_caseIds = teradf_caseIDs.itemID.values.tolist()\n",
    "tera_kershawIds = teradf_kershawIDs.itemID.values.tolist()\n",
    "tera_sogIds = teradf_sogIDs.itemID.values.tolist()\n",
    "tera_spydIds = teradf_spydIDs.itemID.values.tolist()\n",
    "\n",
    "idMerge_bench = teradf_bench.merge(teradf_benchIDs, on='Image')\n",
    "idMerge_buck = teradf_buck.merge(teradf_buckIDs)\n",
    "idMerge_case = teradf_case.merge(teradf_caseIDs)\n",
    "idMerge_kershaw = teradf_kershaw.merge(teradf_kershawIDs)\n",
    "idMerge_spyd = teradf_spyd.merge(teradf_spydIDs)\n",
    "idMerge_sog = teradf_sog.merge(teradf_sogIDs)\n",
    "\n",
    "# idMerge_bench.to_csv('teraform_data/tera_bench_idMerge.csv', index=False)\n",
    "# idMerge_buck.to_csv('teraform_data/tera_buck_idMerge.csv', index=False)\n",
    "# idMerge_case.to_csv('teraform_data/tera_case_idMerge.csv', index=False)\n",
    "# idMerge_kershaw.to_csv('teraform_data/tera_kershaw_idMerge.csv', index=False)\n",
    "# idMerge_spyd.to_csv('teraform_data/tera_spyd_idMerge.csv', index=False)\n",
    "# idMerge_sog.to_csv('teraform_data/tera_sog_idMerge.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_color(line):\n",
    "#     pattern = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['color'] = df3.ItemSpecifics.apply(extract_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_type(line):\n",
    "#     pattern = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_manufacture_region(line):\n",
    "#     pattern = re.compile(\"Country/Region of Manufacture\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['region_of_Manufacture'] = df3.ItemSpecifics.apply(extract_manufacture_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['region_of_Manufacture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_handle_material(line):\n",
    "#     pattern = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['handle_material'] = df3.ItemSpecifics.apply(extract_handle_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['handle_material'].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_lock_type(line):\n",
    "#     pattern = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "\n",
    "# df3['lock_type'] = df3.ItemSpecifics.apply(extract_lock_type)\n",
    "\n",
    "# df3['lock_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_edge(line):\n",
    "#     pattern = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['blade_edge'] = df3.ItemSpecifics.apply(extract_blade_edge)\n",
    "\n",
    "# df3['blade_edge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_dexterity(line):\n",
    "#     pattern = re.compile(\"Dexterity\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['dexterity'] = df3.ItemSpecifics.apply(extract_dexterity)\n",
    "\n",
    "# df3['dexterity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.to_csv('data/item_specifics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root='C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pandas as pd\n",
    "# import matplotlib.pyplot  as plt\n",
    "# from PIL import Image\n",
    "# from pathlib import Path\n",
    "# # import imagesize\n",
    "# import numpy as np\n",
    "\n",
    "# # Get the Image Resolutions\n",
    "# imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "# img_meta = {}\n",
    "# for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# # Convert it to Dataframe and compute aspect ratio\n",
    "# img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "# img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "# img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "# print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "# img_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "    price_list.append(listed_price)\n",
    "    \n",
    "df['price_in_US'] = price_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#atttempt to pull shipping cost from json dict\n",
    "shipping_cost_list = []\n",
    "for row in full_dataset:\n",
    "    shipping_cost = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "    shipping_cost_list.append(shipping_cost)\n",
    "    \n",
    "df['shipping_price'] = shipping_cost_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#pull shipping cost from json dict with regex \n",
    "df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#create new feature 'converted price'\n",
    "df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "df = df.drop_duplicates(subset=['title', 'galleryURL'], keep='first')\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "\n",
    "df.to_csv('data/full_dataset.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwLbGNr0TbIj"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* Were there variables you dropped or created?\n",
    "* How did you address missing values or outliers?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "\n",
    "\n",
    "# here you run your code to clean the data\n",
    "\n",
    "```\n",
    "import code.data_cleaning as dc\n",
    "\n",
    "full_dataset = dc.full_clean()\n",
    "```\n",
    "\n",
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "# here you run your code to model the data\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do in the future to improve this project?\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_knife_dir = 'knife_images'\n",
    "# data_profit_dir = 'data/profit'\n",
    "# new_dir = 'split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_folder = os.path.join(new_dir, 'train')\n",
    "# train_profit = os.path.join(train_folder, 'profit')\n",
    "# os.mkdir(train_folder)\n",
    "# os.mkdir(train_profit)\n",
    "\n",
    "# test_folder = os.path.join(new_dir, 'test')\n",
    "# test_profit = os.path.join(test_folder, 'profit')\n",
    "# os.mkdir(test_folder)\n",
    "# os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# val_folder = os.path.join(new_dir, 'validation')\n",
    "# val_profit = os.path.join(val_folder, 'profit')\n",
    "# os.mkdir(val_folder)\n",
    "# os.mkdir(val_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train knife regression images\n",
    "# #80% of data\n",
    "# imgs = knife_images[:5620]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(train_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # test knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[5620:6322]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(test_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # validation knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[6322:]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(val, img)\n",
    "#     shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "plt.imshow(img_rgb)  # graph it\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_checker(index,):\n",
    "    img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "    plt.imshow(img_rgb)  # graph it\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_benchmade_index[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(2286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(1879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6094)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final processing steps for images\n",
    "\n",
    "image_list = []\n",
    "for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "    img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "    img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "    image_list.append(img_rgb)\n",
    "   \n",
    "    # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=  df_CNN_regression['mean_profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small batch\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256, 3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=30,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub['profit'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.2164 * 41.374303202846974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_batch32.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a train set of 60% and a val and test size of 20% each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model showed a lot of indication that it was overfit\n",
    "#need to retry how I split the data \n",
    "#Instead of manul indexing, will use \n",
    "# from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X[:4918]\n",
    "# y_train = y[:4918]\n",
    "\n",
    "# X_train = X[4918:5971]\n",
    "# y_train = y[4918:5971]\n",
    "\n",
    "# X_test = X[5971:]\n",
    "# y_test = y[5971:]\n",
    "\n",
    "\n",
    "# display(len(X_val)/len(X))\n",
    "# display(len(X_train)/len(X))\n",
    "# len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(224 ,224,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=32,\n",
    "#                     batch_size=300,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ## Data Preparation\n",
    "\n",
    "# Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * Were there variables you dropped or created?\n",
    "# * How did you address missing values or outliers?\n",
    "# * Why are these choices appropriate given the data and the business problem?\n",
    "# ***\n",
    "\n",
    "\n",
    "# # here you run your code to clean the data\n",
    "\n",
    "# ```\n",
    "# import code.data_cleaning as dc\n",
    "\n",
    "# full_dataset = dc.full_clean()\n",
    "# ```\n",
    "\n",
    "# ## Data Modeling\n",
    "# Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * How did you analyze or model the data?\n",
    "# * How did you iterate on your initial approach to make it better?\n",
    "# * Why are these choices appropriate given the data and the business problem?\n",
    "# ***\n",
    "# # here you run your code to model the data\n",
    "\n",
    "\n",
    "# ## Evaluation\n",
    "# Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * How do you interpret the results?\n",
    "# * How well does your model fit your data? How much better is this than your baseline model?\n",
    "# * How confident are you that your results would generalize beyond the data you have?\n",
    "# * How confident are you that this model would benefit the business if put into use?\n",
    "# ***\n",
    "\n",
    "\n",
    "# ## Conclusions\n",
    "# Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "# ***\n",
    "# Questions to consider:\n",
    "# * What would you recommend the business do as a result of this work?\n",
    "# * What are some reasons why your analysis might not fully solve the business problem?\n",
    "# * What else could you do in the future to improve this project?\n",
    "# ***\n",
    "\n",
    "\n",
    "\n",
    "# # data_knife_dir = 'knife_images'\n",
    "# # data_profit_dir = 'data/profit'\n",
    "# # new_dir = 'split'\n",
    "\n",
    "# # os.mkdir(new_dir)\n",
    "\n",
    "# # train_folder = os.path.join(new_dir, 'train')\n",
    "# # train_profit = os.path.join(train_folder, 'profit')\n",
    "# # os.mkdir(train_folder)\n",
    "# # os.mkdir(train_profit)\n",
    "\n",
    "# # test_folder = os.path.join(new_dir, 'test')\n",
    "# # test_profit = os.path.join(test_folder, 'profit')\n",
    "# # os.mkdir(test_folder)\n",
    "# # os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# # val_folder = os.path.join(new_dir, 'validation')\n",
    "# # val_profit = os.path.join(val_folder, 'profit')\n",
    "# # os.mkdir(val_folder)\n",
    "# # os.mkdir(val_profit)\n",
    "\n",
    "# # val_profit\n",
    "\n",
    "# # # train knife regression images\n",
    "# # #80% of data\n",
    "# # imgs = knife_images[:5620]\n",
    "# # for img in imgs:\n",
    "# #     origin = os.path.join(data_knife_dir, img)\n",
    "# #     destination = os.path.join(train_profit, img)\n",
    "# #     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # # test knife regression images\n",
    "# # #10% of data\n",
    "# # imgs = knife_images[5620:6322]\n",
    "# # for img in imgs:\n",
    "# #     origin = os.path.join(data_knife_dir, img)\n",
    "# #     destination = os.path.join(test_profit, img)\n",
    "# #     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # # validation knife regression images\n",
    "# # #10% of data\n",
    "# # imgs = knife_images[6322:]\n",
    "# # for img in imgs:\n",
    "# #     origin = os.path.join(data_knife_dir, img)\n",
    "# #     destination = os.path.join(val, img)\n",
    "# #     shutil.copyfile(origin, destination)\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from keras.models import load_model\n",
    "# from keras.preprocessing import image\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "# img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "# img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "# plt.imshow(img_rgb)  # graph it\n",
    "# plt.show();\n",
    "\n",
    "\n",
    "# def image_checker(index,):\n",
    "#     img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "#     img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "#     plt.imshow(img_rgb)  # graph it\n",
    "#     plt.show();\n",
    "\n",
    "# top_benchmade_index[:50]\n",
    "\n",
    "# image_checker(6158)\n",
    "\n",
    "# image_checker(2286)\n",
    "\n",
    "# image_checker(1879)\n",
    "\n",
    "# image_checker(4326)\n",
    "\n",
    "# image_checker(6094)\n",
    "\n",
    "# #final processing steps for images\n",
    "\n",
    "# image_list = []\n",
    "# for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "#     img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "#     img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "#     img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "#     image_list.append(img_rgb)\n",
    "   \n",
    "#     # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n",
    "\n",
    "\n",
    "# df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())\n",
    "\n",
    "# df_CNN_regression['mean_profit'].describe()\n",
    "\n",
    "# X = np.array(image_list)\n",
    "\n",
    "# y=  df_CNN_regression['mean_profit']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets\n",
    "\n",
    "# X.shape\n",
    "\n",
    "# y.shape\n",
    "\n",
    "# print(\"Xtrain:\", X_train.shape)\n",
    "# print(\"y_train:\", y_train.shape)\n",
    "# print(\"X_test:\", X_test.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n",
    "\n",
    "# X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# print(\"Xtrain:\", X_train.shape)\n",
    "# print(\"y_train:\", y_train.shape)\n",
    "# print(\"X_test:\", X_test.shape)\n",
    "# print(\"y_test:\", y_test.shape)\n",
    "# print(\"X_val:\", X_test.shape)\n",
    "# print(\"y_val:\", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #small batch\n",
    "\n",
    "# # model = models.Sequential()\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "# #                         input_shape=(256 ,256, 3)))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# # model.add(Dense(256, activation='relu'))\n",
    "# # model.add(Dense(128, activation='relu'))\n",
    "# # model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# # model.compile(loss='mean_squared_error',\n",
    "# #               optimizer='Adam',\n",
    "# #                metrics=['mse'])\n",
    "\n",
    "# # history = model.fit(X_train,\n",
    "# #                     y_train,\n",
    "# #                     epochs=30,\n",
    "# #                     batch_size=32,\n",
    "# #                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "# results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "# #model.summary()\n",
    "\n",
    "\n",
    "# df_scrub['profit'].mean()\n",
    "\n",
    "# 2.2164 * 41.374303202846974\n",
    "\n",
    "# df_scrub.head()\n",
    "\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# #The model learned patterns wells until epoch 20\n",
    "# #after that the loss spikes signifcantly before dropping again\n",
    "# fig = plt.figure(figsize=(12,8))\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.plot\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss( mean square error)')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "# plt.show();\n",
    "\n",
    "# model.save('my_model_batch32.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #a train set of 60% and a val and test size of 20% each \n",
    "\n",
    "# #this model showed a lot of indication that it was overfit\n",
    "# #need to retry how I split the data \n",
    "# #Instead of manul indexing, will use \n",
    "# # from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# # X_train = X[:4918]\n",
    "# # y_train = y[:4918]\n",
    "\n",
    "# # X_train = X[4918:5971]\n",
    "# # y_train = y[4918:5971]\n",
    "\n",
    "# # X_test = X[5971:]\n",
    "# # y_test = y[5971:]\n",
    "\n",
    "\n",
    "# # display(len(X_val)/len(X))\n",
    "# # display(len(X_train)/len(X))\n",
    "# # len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# # model = models.Sequential()\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "# #                         input_shape=(224 ,224,  3)))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# # model.add(layers.BatchNormalization())\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# # model.add(layers.Flatten())\n",
    "\n",
    "# # model.add(Dense(512, activation='relu'))\n",
    "# # model.add(Dropout(0.1))\n",
    "\n",
    "# # model.add(Dense(256, activation='relu'))\n",
    "# # model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# # model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# # model.compile(loss='mean_squared_error',\n",
    "# #               optimizer='Adam',\n",
    "# #                metrics=['mse'])\n",
    "# # history = model.fit(X_train,\n",
    "# #                     y_train,\n",
    "# #                     epochs=32,\n",
    "# #                     batch_size=300,\n",
    "# #                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "# #model.summary()\n",
    "\n",
    "\n",
    "# # model.save('my_model_batch500.h5')\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# # model.save('my_model_batch500.h5')\n",
    "\n",
    "# history.history.keys()\n",
    "\n",
    "# #The model is showing a lot of signs of overfitting \n",
    "# fig = plt.figure(figsize=(12,8))\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.plot\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss( mean square error)')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "# plt.show();\n",
    "\n",
    "# X_train.shape\n",
    "\n",
    "# # results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "# #model.summary()\n",
    "\n",
    "\n",
    "# # model.save('my_model_batch500.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
