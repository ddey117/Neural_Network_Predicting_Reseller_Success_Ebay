{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.finding import Connection\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.shopping import Connection as Shopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import  json\n",
    "import numpy as np\n",
    "import re\n",
    "import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tera_df_prepared.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 66881 entries, 0 to 66880\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Image            66881 non-null  object\n",
      " 1   date_sold        52819 non-null  object\n",
      " 2   title            59488 non-null  object\n",
      " 3   avg_price        66881 non-null  object\n",
      " 4   avg_shipping     66881 non-null  object\n",
      " 5   converted_price  66881 non-null  object\n",
      " 6   profit           66881 non-null  object\n",
      " 7   ROI              66881 non-null  object\n",
      " 8   brand            66881 non-null  object\n",
      " 9   cost             66881 non-null  object\n",
      " 10  url              2458 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "request = {\n",
    "           'itemID': list(itemIds),\n",
    "           'IncludeSelector': 'ItemSpecifics'\n",
    "          }\n",
    "response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "response_dict = response.dict()\n",
    "\n",
    "results_list_of_dicts = response_dict['Item']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = response.dict()\n",
    "\n",
    "    #index dict to appropriate index\n",
    "# results_list_of_dicts = response_dict['searchResult']['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_of_dicts = response_dict['Item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "                'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "                'Country', 'Location', 'ConvertedCurrentPrice',\n",
    "                'PictureURL']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "                'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "\n",
    "                'PictureURL']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_list(my_list):\n",
    " \n",
    "    api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "    request = {\n",
    "               'itemID': my_list,\n",
    "               'IncludeSelector': 'ItemSpecifics'\n",
    "              }\n",
    "    response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(itemIds), 20):\n",
    "    process_list(itemIds[i:i+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemIds = list(df.itemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for i in range(0, len(itemIds), 20):\n",
    "    process_list(itemIds[i:i+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop_duplicates(subset='ItemID').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.ItemSpecifics.sample(5).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_opening_mech(line):\n",
    "    pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['opening_mechanism'] = df3.ItemSpecifics.apply(extract_opening_mech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_blade_type(line):\n",
    "    pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.ItemSpecifics.sample(10).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color(line):\n",
    "    pattern = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['color'] = df3.ItemSpecifics.apply(extract_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_blade_type(line):\n",
    "    pattern = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['blade_type'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_manufacture_region(line):\n",
    "    pattern = re.compile(\"Country/Region of Manufacture\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['region_of_Manufacture'] = df3.ItemSpecifics.apply(extract_manufacture_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['region_of_Manufacture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_handle_material(line):\n",
    "    pattern = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['handle_material'] = df3.ItemSpecifics.apply(extract_handle_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['handle_material'].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lock_type(line):\n",
    "    pattern = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "\n",
    "df3['lock_type'] = df3.ItemSpecifics.apply(extract_lock_type)\n",
    "\n",
    "df3['lock_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_blade_edge(line):\n",
    "    pattern = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "df3['blade_edge'] = df3.ItemSpecifics.apply(extract_blade_edge)\n",
    "\n",
    "df3['blade_edge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dexterity(line):\n",
    "    pattern = re.compile(\"Dexterity\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "df3['dexterity'] = df3.ItemSpecifics.apply(extract_dexterity)\n",
    "\n",
    "df3['dexterity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('data/item_specifics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "# request = {\n",
    "#            'itemID': list(itemIds),\n",
    "#            'IncludeSelector': 'ItemSpecifics'\n",
    "#           }\n",
    "# response = api.execute('GetMultipleItems', request)\n",
    "# print(response.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "for imageFolder in listdir('./nn_images2'):\n",
    "    try:\n",
    "        img = Image.open('./nn_images2/'+imageFolder)\n",
    "        img.verify()     # to veify if its an img\n",
    "        img.close()     #to close img and free memory space\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        print('Bad file:', imageFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdir('/nnimages2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root='C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import imagesize\n",
    "import numpy as np\n",
    "\n",
    "# Get the Image Resolutions\n",
    "imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "img_meta = {}\n",
    "for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# Convert it to Dataframe and compute aspect ratio\n",
    "img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "img_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Image Resolutions\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, picker=True)\n",
    "ax.set_title(\"Image Resolution\")\n",
    "ax.set_xlabel(\"Width\", size=14)\n",
    "ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Image Resolutions\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "ax.set_title(\"Image Resolution\")\n",
    "ax.set_xlabel(\"Width\", size=14)\n",
    "ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "        'viewItemURL', 'autoPay', 'postalCode', \n",
    "        'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "        'returnsAccepted', 'condition', 'topRatedListing',\n",
    "        'galleryPlusPictureURL', 'subtitle', 'discountPriceInfo',\n",
    "        'secondaryCategory']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_df(df):\n",
    "#     price_list = []\n",
    "#     for row in full_dataset:\n",
    "#         listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#         price_list.append(listed_price)\n",
    "\n",
    "#     df['price_in_US'] = price_list\n",
    "#     #pull shipping cost from json dict with regex \n",
    "#     df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "#     df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#     #create new feature 'converted price'\n",
    "#     df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#     df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     display(df.head())\n",
    "#     display(df.info())\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    for row in full_dataset:\n",
    "        listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "        price_list.append(listed_price)\n",
    "\n",
    "        try:\n",
    "            listed_ship_price = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "\n",
    "        try:\n",
    "            condition = None\n",
    "            condition = np.float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    display(df.head())\n",
    "    display(df.info())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overhead_cost = 3\n",
    "def prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    "\n",
    "    #remove special characters\n",
    "    df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "    df[str(list(bucket_dict.keys())[bucket_dict_position])] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function was used at first to check accuracy of calls using ebay API\n",
    "#the less complex \"prepare_brands\" function seems sufficient enough\n",
    "#it is redudant to use regex with working ebay API call\n",
    "\n",
    "def modified_prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    "\n",
    "    #remove special characters\n",
    "    df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "    pattern = re.compile(list(bucket_dict.keys())[bucket_dict_position])\n",
    "    df[list(bucket_dict.keys())[bucket_dict_position]] = df_bench.title.apply(lambda x: re.sub(pattern, x, str(list(bucket_dict.values())[bucket_dict_position])))\n",
    "\n",
    "    df[list(bucket_dict.keys())[bucket_dict_position]] = df[list(bucket_dict.keys())[bucket_dict_position]].apply(lambda x: np.float(x))\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "           'categoryId': 48818,\n",
    "            'itemFilter': [\n",
    "                            {'name': 'Condition', 'value': 'Used'},\n",
    "                            {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                          ],\n",
    "            'aspectFilter': [\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Benchmade'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Buck'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Case'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'CRKT'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Leatherman'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Spyderco'},\n",
    "                             ],\n",
    "            'paginationInput': {\n",
    "                                'entriesPerPage': 100,\n",
    "                                'pageNumber': 1\n",
    "\n",
    "                                }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger error at page 23\n",
    "\n",
    "\n",
    "for page in range(1, 22):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "#                                 {'name': 'Condition', 'value': 'Used'},\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Benchmade'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = prepare_df(df_bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = prepare_brands(df_bench, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench['benchmade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench[df_bench['galleryPlusPictureURL'].notna()]['galleryPlusPictureURL'].sample(30).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench['profit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench['ROI'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench.to_csv('data/df_bench.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger error at page 42\n",
    "\n",
    "\n",
    "for page in range(1, 41):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                 {'aspectName': 'Brand', 'aspectValueName': 'Buck'}],\n",
    "\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_buck = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buck = prepare_df(df_buck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buck = prepare_brands(df_buck, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buck.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buck['buck'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buck['profit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_buck['ROI'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buck.to_csv('data/df_buck.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1, 100):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "#                               \n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Case XX'},\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Case'}\n",
    "                                 ],\n",
    "\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_case = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case = prepare_df(df_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case = prepare_brands(df_case, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case.to_csv('data/df_case.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 18\n",
    "\n",
    "\n",
    "for page in range(1, 17):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                    {'aspectName': 'Brand', 'aspectValueName': 'CRKT'}],\n",
    "                 \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_crkt = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crkt = prepare_df(df_crkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crkt = prepare_brands(df_crkt, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_crkt.head())\n",
    "display(df_crkt.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crkt.to_csv('data/df_crkt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger threw error at page 70\n",
    "\n",
    "\n",
    "for page in range(1, 69):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'}\n",
    "                                ],\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_kershaw = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kershaw = prepare_df(df_kershaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kershaw = prepare_brands(df_kershaw, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kershaw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kershaw.to_csv('data/df_kershaw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 18\n",
    "\n",
    "\n",
    "for page in range(1, 17):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Leatherman'}],\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_leatherman = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leatherman = prepare_df(df_leatherman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leatherman = prepare_brands(df_leatherman, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leatherman.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leatherman.to_csv('data/df_leatherman.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 72\n",
    "\n",
    "for page in range(1, 71):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Spyderco'}\n",
    "                              ],\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco = prepare_df(df_spyderco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco = prepare_brands(df_spyderco, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco.to_csv('data/df_spyderco.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "for page in range(1, 21):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'SOG'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_sog = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sog = prepare_df(df_sog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict['sog'] = 15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sog = prepare_brands(df_sog, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sog.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sog.to_csv('data/df_sog.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "for page in range(1, 100):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_victorinox = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_victorinox = prepare_df(df_victorinox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bucket_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_victorinox = prepare_brands(df_victorinox, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_victorinox.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_victorinox.to_csv('data/df_victorinox.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench = pd.read_csv(\"data/df_bench.csv\")\n",
    "# df_buck = pd.read_csv(\"data/df_buck.csv\")\n",
    "# df_case = pd.read_csv(\"data/df_case.csv\")\n",
    "# df_crkt = pd.read_csv(\"data/df_crkt.csv\")\n",
    "# df_kershaw = pd.read_csv(\"data/df_kershaw.csv\")\n",
    "# df_leatherman = pd.read_csv(\"data/df_leatherman.csv\")\n",
    "# df_spyderco = pd.read_csv(\"data/df_spyderco.csv\")\n",
    "# df_victorinox = pd.read_csv(\"data/df_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_bench,df_buck,df_case,df_crkt,df_kershaw,df_leatherman,df_spyderco,df_sog,df_victorinox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/full_dataset2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After succesfully going through 10,000 items on ebay's website and extracting everything possible, there is still a little bit more extracting to do from the json dictionary before saving the dataframe again. We need to get the price of the knives out of the nested dictionary in the dataframe as well as the shipping cost. After that, I would like to create a new feature called\n",
    "**\"converted price,\" which is simply the price of the knife listed on the ebay's website plus shipping.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "    price_list.append(listed_price)\n",
    "    \n",
    "df['price_in_US'] = price_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#atttempt to pull shipping cost from json dict\n",
    "shipping_cost_list = []\n",
    "for row in full_dataset:\n",
    "    shipping_cost = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "    shipping_cost_list.append(shipping_cost)\n",
    "    \n",
    "df['shipping_price'] = shipping_cost_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#pull shipping cost from json dict with regex \n",
    "df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#create new feature 'converted price'\n",
    "df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "df = df.drop_duplicates(subset=['title', 'galleryURL'], keep='first')\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "\n",
    "df.to_csv('data/full_dataset.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwLbGNr0TbIj"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* Were there variables you dropped or created?\n",
    "* How did you address missing values or outliers?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "\n",
    "\n",
    "# here you run your code to clean the data\n",
    "\n",
    "```\n",
    "import code.data_cleaning as dc\n",
    "\n",
    "full_dataset = dc.full_clean()\n",
    "```\n",
    "\n",
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "# here you run your code to model the data\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do in the future to improve this project?\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_knife_dir = 'knife_images'\n",
    "# data_profit_dir = 'data/profit'\n",
    "# new_dir = 'split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_folder = os.path.join(new_dir, 'train')\n",
    "# train_profit = os.path.join(train_folder, 'profit')\n",
    "# os.mkdir(train_folder)\n",
    "# os.mkdir(train_profit)\n",
    "\n",
    "# test_folder = os.path.join(new_dir, 'test')\n",
    "# test_profit = os.path.join(test_folder, 'profit')\n",
    "# os.mkdir(test_folder)\n",
    "# os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# val_folder = os.path.join(new_dir, 'validation')\n",
    "# val_profit = os.path.join(val_folder, 'profit')\n",
    "# os.mkdir(val_folder)\n",
    "# os.mkdir(val_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train knife regression images\n",
    "# #80% of data\n",
    "# imgs = knife_images[:5620]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(train_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # test knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[5620:6322]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(test_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # validation knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[6322:]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(val, img)\n",
    "#     shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "plt.imshow(img_rgb)  # graph it\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_checker(index,):\n",
    "    img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "    plt.imshow(img_rgb)  # graph it\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_benchmade_index[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(2286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(1879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6094)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final processing steps for images\n",
    "\n",
    "image_list = []\n",
    "for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "    img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "    img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "    image_list.append(img_rgb)\n",
    "   \n",
    "    # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=  df_CNN_regression['mean_profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small batch\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256, 3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=30,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub['profit'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.2164 * 41.374303202846974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_batch32.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a train set of 60% and a val and test size of 20% each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model showed a lot of indication that it was overfit\n",
    "#need to retry how I split the data \n",
    "#Instead of manul indexing, will use \n",
    "# from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X[:4918]\n",
    "# y_train = y[:4918]\n",
    "\n",
    "# X_train = X[4918:5971]\n",
    "# y_train = y[4918:5971]\n",
    "\n",
    "# X_test = X[5971:]\n",
    "# y_test = y[5971:]\n",
    "\n",
    "\n",
    "# display(len(X_val)/len(X))\n",
    "# display(len(X_train)/len(X))\n",
    "# len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(224 ,224,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=32,\n",
    "#                     batch_size=300,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model is showing a lot of signs of overfitting \n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from ebaysdk.finding import Connection\n",
    "import requests\n",
    "\n",
    "from ebaysdk.shopping import Connection as Shopping\n",
    "\n",
    "import pandas as pd \n",
    "import  json\n",
    "import numpy as np\n",
    "import re\n",
    "import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "df = pd.read_csv('data/tera_df_prepared.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')\n",
    "\n",
    "df.info()\n",
    "\n",
    "api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "request = {\n",
    "           'itemID': list(itemIds),\n",
    "           'IncludeSelector': 'ItemSpecifics'\n",
    "          }\n",
    "response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "response_dict = response.dict()\n",
    "\n",
    "results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "\n",
    "\n",
    "response_dict = response.dict()\n",
    "\n",
    "    #index dict to appropriate index\n",
    "# results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "                'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "                'Country', 'Location', 'ConvertedCurrentPrice',\n",
    "                'PictureURL']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "                'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "\n",
    "                'PictureURL']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_list(my_list):\n",
    " \n",
    "    api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "    request = {\n",
    "               'itemID': my_list,\n",
    "               'IncludeSelector': 'ItemSpecifics'\n",
    "              }\n",
    "    response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "    return full_dataset\n",
    "\n",
    "full_dataset = []\n",
    "for i in range(0, len(itemIds), 20):\n",
    "    process_list(itemIds[i:i+20])\n",
    "\n",
    "itemIds = list(df.itemId)\n",
    "\n",
    "full_dataset = []\n",
    "for i in range(0, len(itemIds), 20):\n",
    "    process_list(itemIds[i:i+20])\n",
    "\n",
    "len(full_dataset)\n",
    "\n",
    "full_dataset[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(full_dataset)\n",
    "\n",
    "df3 = df2.drop_duplicates(subset='ItemID').copy()\n",
    "\n",
    "df.info()\n",
    "\n",
    "df3.ItemSpecifics.sample(5).apply(print)\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "def extract_opening_mech(line):\n",
    "    pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['opening_mechanism'] = df3.ItemSpecifics.apply(extract_opening_mech)\n",
    "\n",
    "df3\n",
    "\n",
    "pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "def extract_blade_type(line):\n",
    "    pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)\n",
    "\n",
    "df3.ItemSpecifics.sample(10).apply(print)\n",
    "\n",
    "def extract_color(line):\n",
    "    pattern = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['color'] = df3.ItemSpecifics.apply(extract_color)\n",
    "\n",
    "def extract_blade_type(line):\n",
    "    pattern = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)\n",
    "\n",
    "df3['blade_type'].value_counts()[:20]\n",
    "\n",
    "def extract_manufacture_region(line):\n",
    "    pattern = re.compile(\"Country/Region of Manufacture\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "\n",
    "\n",
    "df3['region_of_Manufacture'] = df3.ItemSpecifics.apply(extract_manufacture_region)\n",
    "\n",
    "df3['region_of_Manufacture'].value_counts()\n",
    "\n",
    "def extract_handle_material(line):\n",
    "    pattern = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "\n",
    "\n",
    "df3['handle_material'] = df3.ItemSpecifics.apply(extract_handle_material)\n",
    "\n",
    "df3['handle_material'].value_counts()[:50]\n",
    "\n",
    "def extract_lock_type(line):\n",
    "    pattern = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "\n",
    "df3['lock_type'] = df3.ItemSpecifics.apply(extract_lock_type)\n",
    "\n",
    "df3['lock_type'].value_counts()\n",
    "\n",
    "def extract_blade_edge(line):\n",
    "    pattern = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "df3['blade_edge'] = df3.ItemSpecifics.apply(extract_blade_edge)\n",
    "\n",
    "df3['blade_edge'].value_counts()\n",
    "\n",
    "def extract_dexterity(line):\n",
    "    pattern = re.compile(\"Dexterity\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "df3['dexterity'] = df3.ItemSpecifics.apply(extract_dexterity)\n",
    "\n",
    "df3['dexterity'].value_counts()\n",
    "\n",
    "df3.to_csv('data/item_specifics_df.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "# request = {\n",
    "#            'itemID': list(itemIds),\n",
    "#            'IncludeSelector': 'ItemSpecifics'\n",
    "#           }\n",
    "# response = api.execute('GetMultipleItems', request)\n",
    "# print(response.dict())\n",
    "\n",
    "ls\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "for imageFolder in listdir('./nn_images2'):\n",
    "    try:\n",
    "        img = Image.open('./nn_images2/'+imageFolder)\n",
    "        img.verify()     # to veify if its an img\n",
    "        img.close()     #to close img and free memory space\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        print('Bad file:', imageFolder)\n",
    "\n",
    "listdir('/nnimages2')\n",
    "\n",
    "root='C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images2/'\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import imagesize\n",
    "import numpy as np\n",
    "\n",
    "# Get the Image Resolutions\n",
    "imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "img_meta = {}\n",
    "for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# Convert it to Dataframe and compute aspect ratio\n",
    "img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "img_meta_df.head()\n",
    "\n",
    "# Visualize Image Resolutions\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, picker=True)\n",
    "ax.set_title(\"Image Resolution\")\n",
    "ax.set_xlabel(\"Width\", size=14)\n",
    "ax.set_ylabel(\"Height\", size=14)\n",
    "\n",
    "# Visualize Image Resolutions\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "ax.set_title(\"Image Resolution\")\n",
    "ax.set_xlabel(\"Width\", size=14)\n",
    "ax.set_ylabel(\"Height\", size=14)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "        'viewItemURL', 'autoPay', 'postalCode', \n",
    "        'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "        'returnsAccepted', 'condition', 'topRatedListing',\n",
    "        'galleryPlusPictureURL', 'subtitle', 'discountPriceInfo',\n",
    "        'secondaryCategory']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# def prepare_df(df):\n",
    "#     price_list = []\n",
    "#     for row in full_dataset:\n",
    "#         listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#         price_list.append(listed_price)\n",
    "\n",
    "#     df['price_in_US'] = price_list\n",
    "#     #pull shipping cost from json dict with regex \n",
    "#     df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "#     df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#     #create new feature 'converted price'\n",
    "#     df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#     df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     display(df.head())\n",
    "#     display(df.info())\n",
    "#     return df\n",
    "\n",
    "def prepare_df(df):\n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    for row in full_dataset:\n",
    "        listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "        price_list.append(listed_price)\n",
    "\n",
    "        try:\n",
    "            listed_ship_price = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "\n",
    "        try:\n",
    "            condition = None\n",
    "            condition = np.float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    display(df.head())\n",
    "    display(df.info())\n",
    "    return df\n",
    "\n",
    "overhead_cost = 3\n",
    "def prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    "\n",
    "    #remove special characters\n",
    "    df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "    df[str(list(bucket_dict.keys())[bucket_dict_position])] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from base64 import b64encode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this function was used at first to check accuracy of calls using ebay API\n",
    "#the less complex \"prepare_brands\" function seems sufficient enough\n",
    "#it is redudant to use regex with working ebay API call\n",
    "\n",
    "def modified_prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    "\n",
    "    #remove special characters\n",
    "    df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "    pattern = re.compile(list(bucket_dict.keys())[bucket_dict_position])\n",
    "    df[list(bucket_dict.keys())[bucket_dict_position]] = df_bench.title.apply(lambda x: re.sub(pattern, x, str(list(bucket_dict.values())[bucket_dict_position])))\n",
    "\n",
    "    df[list(bucket_dict.keys())[bucket_dict_position]] = df[list(bucket_dict.keys())[bucket_dict_position]].apply(lambda x: np.float(x))\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "\n",
    "    return df\n",
    "\n",
    "request = {\n",
    "           'categoryId': 48818,\n",
    "            'itemFilter': [\n",
    "                            {'name': 'Condition', 'value': 'Used'},\n",
    "                            {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                          ],\n",
    "            'aspectFilter': [\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Benchmade'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Buck'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Case'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'CRKT'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Leatherman'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Spyderco'},\n",
    "                             ],\n",
    "            'paginationInput': {\n",
    "                                'entriesPerPage': 100,\n",
    "                                'pageNumber': 1\n",
    "\n",
    "                                }}\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger error at page 23\n",
    "\n",
    "\n",
    "for page in range(1, 22):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "#                                 {'name': 'Condition', 'value': 'Used'},\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Benchmade'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_bench = pd.DataFrame(full_dataset)\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "df_bench = prepare_df(df_bench)\n",
    "\n",
    "df_bench = prepare_brands(df_bench, 0)\n",
    "\n",
    "df_bench['benchmade'].value_counts()\n",
    "\n",
    "df_bench.info()\n",
    "\n",
    "df_bench[df_bench['galleryPlusPictureURL'].notna()]['galleryPlusPictureURL'].sample(30).apply(print)\n",
    "\n",
    "df_bench['profit'].describe()\n",
    "\n",
    "df_bench['ROI'].describe()\n",
    "\n",
    "df_bench.to_csv('data/df_bench.csv', index=False)\n",
    "\n",
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $7\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger error at page 42\n",
    "\n",
    "\n",
    "for page in range(1, 41):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                 {'aspectName': 'Brand', 'aspectValueName': 'Buck'}],\n",
    "\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_buck = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_buck = prepare_df(df_buck)\n",
    "\n",
    "df_buck = prepare_brands(df_buck, 1)\n",
    "\n",
    "df_buck.info()\n",
    "\n",
    "df_buck['buck'].value_counts()\n",
    "\n",
    "df_buck['profit'].describe()\n",
    "\n",
    "df_buck['ROI'].describe()\n",
    "\n",
    "df_buck.to_csv('data/df_buck.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1, 100):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "#                               \n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Case XX'},\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Case'}\n",
    "                                 ],\n",
    "\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_case = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_case = prepare_df(df_case)\n",
    "\n",
    "df_case = prepare_brands(df_case, 2)\n",
    "\n",
    "df_case.info()\n",
    "\n",
    "df_case.to_csv('data/df_case.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 18\n",
    "\n",
    "\n",
    "for page in range(1, 17):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                    {'aspectName': 'Brand', 'aspectValueName': 'CRKT'}],\n",
    "                 \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_crkt = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_crkt = prepare_df(df_crkt)\n",
    "\n",
    "bucket_dict \n",
    "\n",
    "df_crkt = prepare_brands(df_crkt, 3)\n",
    "\n",
    "display(df_crkt.head())\n",
    "display(df_crkt.info())\n",
    "\n",
    "df_crkt.to_csv('data/df_crkt.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger threw error at page 70\n",
    "\n",
    "\n",
    "for page in range(1, 69):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'}\n",
    "                                ],\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_kershaw = pd.DataFrame(full_dataset)\n",
    "\n",
    "bucket_dict\n",
    "\n",
    "df_kershaw = prepare_df(df_kershaw)\n",
    "\n",
    "df_kershaw = prepare_brands(df_kershaw, 4)\n",
    "\n",
    "df_kershaw.info()\n",
    "\n",
    "df_kershaw.to_csv('data/df_kershaw.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 18\n",
    "\n",
    "\n",
    "for page in range(1, 17):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Leatherman'}],\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_leatherman = pd.DataFrame(full_dataset)\n",
    "\n",
    "bucket_dict\n",
    "\n",
    "df_leatherman = prepare_df(df_leatherman)\n",
    "\n",
    "df_leatherman = prepare_brands(df_leatherman, 5)\n",
    "\n",
    "df_leatherman.info()\n",
    "\n",
    "df_leatherman.to_csv('data/df_leatherman.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 72\n",
    "\n",
    "for page in range(1, 71):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Spyderco'}\n",
    "                              ],\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "df_spyderco = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_spyderco = prepare_df(df_spyderco)\n",
    "\n",
    "bucket_dict \n",
    "\n",
    "df_spyderco = prepare_brands(df_spyderco, 6)\n",
    "\n",
    "df_spyderco.info()\n",
    "\n",
    "df_spyderco.to_csv('data/df_spyderco.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "for page in range(1, 21):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'SOG'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_sog = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_sog = prepare_df(df_sog)\n",
    "\n",
    "bucket_dict['sog'] = 15.0\n",
    "\n",
    "df_sog = prepare_brands(df_sog, 8)\n",
    "\n",
    "df_sog.info()\n",
    "\n",
    "df_sog.to_csv('data/df_sog.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "for page in range(1, 100):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_victorinox = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_victorinox = prepare_df(df_victorinox)\n",
    "\n",
    "bucket_dict\n",
    "\n",
    "df_victorinox = prepare_brands(df_victorinox, 7)\n",
    "\n",
    "df_victorinox.info()\n",
    "\n",
    "df_victorinox.to_csv('data/df_victorinox.csv', index=False)\n",
    "\n",
    "# df_bench = pd.read_csv(\"data/df_bench.csv\")\n",
    "# df_buck = pd.read_csv(\"data/df_buck.csv\")\n",
    "# df_case = pd.read_csv(\"data/df_case.csv\")\n",
    "# df_crkt = pd.read_csv(\"data/df_crkt.csv\")\n",
    "# df_kershaw = pd.read_csv(\"data/df_kershaw.csv\")\n",
    "# df_leatherman = pd.read_csv(\"data/df_leatherman.csv\")\n",
    "# df_spyderco = pd.read_csv(\"data/df_spyderco.csv\")\n",
    "# df_victorinox = pd.read_csv(\"data/df_victorinox.csv\")\n",
    "\n",
    "df = pd.concat([df_bench,df_buck,df_case,df_crkt,df_kershaw,df_leatherman,df_spyderco,df_sog,df_victorinox])\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "df.to_csv(\"data/full_dataset2.csv\", index=False)\n",
    "\n",
    "After succesfully going through 10,000 items on ebay's website and extracting everything possible, there is still a little bit more extracting to do from the json dictionary before saving the dataframe again. We need to get the price of the knives out of the nested dictionary in the dataframe as well as the shipping cost. After that, I would like to create a new feature called\n",
    "**\"converted price,\" which is simply the price of the knife listed on the ebay's website plus shipping.**\n",
    "\n",
    "```\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "    price_list.append(listed_price)\n",
    "    \n",
    "df['price_in_US'] = price_list\n",
    "```\n",
    "\n",
    "```\n",
    "#atttempt to pull shipping cost from json dict\n",
    "shipping_cost_list = []\n",
    "for row in full_dataset:\n",
    "    shipping_cost = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "    shipping_cost_list.append(shipping_cost)\n",
    "    \n",
    "df['shipping_price'] = shipping_cost_list\n",
    "```\n",
    "\n",
    "```\n",
    "#pull shipping cost from json dict with regex \n",
    "df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#create new feature 'converted price'\n",
    "df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "df = df.drop_duplicates(subset=['title', 'galleryURL'], keep='first')\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "\n",
    "df.to_csv('data/full_dataset.csv', index=False)\n",
    "```\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* Were there variables you dropped or created?\n",
    "* How did you address missing values or outliers?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "\n",
    "\n",
    "# here you run your code to clean the data\n",
    "\n",
    "```\n",
    "import code.data_cleaning as dc\n",
    "\n",
    "full_dataset = dc.full_clean()\n",
    "```\n",
    "\n",
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "# here you run your code to model the data\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do in the future to improve this project?\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "# data_knife_dir = 'knife_images'\n",
    "# data_profit_dir = 'data/profit'\n",
    "# new_dir = 'split'\n",
    "\n",
    "# os.mkdir(new_dir)\n",
    "\n",
    "# train_folder = os.path.join(new_dir, 'train')\n",
    "# train_profit = os.path.join(train_folder, 'profit')\n",
    "# os.mkdir(train_folder)\n",
    "# os.mkdir(train_profit)\n",
    "\n",
    "# test_folder = os.path.join(new_dir, 'test')\n",
    "# test_profit = os.path.join(test_folder, 'profit')\n",
    "# os.mkdir(test_folder)\n",
    "# os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# val_folder = os.path.join(new_dir, 'validation')\n",
    "# val_profit = os.path.join(val_folder, 'profit')\n",
    "# os.mkdir(val_folder)\n",
    "# os.mkdir(val_profit)\n",
    "\n",
    "# val_profit\n",
    "\n",
    "# # train knife regression images\n",
    "# #80% of data\n",
    "# imgs = knife_images[:5620]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(train_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # test knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[5620:6322]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(test_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # validation knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[6322:]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(val, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "plt.imshow(img_rgb)  # graph it\n",
    "plt.show();\n",
    "\n",
    "\n",
    "def image_checker(index,):\n",
    "    img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "    plt.imshow(img_rgb)  # graph it\n",
    "    plt.show();\n",
    "\n",
    "top_benchmade_index[:50]\n",
    "\n",
    "image_checker(6158)\n",
    "\n",
    "image_checker(2286)\n",
    "\n",
    "image_checker(1879)\n",
    "\n",
    "image_checker(4326)\n",
    "\n",
    "image_checker(6094)\n",
    "\n",
    "#final processing steps for images\n",
    "\n",
    "image_list = []\n",
    "for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "    img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "    img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "    image_list.append(img_rgb)\n",
    "   \n",
    "    # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n",
    "\n",
    "\n",
    "df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())\n",
    "\n",
    "df_CNN_regression['mean_profit'].describe()\n",
    "\n",
    "X = np.array(image_list)\n",
    "\n",
    "y=  df_CNN_regression['mean_profit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets\n",
    "\n",
    "X.shape\n",
    "\n",
    "y.shape\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#small batch\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256, 3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=30,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "df_scrub['profit'].mean()\n",
    "\n",
    "2.2164 * 41.374303202846974\n",
    "\n",
    "df_scrub.head()\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();\n",
    "\n",
    "model.save('my_model_batch32.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#a train set of 60% and a val and test size of 20% each \n",
    "\n",
    "#this model showed a lot of indication that it was overfit\n",
    "#need to retry how I split the data \n",
    "#Instead of manul indexing, will use \n",
    "# from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X[:4918]\n",
    "# y_train = y[:4918]\n",
    "\n",
    "# X_train = X[4918:5971]\n",
    "# y_train = y[4918:5971]\n",
    "\n",
    "# X_test = X[5971:]\n",
    "# y_test = y[5971:]\n",
    "\n",
    "\n",
    "# display(len(X_val)/len(X))\n",
    "# display(len(X_train)/len(X))\n",
    "# len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(224 ,224,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=32,\n",
    "#                     batch_size=300,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')\n",
    "\n",
    "results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# model.save('my_model_batch500.h5')\n",
    "\n",
    "history.history.keys()\n",
    "\n",
    "#The model is showing a lot of signs of overfitting \n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
