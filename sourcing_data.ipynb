{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.finding import Connection\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebaysdk.shopping import Connection as Shopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import  json\n",
    "import numpy as np\n",
    "import re\n",
    "import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create function for organizing API call\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['itemId', 'title', 'galleryURL', \n",
    "#         'viewItemURL', 'autoPay', 'postalCode', \n",
    "#         'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "#         'returnsAccepted', 'condition', 'topRatedListing',\n",
    "#         'galleryPlusPictureURL']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         # Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_df(df):\n",
    "#     price_list = []\n",
    "#     for row in full_dataset:\n",
    "#         listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#         price_list.append(listed_price)\n",
    "\n",
    "#     df['price_in_US'] = price_list\n",
    "#     #pull shipping cost from json dict with regex \n",
    "#     df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "#     df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#     #create new feature 'converted price'\n",
    "#     df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#     df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     display(df.head())\n",
    "#     display(df.info())\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from base64 import b64encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #this function was used at first to check accuracy of calls using ebay API\n",
    "# #the less complex \"prepare_brands\" function seems sufficient enough\n",
    "# #it is redudant to use regex with working ebay API call\n",
    "\n",
    "# def modified_prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "#     df.title = df.title.apply(str.lower)\n",
    "\n",
    "#     #remove special characters\n",
    "#     df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "#     pattern = re.compile(list(bucket_dict.keys())[bucket_dict_position])\n",
    "#     df[list(bucket_dict.keys())[bucket_dict_position]] = df_bench.title.apply(lambda x: re.sub(pattern, x, str(list(bucket_dict.values())[bucket_dict_position])))\n",
    "\n",
    "#     df[list(bucket_dict.keys())[bucket_dict_position]] = df[list(bucket_dict.keys())[bucket_dict_position]].apply(lambda x: np.float(x))\n",
    "    \n",
    "#     df['profit'] = (df['converted_price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "#     df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    condition = None\n",
    "    for row in full_dataset:\n",
    "        listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "        price_list.append(listed_price)\n",
    "     \n",
    "        try:\n",
    "            listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "\n",
    "        try:\n",
    "            condition = float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request = {\n",
    "#            'categoryId': 48818,\n",
    "#             'itemFilter': [\n",
    "#                             {'name': 'Condition', 'value': 'Used'},\n",
    "#                             {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "#                           ],\n",
    "#             'aspectFilter': [\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Benchmade'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Buck'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Case'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'CRKT'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Leatherman'},\n",
    "#                               {'aspectName': 'Brand', 'aspectValueName': 'Spyderco'},\n",
    "#                              ],\n",
    "#             'paginationInput': {\n",
    "#                                 'entriesPerPage': 100,\n",
    "#                                 'pageNumber': 1\n",
    "\n",
    "#                                 }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0,\n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "overhead_cost = 3\n",
    "def prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    " \n",
    "    #remove special characters\n",
    "    df.title.apply(pp.remove_special_chars)\n",
    "    df['brand'] = str(list(bucket_dict.keys())[bucket_dict_position])\n",
    "    df['cost'] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    df['profit'] = (df['converted_price'] -  df['cost'] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/( df['cost'] + overhead_cost))*100.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "        'viewItemURL', 'autoPay', 'postalCode', \n",
    "        'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "        'returnsAccepted', 'condition', 'topRatedListing',\n",
    "        'galleryPlusPictureURL']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "\n",
    "def knife_request(Brand, dict_pos):\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "    #                                 {'name': 'Condition', 'value': 'Used'},\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': 1\n",
    "\n",
    "                                    },\n",
    "\n",
    "                }\n",
    "\n",
    "    #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "\n",
    "    response_pages = response.dict()\n",
    "\n",
    "    full_dataset = []\n",
    "    \n",
    "    total_pages = int(response_pages['paginationOutput']['totalPages'])\n",
    "\n",
    "    if total_pages > 100:\n",
    "        pages_to_request = 100\n",
    "        \n",
    "    else:\n",
    "        pages_to_request = total_pages - 1\n",
    "        \n",
    "        \n",
    "\n",
    "    for page in range(1, pages_to_request):\n",
    "        # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "        # Make the query and get the response\n",
    "\n",
    "        api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "        request = {\n",
    "                    'categoryId': 48818,\n",
    "                    'itemFilter': [\n",
    "    #                                 {'name': 'Condition', 'value': 'Used'},\n",
    "                                    {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                                  ],\n",
    "                    'aspectFilter': [\n",
    "                                      {'aspectName': 'Brand', 'aspectValueName': Brand}],\n",
    "\n",
    "\n",
    "                    'paginationInput': {\n",
    "                                        'entriesPerPage': 100,\n",
    "                                        'pageNumber': page\n",
    "\n",
    "                                        },\n",
    "\n",
    "                    }\n",
    "\n",
    "\n",
    "        response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "        #save the response as a json dict\n",
    "        response_dict = response.dict()\n",
    "\n",
    "\n",
    "        #index dict to appropriate index\n",
    "        results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "        # Call the prepare_data function to get a list of processed data\n",
    "        prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "        # Extend full_dataset with this list (don't append, or you'll get\n",
    "        # a list of lists instead of a flat list)\n",
    "        full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "    display(len(full_dataset))\n",
    "    \n",
    "    df = pd.DataFrame(full_dataset)\n",
    "    \n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    condition = None\n",
    "    for row in full_dataset:\n",
    "        try:\n",
    "            listed_price = float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "            price_list.append(listed_price)\n",
    "        except:\n",
    "            listed_price = \"Na\"\n",
    "            price_list.append(listed_price)\n",
    "        try:\n",
    "            listed_ship_price = float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        try:\n",
    "            condition = float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df = prepare_brands(df, dict_pos)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25870"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bench_df = knife_request('Benchmade', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1974 entries, 0 to 1973\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 1974 non-null   object \n",
      " 1   title                  1974 non-null   object \n",
      " 2   galleryURL             1972 non-null   object \n",
      " 3   viewItemURL            1974 non-null   object \n",
      " 4   autoPay                1974 non-null   object \n",
      " 5   postalCode             1862 non-null   object \n",
      " 6   sellingStatus          1974 non-null   object \n",
      " 7   shippingInfo           1974 non-null   object \n",
      " 8   listingInfo            1974 non-null   object \n",
      " 9   returnsAccepted        1974 non-null   object \n",
      " 10  condition              1974 non-null   float64\n",
      " 11  topRatedListing        1974 non-null   object \n",
      " 12  galleryPlusPictureURL  236 non-null    object \n",
      " 13  shipping_cost          1974 non-null   float64\n",
      " 14  price_in_US            1974 non-null   float64\n",
      " 15  converted_price        1974 non-null   float64\n",
      " 16  brand                  1974 non-null   object \n",
      " 17  cost                   1974 non-null   float64\n",
      " 18  profit                 1974 non-null   float64\n",
      " 19  ROI                    1974 non-null   float64\n",
      "dtypes: float64(7), object(13)\n",
      "memory usage: 308.6+ KB\n"
     ]
    }
   ],
   "source": [
    "bench_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_df.to_csv('data/df_bench1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmade': 45.0,\n",
       " 'buck': 20.0,\n",
       " 'case': 20.0,\n",
       " 'crkt': 15.0,\n",
       " 'kershaw': 15.0,\n",
       " 'leatherman': 30.0,\n",
       " 'sog': 15.0,\n",
       " 'spyderco': 30.0,\n",
       " 'victorinox': 20.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "buck_df = knife_request('Buck', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2940 entries, 0 to 2939\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 2940 non-null   object \n",
      " 1   title                  2940 non-null   object \n",
      " 2   galleryURL             2937 non-null   object \n",
      " 3   viewItemURL            2940 non-null   object \n",
      " 4   autoPay                2940 non-null   object \n",
      " 5   postalCode             2896 non-null   object \n",
      " 6   sellingStatus          2940 non-null   object \n",
      " 7   shippingInfo           2940 non-null   object \n",
      " 8   listingInfo            2940 non-null   object \n",
      " 9   returnsAccepted        2940 non-null   object \n",
      " 10  condition              2940 non-null   float64\n",
      " 11  topRatedListing        2940 non-null   object \n",
      " 12  galleryPlusPictureURL  347 non-null    object \n",
      " 13  shipping_cost          2940 non-null   float64\n",
      " 14  price_in_US            2940 non-null   float64\n",
      " 15  converted_price        2940 non-null   float64\n",
      " 16  brand                  2940 non-null   object \n",
      " 17  cost                   2940 non-null   float64\n",
      " 18  profit                 2940 non-null   float64\n",
      " 19  ROI                    2940 non-null   float64\n",
      "dtypes: float64(7), object(13)\n",
      "memory usage: 459.5+ KB\n"
     ]
    }
   ],
   "source": [
    "buck_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "buck_df.to_csv('data/df_buck.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128700"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case_df = knife_request('Case', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9684 entries, 0 to 9683\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 9684 non-null   object \n",
      " 1   title                  9684 non-null   object \n",
      " 2   galleryURL             9427 non-null   object \n",
      " 3   viewItemURL            9684 non-null   object \n",
      " 4   autoPay                9684 non-null   object \n",
      " 5   postalCode             9515 non-null   object \n",
      " 6   sellingStatus          9684 non-null   object \n",
      " 7   shippingInfo           9684 non-null   object \n",
      " 8   listingInfo            9684 non-null   object \n",
      " 9   returnsAccepted        9684 non-null   object \n",
      " 10  condition              9684 non-null   float64\n",
      " 11  topRatedListing        9684 non-null   object \n",
      " 12  galleryPlusPictureURL  4234 non-null   object \n",
      " 13  shipping_cost          9684 non-null   float64\n",
      " 14  price_in_US            9684 non-null   float64\n",
      " 15  converted_price        9684 non-null   float64\n",
      " 16  brand                  9684 non-null   object \n",
      " 17  cost                   9684 non-null   float64\n",
      " 18  profit                 9684 non-null   float64\n",
      " 19  ROI                    9684 non-null   float64\n",
      "dtypes: float64(7), object(13)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "case_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df.to_csv('data/df_case.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107900"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_caseXX = knife_request('Case XX', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8077 entries, 0 to 8076\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 8077 non-null   object \n",
      " 1   title                  8077 non-null   object \n",
      " 2   galleryURL             8070 non-null   object \n",
      " 3   viewItemURL            8077 non-null   object \n",
      " 4   autoPay                8077 non-null   object \n",
      " 5   postalCode             7533 non-null   object \n",
      " 6   sellingStatus          8077 non-null   object \n",
      " 7   shippingInfo           8077 non-null   object \n",
      " 8   listingInfo            8077 non-null   object \n",
      " 9   returnsAccepted        8077 non-null   object \n",
      " 10  condition              8077 non-null   float64\n",
      " 11  topRatedListing        8077 non-null   object \n",
      " 12  galleryPlusPictureURL  2056 non-null   object \n",
      " 13  shipping_cost          8077 non-null   float64\n",
      " 14  price_in_US            8077 non-null   float64\n",
      " 15  converted_price        8077 non-null   float64\n",
      " 16  brand                  8077 non-null   object \n",
      " 17  cost                   8077 non-null   float64\n",
      " 18  profit                 8077 non-null   float64\n",
      " 19  ROI                    8077 non-null   float64\n",
      "dtypes: float64(7), object(13)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_caseXX.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caseXX.to_csv('data/df_CaseXX.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmade': 45.0,\n",
       " 'buck': 20.0,\n",
       " 'case': 20.0,\n",
       " 'crkt': 15.0,\n",
       " 'kershaw': 15.0,\n",
       " 'leatherman': 30.0,\n",
       " 'sog': 15.0,\n",
       " 'spyderco': 30.0,\n",
       " 'victorinox': 20.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_crkt = knife_request(\"CRKT\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crkt.to_csv('data/df_crkt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-51c443a96074>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_kershaw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknife_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Kershaw'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-62586568e37f>\u001b[0m in \u001b[0;36mknife_request\u001b[1;34m(Brand, dict_pos)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;31m#index dict to appropriate index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mresults_list_of_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'searchResult'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'item'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# Call the prepare_data function to get a list of processed data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'item'"
     ]
    }
   ],
   "source": [
    "df_kershaw = knife_request('Kershaw', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for page in range(1, 58):\n",
    "#         # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "#         # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "    #                                 {'name': 'Condition', 'value': 'Used'},\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'}],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                        }\n",
    "\n",
    "        #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "    # Check the length of the full dataset. It will be up to `total`,\n",
    "    # potentially less if there were missing values\n",
    "\n",
    "    df = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemId</th>\n",
       "      <th>title</th>\n",
       "      <th>galleryURL</th>\n",
       "      <th>viewItemURL</th>\n",
       "      <th>autoPay</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>sellingStatus</th>\n",
       "      <th>shippingInfo</th>\n",
       "      <th>listingInfo</th>\n",
       "      <th>returnsAccepted</th>\n",
       "      <th>condition</th>\n",
       "      <th>topRatedListing</th>\n",
       "      <th>galleryPlusPictureURL</th>\n",
       "      <th>shipping_cost</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>converted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>234780801512</td>\n",
       "      <td>Kershaw Tarheel 1364 Tactical Liner Lock Foldi...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/zJcAAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/Kershaw-Tarheel-1364-...</td>\n",
       "      <td>false</td>\n",
       "      <td>373**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>true</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.69</td>\n",
       "      <td>18.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>283684293533</td>\n",
       "      <td>1660 Kershaw Leek Knife silver plain Blade New...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/VQ0AAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/1660-Kershaw-Leek-Kni...</td>\n",
       "      <td>true</td>\n",
       "      <td>982**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>true</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.95</td>\n",
       "      <td>41.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195454753696</td>\n",
       "      <td>Kershaw Damascus Skyline 1760DAM! Discontinued...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/itIAAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/Kershaw-Damascus-Skyl...</td>\n",
       "      <td>false</td>\n",
       "      <td>923**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>true</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>275535106552</td>\n",
       "      <td>Kershaw Zing Framelock Knife Folding Blade Sta...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/grcAAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/Kershaw-Zing-Frameloc...</td>\n",
       "      <td>true</td>\n",
       "      <td>125**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>true</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.99</td>\n",
       "      <td>19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175006871402</td>\n",
       "      <td>Kershaw Kuro Assisted Folding Knife 3.1\" 8Cr13...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/2o4AAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/Kershaw-Kuro-Assisted...</td>\n",
       "      <td>true</td>\n",
       "      <td>243**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>true</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>true</td>\n",
       "      <td>https://galleryplus.ebayimg.com/ws/web/1750068...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.39</td>\n",
       "      <td>25.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         itemId                                              title                                         galleryURL                                        viewItemURL autoPay postalCode                                      sellingStatus                                       shippingInfo                                        listingInfo returnsAccepted  condition topRatedListing                              galleryPlusPictureURL  shipping_cost  price_in_US  converted_price\n",
       "0  234780801512  Kershaw Tarheel 1364 Tactical Liner Lock Foldi...  https://i.ebayimg.com/thumbs/images/g/zJcAAOSw...  https://www.ebay.com/itm/Kershaw-Tarheel-1364-...   false      373**  {'currentPrice': {'_currencyId': 'USD', 'value...  {'shippingServiceCost': {'_currencyId': 'USD',...  {'bestOfferEnabled': 'false', 'buyItNowAvailab...            true     1000.0            true                                               None            0.0        18.69            18.69\n",
       "1  283684293533  1660 Kershaw Leek Knife silver plain Blade New...  https://i.ebayimg.com/thumbs/images/g/VQ0AAOSw...  https://www.ebay.com/itm/1660-Kershaw-Leek-Kni...    true      982**  {'currentPrice': {'_currencyId': 'USD', 'value...  {'shippingServiceCost': {'_currencyId': 'USD',...  {'bestOfferEnabled': 'false', 'buyItNowAvailab...            true     1000.0            true                                               None            0.0        41.95            41.95\n",
       "2  195454753696  Kershaw Damascus Skyline 1760DAM! Discontinued...  https://i.ebayimg.com/thumbs/images/g/itIAAOSw...  https://www.ebay.com/itm/Kershaw-Damascus-Skyl...   false      923**  {'currentPrice': {'_currencyId': 'USD', 'value...  {'shippingServiceCost': {'_currencyId': 'USD',...  {'bestOfferEnabled': 'false', 'buyItNowAvailab...            true     1000.0            true                                               None            0.0       100.00           100.00\n",
       "3  275535106552  Kershaw Zing Framelock Knife Folding Blade Sta...  https://i.ebayimg.com/thumbs/images/g/grcAAOSw...  https://www.ebay.com/itm/Kershaw-Zing-Frameloc...    true      125**  {'currentPrice': {'_currencyId': 'USD', 'value...  {'shippingServiceCost': {'_currencyId': 'USD',...  {'bestOfferEnabled': 'false', 'buyItNowAvailab...            true     1000.0           false                                               None            0.0        19.99            19.99\n",
       "4  175006871402  Kershaw Kuro Assisted Folding Knife 3.1\" 8Cr13...  https://i.ebayimg.com/thumbs/images/g/2o4AAOSw...  https://www.ebay.com/itm/Kershaw-Kuro-Assisted...    true      243**  {'currentPrice': {'_currencyId': 'USD', 'value...  {'shippingServiceCost': {'_currencyId': 'USD',...  {'bestOfferEnabled': 'false', 'buyItNowAvailab...            true     1000.0            true  https://galleryplus.ebayimg.com/ws/web/1750068...            0.0        25.39            25.39"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5587 entries, 0 to 5586\n",
      "Data columns (total 16 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 5587 non-null   object \n",
      " 1   title                  5587 non-null   object \n",
      " 2   galleryURL             5586 non-null   object \n",
      " 3   viewItemURL            5587 non-null   object \n",
      " 4   autoPay                5587 non-null   object \n",
      " 5   postalCode             5452 non-null   object \n",
      " 6   sellingStatus          5587 non-null   object \n",
      " 7   shippingInfo           5587 non-null   object \n",
      " 8   listingInfo            5587 non-null   object \n",
      " 9   returnsAccepted        5587 non-null   object \n",
      " 10  condition              5587 non-null   float64\n",
      " 11  topRatedListing        5587 non-null   object \n",
      " 12  galleryPlusPictureURL  1014 non-null   object \n",
      " 13  shipping_cost          5587 non-null   float64\n",
      " 14  price_in_US            5587 non-null   float64\n",
      " 15  converted_price        5587 non-null   float64\n",
      "dtypes: float64(4), object(12)\n",
      "memory usage: 698.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_kershaw = prepare_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kershaw.to_csv('data/df_kershaw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2522"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_leatherman = knife_request('Leatherman', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 194 entries, 0 to 193\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 194 non-null    object \n",
      " 1   title                  194 non-null    object \n",
      " 2   galleryURL             194 non-null    object \n",
      " 3   viewItemURL            194 non-null    object \n",
      " 4   autoPay                194 non-null    object \n",
      " 5   postalCode             174 non-null    object \n",
      " 6   sellingStatus          194 non-null    object \n",
      " 7   shippingInfo           194 non-null    object \n",
      " 8   listingInfo            194 non-null    object \n",
      " 9   returnsAccepted        194 non-null    object \n",
      " 10  condition              194 non-null    float64\n",
      " 11  topRatedListing        194 non-null    object \n",
      " 12  galleryPlusPictureURL  24 non-null     object \n",
      " 13  shipping_cost          194 non-null    float64\n",
      " 14  price_in_US            194 non-null    float64\n",
      " 15  converted_price        194 non-null    float64\n",
      " 16  brand                  194 non-null    object \n",
      " 17  cost                   194 non-null    float64\n",
      " 18  profit                 194 non-null    float64\n",
      " 19  ROI                    194 non-null    float64\n",
      "dtypes: float64(7), object(13)\n",
      "memory usage: 30.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_leatherman.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leatherman.to_csv('data/df_leatherman.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16900"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sog = knife_request('SOG', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1282 entries, 0 to 1281\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 1282 non-null   object \n",
      " 1   title                  1282 non-null   object \n",
      " 2   galleryURL             1282 non-null   object \n",
      " 3   viewItemURL            1282 non-null   object \n",
      " 4   autoPay                1282 non-null   object \n",
      " 5   postalCode             1237 non-null   object \n",
      " 6   sellingStatus          1282 non-null   object \n",
      " 7   shippingInfo           1282 non-null   object \n",
      " 8   listingInfo            1282 non-null   object \n",
      " 9   returnsAccepted        1282 non-null   object \n",
      " 10  condition              1282 non-null   float64\n",
      " 11  topRatedListing        1282 non-null   object \n",
      " 12  galleryPlusPictureURL  270 non-null    object \n",
      " 13  shipping_cost          1282 non-null   float64\n",
      " 14  price_in_US            1282 non-null   float64\n",
      " 15  converted_price        1282 non-null   float64\n",
      " 16  brand                  1282 non-null   object \n",
      " 17  cost                   1282 non-null   float64\n",
      " 18  profit                 1282 non-null   float64\n",
      " 19  ROI                    1282 non-null   float64\n",
      "dtypes: float64(7), object(13)\n",
      "memory usage: 200.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_sog.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sog.to_csv('data/df_sog.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103090"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_spyderco = knife_request('Spyderco', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7768 entries, 0 to 7767\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 7768 non-null   object \n",
      " 1   title                  7768 non-null   object \n",
      " 2   galleryURL             7765 non-null   object \n",
      " 3   viewItemURL            7768 non-null   object \n",
      " 4   autoPay                7768 non-null   object \n",
      " 5   postalCode             7374 non-null   object \n",
      " 6   sellingStatus          7768 non-null   object \n",
      " 7   shippingInfo           7768 non-null   object \n",
      " 8   listingInfo            7768 non-null   object \n",
      " 9   returnsAccepted        7768 non-null   object \n",
      " 10  condition              7768 non-null   float64\n",
      " 11  topRatedListing        7768 non-null   object \n",
      " 12  galleryPlusPictureURL  1854 non-null   object \n",
      " 13  shipping_cost          7768 non-null   float64\n",
      " 14  price_in_US            7768 non-null   float64\n",
      " 15  converted_price        7768 non-null   float64\n",
      " 16  brand                  7768 non-null   object \n",
      " 17  cost                   7768 non-null   float64\n",
      " 18  profit                 7768 non-null   float64\n",
      " 19  ROI                    7768 non-null   float64\n",
      "dtypes: float64(7), object(13)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_spyderco.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spyderco.to_csv('data/df_spyderco.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pages: 97\n"
     ]
    }
   ],
   "source": [
    "# full_dataset = []\n",
    "# for page in range(1, 58):\n",
    "#         # Add or update the \"offset\" key-value pair in url_params\n",
    "\n",
    "#         # Make the query and get the response\n",
    "\n",
    "api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "request = {\n",
    "            'categoryId': 48818,\n",
    "            'itemFilter': [\n",
    "#                                 {'name': 'Condition', 'value': 'Used'},\n",
    "                            {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                          ],\n",
    "            'aspectFilter': [\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "\n",
    "            'paginationInput': {\n",
    "                                'entriesPerPage': 100,\n",
    "                                'pageNumber': 1\n",
    "\n",
    "                                },\n",
    "\n",
    "                    }\n",
    "\n",
    "    #     request['paginationInput']['pageNumber'] = page\n",
    "\n",
    "response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "#save the response as a json dict\n",
    "response_dict = response.dict()\n",
    "\n",
    "tot_pages = response_dict['paginationOutput']['totalPages']\n",
    "\n",
    "print(f\"total pages: {tot_pages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = []\n",
    "for page in range(1, 86):\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=False, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "    #                                 {'name': 'Condition', 'value': 'Used'},\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "\n",
    "                                    },\n",
    "\n",
    "                        }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    response_dict = response.dict()\n",
    "\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    full_dataset.extend(prepared_knives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 104533 entries, 0 to 104532\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count   Dtype \n",
      "---  ------                 --------------   ----- \n",
      " 0   itemId                 104533 non-null  object\n",
      " 1   title                  104533 non-null  object\n",
      " 2   galleryURL             104507 non-null  object\n",
      " 3   viewItemURL            104533 non-null  object\n",
      " 4   autoPay                104533 non-null  object\n",
      " 5   postalCode             92768 non-null   object\n",
      " 6   sellingStatus          104533 non-null  object\n",
      " 7   shippingInfo           104533 non-null  object\n",
      " 8   listingInfo            104533 non-null  object\n",
      " 9   returnsAccepted        104533 non-null  object\n",
      " 10  condition              101725 non-null  object\n",
      " 11  topRatedListing        104533 non-null  object\n",
      " 12  galleryPlusPictureURL  16848 non-null   object\n",
      "dtypes: object(13)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemId</th>\n",
       "      <th>title</th>\n",
       "      <th>galleryURL</th>\n",
       "      <th>viewItemURL</th>\n",
       "      <th>autoPay</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>sellingStatus</th>\n",
       "      <th>shippingInfo</th>\n",
       "      <th>listingInfo</th>\n",
       "      <th>returnsAccepted</th>\n",
       "      <th>condition</th>\n",
       "      <th>topRatedListing</th>\n",
       "      <th>galleryPlusPictureURL</th>\n",
       "      <th>shipping_cost</th>\n",
       "      <th>price_in_US</th>\n",
       "      <th>converted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165614958701</td>\n",
       "      <td>Victorinox Classic SD Mini Swiss Army Pocket K...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/V8MAAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/Victorinox-Classic-SD...</td>\n",
       "      <td>true</td>\n",
       "      <td>338**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>false</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>8.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>193932638989</td>\n",
       "      <td>NEW Victorinox Super Tinker Red Multifunction ...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/60EAAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/NEW-Victorinox-Super-...</td>\n",
       "      <td>true</td>\n",
       "      <td>331**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>true</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.99</td>\n",
       "      <td>29.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204160933903</td>\n",
       "      <td>Victorinox Classic SD Mini Swiss Army Pocket K...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/7Q8AAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/Victorinox-Classic-SD...</td>\n",
       "      <td>true</td>\n",
       "      <td>492**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>true</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>true</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.25</td>\n",
       "      <td>9.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154848639295</td>\n",
       "      <td>Victorinox Swiss Army 54111 Classic SD Red Wit...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/pB8AAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/Victorinox-Swiss-Army...</td>\n",
       "      <td>false</td>\n",
       "      <td>775**</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>false</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.99</td>\n",
       "      <td>20.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>222775594040</td>\n",
       "      <td>SWISS ARMY KNIFE VICTORINOX 91mm SCALES/HANDLE...</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/NicAAOSw...</td>\n",
       "      <td>https://www.ebay.com/itm/SWISS-ARMY-KNIFE-VICT...</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>{'currentPrice': {'_currencyId': 'USD', 'value...</td>\n",
       "      <td>{'shippingServiceCost': {'_currencyId': 'USD',...</td>\n",
       "      <td>{'bestOfferEnabled': 'false', 'buyItNowAvailab...</td>\n",
       "      <td>true</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>false</td>\n",
       "      <td>None</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2.80</td>\n",
       "      <td>7.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         itemId                                              title                                         galleryURL                                        viewItemURL autoPay postalCode                                      sellingStatus                                       shippingInfo                                        listingInfo returnsAccepted  condition topRatedListing galleryPlusPictureURL  shipping_cost  price_in_US  converted_price\n",
       "0  165614958701  Victorinox Classic SD Mini Swiss Army Pocket K...  https://i.ebayimg.com/thumbs/images/g/V8MAAOSw...  https://www.ebay.com/itm/Victorinox-Classic-SD...    true      338**  {'currentPrice': {'_currencyId': 'USD', 'value...  {'shippingServiceCost': {'_currencyId': 'USD',...  {'bestOfferEnabled': 'false', 'buyItNowAvailab...           false     3000.0           false                  None            3.5         5.00             8.50\n",
       "1  193932638989  NEW Victorinox Super Tinker Red Multifunction ...  https://i.ebayimg.com/thumbs/images/g/60EAAOSw...  https://www.ebay.com/itm/NEW-Victorinox-Super-...    true      331**  {'currentPrice': {'_currencyId': 'USD', 'value...  {'shippingServiceCost': {'_currencyId': 'USD',...  {'bestOfferEnabled': 'false', 'buyItNowAvailab...            true     1000.0            true                  None            0.0        29.99            29.99\n",
       "2  204160933903  Victorinox Classic SD Mini Swiss Army Pocket K...  https://i.ebayimg.com/thumbs/images/g/7Q8AAOSw...  https://www.ebay.com/itm/Victorinox-Classic-SD...    true      492**  {'currentPrice': {'_currencyId': 'USD', 'value...  {'shippingServiceCost': {'_currencyId': 'USD',...  {'bestOfferEnabled': 'false', 'buyItNowAvailab...            true     3000.0            true                  None            0.0         9.25             9.25\n",
       "3  154848639295  Victorinox Swiss Army 54111 Classic SD Red Wit...  https://i.ebayimg.com/thumbs/images/g/pB8AAOSw...  https://www.ebay.com/itm/Victorinox-Swiss-Army...   false      775**  {'currentPrice': {'_currencyId': 'USD', 'value...  {'shippingServiceCost': {'_currencyId': 'USD',...  {'bestOfferEnabled': 'false', 'buyItNowAvailab...           false     1000.0           false                  None            0.0        20.99            20.99\n",
       "4  222775594040  SWISS ARMY KNIFE VICTORINOX 91mm SCALES/HANDLE...  https://i.ebayimg.com/thumbs/images/g/NicAAOSw...  https://www.ebay.com/itm/SWISS-ARMY-KNIFE-VICT...   false       None  {'currentPrice': {'_currencyId': 'USD', 'value...  {'shippingServiceCost': {'_currencyId': 'USD',...  {'bestOfferEnabled': 'false', 'buyItNowAvailab...            true     1000.0           false                  None            4.8         2.80             7.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7979 entries, 0 to 7978\n",
      "Data columns (total 16 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 7979 non-null   object \n",
      " 1   title                  7979 non-null   object \n",
      " 2   galleryURL             7977 non-null   object \n",
      " 3   viewItemURL            7979 non-null   object \n",
      " 4   autoPay                7979 non-null   object \n",
      " 5   postalCode             7079 non-null   object \n",
      " 6   sellingStatus          7979 non-null   object \n",
      " 7   shippingInfo           7979 non-null   object \n",
      " 8   listingInfo            7979 non-null   object \n",
      " 9   returnsAccepted        7979 non-null   object \n",
      " 10  condition              7979 non-null   float64\n",
      " 11  topRatedListing        7979 non-null   object \n",
      " 12  galleryPlusPictureURL  1285 non-null   object \n",
      " 13  shipping_cost          7979 non-null   float64\n",
      " 14  price_in_US            7979 non-null   float64\n",
      " 15  converted_price        7979 non-null   float64\n",
      "dtypes: float64(4), object(12)\n",
      "memory usage: 997.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_victorinox = prepare_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7979 entries, 0 to 7978\n",
      "Data columns (total 16 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 7979 non-null   object \n",
      " 1   title                  7979 non-null   object \n",
      " 2   galleryURL             7977 non-null   object \n",
      " 3   viewItemURL            7979 non-null   object \n",
      " 4   autoPay                7979 non-null   object \n",
      " 5   postalCode             7079 non-null   object \n",
      " 6   sellingStatus          7979 non-null   object \n",
      " 7   shippingInfo           7979 non-null   object \n",
      " 8   listingInfo            7979 non-null   object \n",
      " 9   returnsAccepted        7979 non-null   object \n",
      " 10  condition              7979 non-null   float64\n",
      " 11  topRatedListing        7979 non-null   object \n",
      " 12  galleryPlusPictureURL  1285 non-null   object \n",
      " 13  shipping_cost          7979 non-null   float64\n",
      " 14  price_in_US            7979 non-null   float64\n",
      " 15  converted_price        7979 non-null   float64\n",
      "dtypes: float64(4), object(12)\n",
      "memory usage: 997.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_victorinox.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_victorinox.to_csv('data/df_victo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bench = pd.read_csv(\"data/df_bench.csv\")\n",
    "# df_buck = pd.read_csv(\"data/df_buck.csv\")\n",
    "# df_case = pd.read_csv(\"data/df_case.csv\")\n",
    "# df_crkt = pd.read_csv(\"data/df_crkt.csv\")\n",
    "# df_kershaw = pd.read_csv(\"data/df_kershaw.csv\")\n",
    "# df_leatherman = pd.read_csv(\"data/df_leatherman.csv\")\n",
    "# df_spyderco = pd.read_csv(\"data/df_spyderco.csv\")\n",
    "# df_victorinox = pd.read_csv(\"data/df_victorinox.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_bench,df_buck,df_case,df_crkt,df_kershaw,df_leatherman,df_spyderco,df_sog,df_victorinox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/full_dataset2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After succesfully going through 10,000 items on ebay's website and extracting everything possible, there is still a little bit more extracting to do from the json dictionary before saving the dataframe again. We need to get the price of the knives out of the nested dictionary in the dataframe as well as the shipping cost. After that, I would like to create a new feature called\n",
    "**\"converted price,\" which is simply the price of the knife listed on the ebay's website plus shipping.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tera_df_prepared.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "request = {\n",
    "           'itemID': list(itemIds),\n",
    "           'IncludeSelector': 'ItemSpecifics'\n",
    "          }\n",
    "response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "response_dict = response.dict()\n",
    "\n",
    "results_list_of_dicts = response_dict['Item']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = response.dict()\n",
    "\n",
    "    #index dict to appropriate index\n",
    "# results_list_of_dicts = response_dict['searchResult']['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_of_dicts = response_dict['Item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create function for organizing API call\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "#                 'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "#                 'Country', 'Location', 'ConvertedCurrentPrice',\n",
    "#                 'PictureURL']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "# #         Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create function for organizing API call\n",
    "# def prepare_data(data_list):\n",
    "#     \"\"\"\n",
    "#     This function takes in a list of dictionaries and prepares it\n",
    "#     for analysis\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Make a new list to hold results\n",
    "#     results = []\n",
    "    \n",
    "#     for business_data in data_list:\n",
    "    \n",
    "#         # Make a new dictionary to hold prepared data for this business\n",
    "#         prepared_data = {}\n",
    "        \n",
    "#         # Extract name, review_count, rating, and price key-value pairs\n",
    "#         # from business_data and add to prepared_data\n",
    "#         # If a key is not present in business_data, add it to prepared_data\n",
    "#         # with an associated value of None\n",
    "        \n",
    "#         keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "#                 'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "\n",
    "#                 'PictureURL']\n",
    "        \n",
    "#         for key in keys:\n",
    "#             prepared_data[key] = business_data.get(key, None)\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "# #         Add to list if all values are present\n",
    "# #         if all(prepared_data.values()):\n",
    "# #             results.append(prepared_data)\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_list(my_list):\n",
    " \n",
    "#     api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "#     request = {\n",
    "#                'itemID': my_list,\n",
    "#                'IncludeSelector': 'ItemSpecifics'\n",
    "#               }\n",
    "#     response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "#     #save the response as a json dict\n",
    "#     response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "#     #index dict to appropriate index\n",
    "#     results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "#     # Call the prepare_data function to get a list of processed data\n",
    "#     prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "#     # Extend full_dataset with this list (don't append, or you'll get\n",
    "#     # a list of lists instead of a flat list)\n",
    "#     full_dataset.extend(prepared_knives)\n",
    "    \n",
    "#     return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset = []\n",
    "# for i in range(0, len(itemIds), 20):\n",
    "#     process_list(itemIds[i:i+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemIds = list(df.itemId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset = []\n",
    "# for i in range(0, len(itemIds), 20):\n",
    "#     process_list(itemIds[i:i+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_opening_mech(line):\n",
    "#     pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['opening_mechanism'] = df3.ItemSpecifics.apply(extract_opening_mech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_type(line):\n",
    "#     pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.ItemSpecifics.sample(10).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_color(line):\n",
    "#     pattern = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['color'] = df3.ItemSpecifics.apply(extract_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_type(line):\n",
    "#     pattern = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         match = 'NA'\n",
    "        \n",
    "#     return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['blade_type'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_manufacture_region(line):\n",
    "#     pattern = re.compile(\"Country/Region of Manufacture\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['region_of_Manufacture'] = df3.ItemSpecifics.apply(extract_manufacture_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['region_of_Manufacture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_handle_material(line):\n",
    "#     pattern = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['handle_material'] = df3.ItemSpecifics.apply(extract_handle_material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['handle_material'].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_lock_type(line):\n",
    "#     pattern = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "\n",
    "# df3['lock_type'] = df3.ItemSpecifics.apply(extract_lock_type)\n",
    "\n",
    "# df3['lock_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_blade_edge(line):\n",
    "#     pattern = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['blade_edge'] = df3.ItemSpecifics.apply(extract_blade_edge)\n",
    "\n",
    "# df3['blade_edge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_dexterity(line):\n",
    "#     pattern = re.compile(\"Dexterity\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "#     if re.findall(pattern,str(line)):\n",
    "#         match = re.findall(pattern,str(line))[0]\n",
    "#     else:\n",
    "#         match = 'NA'\n",
    "#     return match\n",
    "        \n",
    "# df3['dexterity'] = df3.ItemSpecifics.apply(extract_dexterity)\n",
    "\n",
    "# df3['dexterity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.to_csv('data/item_specifics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "# request = {\n",
    "#            'itemID': list(itemIds),\n",
    "#            'IncludeSelector': 'ItemSpecifics'\n",
    "#           }\n",
    "# response = api.execute('GetMultipleItems', request)\n",
    "# print(response.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import listdir\n",
    "# from PIL import Image\n",
    "\n",
    "# for imageFolder in listdir('./nn_images2'):\n",
    "#     try:\n",
    "#         img = Image.open('./nn_images2/'+imageFolder)\n",
    "#         img.verify()     # to veify if its an img\n",
    "#         img.close()     #to close img and free memory space\n",
    "#     except (IOError, SyntaxError) as e:\n",
    "#         print('Bad file:', imageFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listdir('/nnimages2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root='C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pandas as pd\n",
    "# import matplotlib.pyplot  as plt\n",
    "# from PIL import Image\n",
    "# from pathlib import Path\n",
    "# # import imagesize\n",
    "# import numpy as np\n",
    "\n",
    "# # Get the Image Resolutions\n",
    "# imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "# img_meta = {}\n",
    "# for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# # Convert it to Dataframe and compute aspect ratio\n",
    "# img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "# img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "# img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "# print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "# img_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "    price_list.append(listed_price)\n",
    "    \n",
    "df['price_in_US'] = price_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#atttempt to pull shipping cost from json dict\n",
    "shipping_cost_list = []\n",
    "for row in full_dataset:\n",
    "    shipping_cost = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "    shipping_cost_list.append(shipping_cost)\n",
    "    \n",
    "df['shipping_price'] = shipping_cost_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#pull shipping cost from json dict with regex \n",
    "df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#create new feature 'converted price'\n",
    "df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "df = df.drop_duplicates(subset=['title', 'galleryURL'], keep='first')\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "\n",
    "df.to_csv('data/full_dataset.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwLbGNr0TbIj"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* Were there variables you dropped or created?\n",
    "* How did you address missing values or outliers?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "\n",
    "\n",
    "# here you run your code to clean the data\n",
    "\n",
    "```\n",
    "import code.data_cleaning as dc\n",
    "\n",
    "full_dataset = dc.full_clean()\n",
    "```\n",
    "\n",
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "# here you run your code to model the data\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do in the future to improve this project?\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_knife_dir = 'knife_images'\n",
    "# data_profit_dir = 'data/profit'\n",
    "# new_dir = 'split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_folder = os.path.join(new_dir, 'train')\n",
    "# train_profit = os.path.join(train_folder, 'profit')\n",
    "# os.mkdir(train_folder)\n",
    "# os.mkdir(train_profit)\n",
    "\n",
    "# test_folder = os.path.join(new_dir, 'test')\n",
    "# test_profit = os.path.join(test_folder, 'profit')\n",
    "# os.mkdir(test_folder)\n",
    "# os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# val_folder = os.path.join(new_dir, 'validation')\n",
    "# val_profit = os.path.join(val_folder, 'profit')\n",
    "# os.mkdir(val_folder)\n",
    "# os.mkdir(val_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train knife regression images\n",
    "# #80% of data\n",
    "# imgs = knife_images[:5620]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(train_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # test knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[5620:6322]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(test_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # validation knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[6322:]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(val, img)\n",
    "#     shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "plt.imshow(img_rgb)  # graph it\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_checker(index,):\n",
    "    img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "    plt.imshow(img_rgb)  # graph it\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_benchmade_index[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(2286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(1879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_checker(6094)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final processing steps for images\n",
    "\n",
    "image_list = []\n",
    "for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "    img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "    img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "    image_list.append(img_rgb)\n",
    "   \n",
    "    # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_regression['mean_profit'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=  df_CNN_regression['mean_profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small batch\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256, 3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=30,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub['profit'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.2164 * 41.374303202846974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_batch32.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a train set of 60% and a val and test size of 20% each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model showed a lot of indication that it was overfit\n",
    "#need to retry how I split the data \n",
    "#Instead of manul indexing, will use \n",
    "# from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X[:4918]\n",
    "# y_train = y[:4918]\n",
    "\n",
    "# X_train = X[4918:5971]\n",
    "# y_train = y[4918:5971]\n",
    "\n",
    "# X_test = X[5971:]\n",
    "# y_test = y[5971:]\n",
    "\n",
    "\n",
    "# display(len(X_val)/len(X))\n",
    "# display(len(X_train)/len(X))\n",
    "# len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(224 ,224,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=32,\n",
    "#                     batch_size=300,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model is showing a lot of signs of overfitting \n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from ebaysdk.finding import Connection\n",
    "import requests\n",
    "\n",
    "from ebaysdk.shopping import Connection as Shopping\n",
    "\n",
    "import pandas as pd \n",
    "import  json\n",
    "import numpy as np\n",
    "import re\n",
    "import preprocess_ddey117 as pp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "df = pd.read_csv('data/tera_df_prepared.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')\n",
    "\n",
    "df.info()\n",
    "\n",
    "api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "request = {\n",
    "           'itemID': list(itemIds),\n",
    "           'IncludeSelector': 'ItemSpecifics'\n",
    "          }\n",
    "response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "response_dict = response.dict()\n",
    "\n",
    "results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "\n",
    "\n",
    "response_dict = response.dict()\n",
    "\n",
    "    #index dict to appropriate index\n",
    "# results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "                'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "                'Country', 'Location', 'ConvertedCurrentPrice',\n",
    "                'PictureURL']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['ItemID', 'Title', 'GalleryURL', \n",
    "                'ViewItemURLForNaturalSearch', 'ItemSpecifics', \n",
    "\n",
    "                'PictureURL']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "#         Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_list(my_list):\n",
    " \n",
    "    api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "    request = {\n",
    "               'itemID': my_list,\n",
    "               'IncludeSelector': 'ItemSpecifics'\n",
    "              }\n",
    "    response = api.execute('GetMultipleItems', request)\n",
    "\n",
    "    \n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['Item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "    \n",
    "    return full_dataset\n",
    "\n",
    "full_dataset = []\n",
    "for i in range(0, len(itemIds), 20):\n",
    "    process_list(itemIds[i:i+20])\n",
    "\n",
    "itemIds = list(df.itemId)\n",
    "\n",
    "full_dataset = []\n",
    "for i in range(0, len(itemIds), 20):\n",
    "    process_list(itemIds[i:i+20])\n",
    "\n",
    "len(full_dataset)\n",
    "\n",
    "full_dataset[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(full_dataset)\n",
    "\n",
    "df3 = df2.drop_duplicates(subset='ItemID').copy()\n",
    "\n",
    "df.info()\n",
    "\n",
    "df3.ItemSpecifics.sample(5).apply(print)\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "def extract_opening_mech(line):\n",
    "    pattern = re.compile(\"Opening Mechanism\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['opening_mechanism'] = df3.ItemSpecifics.apply(extract_opening_mech)\n",
    "\n",
    "df3\n",
    "\n",
    "pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "\n",
    "def extract_blade_type(line):\n",
    "    pattern = re.compile(\"Blade Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)\n",
    "\n",
    "df3.ItemSpecifics.sample(10).apply(print)\n",
    "\n",
    "def extract_color(line):\n",
    "    pattern = re.compile(\"Color\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['color'] = df3.ItemSpecifics.apply(extract_color)\n",
    "\n",
    "def extract_blade_type(line):\n",
    "    pattern = re.compile(\"Blade Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        match = 'NA'\n",
    "        \n",
    "    return match\n",
    "\n",
    "df3['blade_type'] = df3.ItemSpecifics.apply(extract_blade_type)\n",
    "\n",
    "df3['blade_type'].value_counts()[:20]\n",
    "\n",
    "def extract_manufacture_region(line):\n",
    "    pattern = re.compile(\"Country/Region of Manufacture\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "\n",
    "\n",
    "df3['region_of_Manufacture'] = df3.ItemSpecifics.apply(extract_manufacture_region)\n",
    "\n",
    "df3['region_of_Manufacture'].value_counts()\n",
    "\n",
    "def extract_handle_material(line):\n",
    "    pattern = re.compile(\"Handle Material\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "\n",
    "\n",
    "df3['handle_material'] = df3.ItemSpecifics.apply(extract_handle_material)\n",
    "\n",
    "df3['handle_material'].value_counts()[:50]\n",
    "\n",
    "def extract_lock_type(line):\n",
    "    pattern = re.compile(\"Lock Type\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "\n",
    "df3['lock_type'] = df3.ItemSpecifics.apply(extract_lock_type)\n",
    "\n",
    "df3['lock_type'].value_counts()\n",
    "\n",
    "def extract_blade_edge(line):\n",
    "    pattern = re.compile(\"Blade Edge\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "df3['blade_edge'] = df3.ItemSpecifics.apply(extract_blade_edge)\n",
    "\n",
    "df3['blade_edge'].value_counts()\n",
    "\n",
    "def extract_dexterity(line):\n",
    "    pattern = re.compile(\"Dexterity\\s*\\S+\\S+\\s*\\S+\\S+\\s\\S(\\w+)\")\n",
    "    if re.findall(pattern,str(line)):\n",
    "        match = re.findall(pattern,str(line))[0]\n",
    "    else:\n",
    "        match = 'NA'\n",
    "    return match\n",
    "        \n",
    "df3['dexterity'] = df3.ItemSpecifics.apply(extract_dexterity)\n",
    "\n",
    "df3['dexterity'].value_counts()\n",
    "\n",
    "df3.to_csv('data/item_specifics_df.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# api = Shopping(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "# request = {\n",
    "#            'itemID': list(itemIds),\n",
    "#            'IncludeSelector': 'ItemSpecifics'\n",
    "#           }\n",
    "# response = api.execute('GetMultipleItems', request)\n",
    "# print(response.dict())\n",
    "\n",
    "ls\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "for imageFolder in listdir('./nn_images2'):\n",
    "    try:\n",
    "        img = Image.open('./nn_images2/'+imageFolder)\n",
    "        img.verify()     # to veify if its an img\n",
    "        img.close()     #to close img and free memory space\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        print('Bad file:', imageFolder)\n",
    "\n",
    "listdir('/nnimages2')\n",
    "\n",
    "root='C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images2/'\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import imagesize\n",
    "import numpy as np\n",
    "\n",
    "# Get the Image Resolutions\n",
    "imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "img_meta = {}\n",
    "for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# Convert it to Dataframe and compute aspect ratio\n",
    "img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "img_meta_df.head()\n",
    "\n",
    "# Visualize Image Resolutions\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, picker=True)\n",
    "ax.set_title(\"Image Resolution\")\n",
    "ax.set_xlabel(\"Width\", size=14)\n",
    "ax.set_ylabel(\"Height\", size=14)\n",
    "\n",
    "# Visualize Image Resolutions\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "ax.set_title(\"Image Resolution\")\n",
    "ax.set_xlabel(\"Width\", size=14)\n",
    "ax.set_ylabel(\"Height\", size=14)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create function for organizing API call\n",
    "def prepare_data(data_list):\n",
    "    \"\"\"\n",
    "    This function takes in a list of dictionaries and prepares it\n",
    "    for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a new list to hold results\n",
    "    results = []\n",
    "    \n",
    "    for business_data in data_list:\n",
    "    \n",
    "        # Make a new dictionary to hold prepared data for this business\n",
    "        prepared_data = {}\n",
    "        \n",
    "        # Extract name, review_count, rating, and price key-value pairs\n",
    "        # from business_data and add to prepared_data\n",
    "        # If a key is not present in business_data, add it to prepared_data\n",
    "        # with an associated value of None\n",
    "        \n",
    "        keys = ['itemId', 'title', 'galleryURL', \n",
    "        'viewItemURL', 'autoPay', 'postalCode', \n",
    "        'sellingStatus', 'shippingInfo', 'listingInfo',\n",
    "        'returnsAccepted', 'condition', 'topRatedListing',\n",
    "        'galleryPlusPictureURL', 'subtitle', 'discountPriceInfo',\n",
    "        'secondaryCategory']\n",
    "        \n",
    "        for key in keys:\n",
    "            prepared_data[key] = business_data.get(key, None)\n",
    "            results.append(prepared_data)\n",
    "    \n",
    "       \n",
    "        # Add to list if all values are present\n",
    "#         if all(prepared_data.values()):\n",
    "#             results.append(prepared_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# def prepare_df(df):\n",
    "#     price_list = []\n",
    "#     for row in full_dataset:\n",
    "#         listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "#         price_list.append(listed_price)\n",
    "\n",
    "#     df['price_in_US'] = price_list\n",
    "#     #pull shipping cost from json dict with regex \n",
    "#     df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "#     df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "#     df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#     #create new feature 'converted price'\n",
    "#     df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "#     df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "#     display(df.head())\n",
    "#     display(df.info())\n",
    "#     return df\n",
    "\n",
    "def prepare_df(df):\n",
    "    price_list = []\n",
    "    ship_price_list = []\n",
    "    condition_list = []\n",
    "    for row in full_dataset:\n",
    "        listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "        price_list.append(listed_price)\n",
    "\n",
    "        try:\n",
    "            listed_ship_price = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "        except: \n",
    "            listed_ship_price = 0\n",
    "            ship_price_list.append(listed_ship_price)\n",
    "\n",
    "        try:\n",
    "            condition = None\n",
    "            condition = np.float(row['condition']['conditionId'])\n",
    "            condition_list.append(condition)\n",
    "        except: \n",
    "            conditon = 0\n",
    "            condition_list.append(condition)\n",
    "\n",
    "    df['shipping_cost'] = ship_price_list\n",
    "    df['price_in_US'] = price_list\n",
    "    df['condition'] = condition_list\n",
    "    \n",
    "    #create new feature 'converted price'\n",
    "    df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "    df.drop_duplicates(subset=['itemId'],  keep='first', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    display(df.head())\n",
    "    display(df.info())\n",
    "    return df\n",
    "\n",
    "overhead_cost = 3\n",
    "def prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    "\n",
    "    #remove special characters\n",
    "    df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "    df[str(list(bucket_dict.keys())[bucket_dict_position])] = float(list(bucket_dict.values())[bucket_dict_position])\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from base64 import b64encode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this function was used at first to check accuracy of calls using ebay API\n",
    "#the less complex \"prepare_brands\" function seems sufficient enough\n",
    "#it is redudant to use regex with working ebay API call\n",
    "\n",
    "def modified_prepare_brands(df, bucket_dict_position):\n",
    "\n",
    "    df.title = df.title.apply(str.lower)\n",
    "\n",
    "    #remove special characters\n",
    "    df.title.apply(pp.remove_special_chars)\n",
    "\n",
    "    pattern = re.compile(list(bucket_dict.keys())[bucket_dict_position])\n",
    "    df[list(bucket_dict.keys())[bucket_dict_position]] = df_bench.title.apply(lambda x: re.sub(pattern, x, str(list(bucket_dict.values())[bucket_dict_position])))\n",
    "\n",
    "    df[list(bucket_dict.keys())[bucket_dict_position]] = df[list(bucket_dict.keys())[bucket_dict_position]].apply(lambda x: np.float(x))\n",
    "    \n",
    "    df['profit'] = (df['converted_price'] - df[list(bucket_dict.keys())[bucket_dict_position]] - overhead_cost)\n",
    "    df['ROI'] = (df['profit']/(df[list(bucket_dict.keys())[bucket_dict_position]] + overhead_cost))*100.0\n",
    "\n",
    "    return df\n",
    "\n",
    "request = {\n",
    "           'categoryId': 48818,\n",
    "            'itemFilter': [\n",
    "                            {'name': 'Condition', 'value': 'Used'},\n",
    "                            {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                          ],\n",
    "            'aspectFilter': [\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Benchmade'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Buck'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Case'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'CRKT'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Leatherman'},\n",
    "                              {'aspectName': 'Brand', 'aspectValueName': 'Spyderco'},\n",
    "                             ],\n",
    "            'paginationInput': {\n",
    "                                'entriesPerPage': 100,\n",
    "                                'pageNumber': 1\n",
    "\n",
    "                                }}\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger error at page 23\n",
    "\n",
    "\n",
    "for page in range(1, 22):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'categoryId': 48818,\n",
    "                'itemFilter': [\n",
    "#                                 {'name': 'Condition', 'value': 'Used'},\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Benchmade'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_bench = pd.DataFrame(full_dataset)\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'leatherman': 30.0, \n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "              }\n",
    "\n",
    "df_bench = prepare_df(df_bench)\n",
    "\n",
    "df_bench = prepare_brands(df_bench, 0)\n",
    "\n",
    "df_bench['benchmade'].value_counts()\n",
    "\n",
    "df_bench.info()\n",
    "\n",
    "df_bench[df_bench['galleryPlusPictureURL'].notna()]['galleryPlusPictureURL'].sample(30).apply(print)\n",
    "\n",
    "df_bench['profit'].describe()\n",
    "\n",
    "df_bench['ROI'].describe()\n",
    "\n",
    "df_bench.to_csv('data/df_bench.csv', index=False)\n",
    "\n",
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): $7\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger error at page 42\n",
    "\n",
    "\n",
    "for page in range(1, 41):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "                                 {'aspectName': 'Brand', 'aspectValueName': 'Buck'}],\n",
    "\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_buck = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_buck = prepare_df(df_buck)\n",
    "\n",
    "df_buck = prepare_brands(df_buck, 1)\n",
    "\n",
    "df_buck.info()\n",
    "\n",
    "df_buck['buck'].value_counts()\n",
    "\n",
    "df_buck['profit'].describe()\n",
    "\n",
    "df_buck['ROI'].describe()\n",
    "\n",
    "df_buck.to_csv('data/df_buck.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "\n",
    "for page in range(1, 100):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "#                               \n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Case XX'},\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Case'}\n",
    "                                 ],\n",
    "\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_case = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_case = prepare_df(df_case)\n",
    "\n",
    "df_case = prepare_brands(df_case, 2)\n",
    "\n",
    "df_case.info()\n",
    "\n",
    "df_case.to_csv('data/df_case.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 18\n",
    "\n",
    "\n",
    "for page in range(1, 17):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                    {'aspectName': 'Brand', 'aspectValueName': 'CRKT'}],\n",
    "                 \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_crkt = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_crkt = prepare_df(df_crkt)\n",
    "\n",
    "bucket_dict \n",
    "\n",
    "df_crkt = prepare_brands(df_crkt, 3)\n",
    "\n",
    "display(df_crkt.head())\n",
    "display(df_crkt.info())\n",
    "\n",
    "df_crkt.to_csv('data/df_crkt.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debugger threw error at page 70\n",
    "\n",
    "\n",
    "for page in range(1, 69):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                  {'aspectName': 'Brand', 'aspectValueName': 'Kershaw'}\n",
    "                                ],\n",
    "\n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_kershaw = pd.DataFrame(full_dataset)\n",
    "\n",
    "bucket_dict\n",
    "\n",
    "df_kershaw = prepare_df(df_kershaw)\n",
    "\n",
    "df_kershaw = prepare_brands(df_kershaw, 4)\n",
    "\n",
    "df_kershaw.info()\n",
    "\n",
    "df_kershaw.to_csv('data/df_kershaw.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 18\n",
    "\n",
    "\n",
    "for page in range(1, 17):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Leatherman'}],\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_leatherman = pd.DataFrame(full_dataset)\n",
    "\n",
    "bucket_dict\n",
    "\n",
    "df_leatherman = prepare_df(df_leatherman)\n",
    "\n",
    "df_leatherman = prepare_brands(df_leatherman, 5)\n",
    "\n",
    "df_leatherman.info()\n",
    "\n",
    "df_leatherman.to_csv('data/df_leatherman.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "#debug error page 72\n",
    "\n",
    "for page in range(1, 71):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Spyderco'}\n",
    "                              ],\n",
    "                    \n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "df_spyderco = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_spyderco = prepare_df(df_spyderco)\n",
    "\n",
    "bucket_dict \n",
    "\n",
    "df_spyderco = prepare_brands(df_spyderco, 6)\n",
    "\n",
    "df_spyderco.info()\n",
    "\n",
    "df_spyderco.to_csv('data/df_spyderco.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "for page in range(1, 21):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'SOG'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_sog = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_sog = prepare_df(df_sog)\n",
    "\n",
    "bucket_dict['sog'] = 15.0\n",
    "\n",
    "df_sog = prepare_brands(df_sog, 8)\n",
    "\n",
    "df_sog.info()\n",
    "\n",
    "df_sog.to_csv('data/df_sog.csv', index=False)\n",
    "\n",
    "# Create an empty list for the full prepared dataset\n",
    "full_dataset = []\n",
    "\n",
    "\n",
    "for page in range(1, 100):\n",
    "    # Add or update the \"offset\" key-value pair in url_params\n",
    "    request['paginationInput']['pageNumber'] = page\n",
    "    \n",
    "    # Make the query and get the response\n",
    "\n",
    "    api = Connection(config_file='ebay.yaml', debug=True, siteid=\"EBAY-US\")\n",
    "\n",
    "    request = {\n",
    "                'keywords': 'knife',\n",
    "                'itemFilter': [\n",
    "                                {'name': 'ListingType', 'value': 'FixedPrice'}\n",
    "                              ],\n",
    "                'aspectFilter': [\n",
    "\n",
    "                                   {'aspectName': 'Brand', 'aspectValueName': 'Victorinox'}],\n",
    "\n",
    "                    \n",
    "                'paginationInput': {\n",
    "                                    'entriesPerPage': 100,\n",
    "                                    'pageNumber': page\n",
    "                    \n",
    "                                    },\n",
    "               \n",
    "                }\n",
    "\n",
    "    response = api.execute('findItemsAdvanced', request)\n",
    "\n",
    "    #save the response as a json dict\n",
    "    response_dict = response.dict()\n",
    "\n",
    "\n",
    "\n",
    "    #index dict to appropriate index\n",
    "    results_list_of_dicts = response_dict['searchResult']['item']\n",
    "\n",
    "    # Call the prepare_data function to get a list of processed data\n",
    "    prepared_knives = prepare_data(results_list_of_dicts)\n",
    "\n",
    "    # Extend full_dataset with this list (don't append, or you'll get\n",
    "    # a list of lists instead of a flat list)\n",
    "    full_dataset.extend(prepared_knives)\n",
    "\n",
    "# Check the length of the full dataset. It will be up to `total`,\n",
    "# potentially less if there were missing values\n",
    "display(len(full_dataset))\n",
    "\n",
    "\n",
    "df_victorinox = pd.DataFrame(full_dataset)\n",
    "\n",
    "df_victorinox = prepare_df(df_victorinox)\n",
    "\n",
    "bucket_dict\n",
    "\n",
    "df_victorinox = prepare_brands(df_victorinox, 7)\n",
    "\n",
    "df_victorinox.info()\n",
    "\n",
    "df_victorinox.to_csv('data/df_victorinox.csv', index=False)\n",
    "\n",
    "# df_bench = pd.read_csv(\"data/df_bench.csv\")\n",
    "# df_buck = pd.read_csv(\"data/df_buck.csv\")\n",
    "# df_case = pd.read_csv(\"data/df_case.csv\")\n",
    "# df_crkt = pd.read_csv(\"data/df_crkt.csv\")\n",
    "# df_kershaw = pd.read_csv(\"data/df_kershaw.csv\")\n",
    "# df_leatherman = pd.read_csv(\"data/df_leatherman.csv\")\n",
    "# df_spyderco = pd.read_csv(\"data/df_spyderco.csv\")\n",
    "# df_victorinox = pd.read_csv(\"data/df_victorinox.csv\")\n",
    "\n",
    "df = pd.concat([df_bench,df_buck,df_case,df_crkt,df_kershaw,df_leatherman,df_spyderco,df_sog,df_victorinox])\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "df.to_csv(\"data/full_dataset2.csv\", index=False)\n",
    "\n",
    "After succesfully going through 10,000 items on ebay's website and extracting everything possible, there is still a little bit more extracting to do from the json dictionary before saving the dataframe again. We need to get the price of the knives out of the nested dictionary in the dataframe as well as the shipping cost. After that, I would like to create a new feature called\n",
    "**\"converted price,\" which is simply the price of the knife listed on the ebay's website plus shipping.**\n",
    "\n",
    "```\n",
    "#Create row for converted Price of Knives in US dollars\n",
    "price_list = []\n",
    "for row in full_dataset:\n",
    "    listed_price = np.float(row['sellingStatus']['convertedCurrentPrice']['value'])\n",
    "    price_list.append(listed_price)\n",
    "    \n",
    "df['price_in_US'] = price_list\n",
    "```\n",
    "\n",
    "```\n",
    "#atttempt to pull shipping cost from json dict\n",
    "shipping_cost_list = []\n",
    "for row in full_dataset:\n",
    "    shipping_cost = np.float(row['shippingInfo']['shippingServiceCost']['value'])\n",
    "    shipping_cost_list.append(shipping_cost)\n",
    "    \n",
    "df['shipping_price'] = shipping_cost_list\n",
    "```\n",
    "\n",
    "```\n",
    "#pull shipping cost from json dict with regex \n",
    "df['shipping_cost'] = df['shippingInfo'].apply(lambda x: re.findall(\"(\\d+\\S+\\d)\", json.dumps(x)))\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: ''.join(x))\n",
    "df.drop(df[df['shipping_cost'] == ''].index, inplace=True)\n",
    "df['shipping_cost'] = df['shipping_cost'].apply(lambda x: np.float(x))\n",
    "\n",
    "#create new feature 'converted price'\n",
    "df['converted_price'] = df['shipping_cost'] + df['price_in_US']\n",
    "df = df.drop_duplicates(subset=['title', 'galleryURL'], keep='first')\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "\n",
    "df.to_csv('data/full_dataset.csv', index=False)\n",
    "```\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Describe and justify the process for preparing the data for analysis.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* Were there variables you dropped or created?\n",
    "* How did you address missing values or outliers?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "\n",
    "\n",
    "# here you run your code to clean the data\n",
    "\n",
    "```\n",
    "import code.data_cleaning as dc\n",
    "\n",
    "full_dataset = dc.full_clean()\n",
    "```\n",
    "\n",
    "## Data Modeling\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How did you analyze or model the data?\n",
    "* How did you iterate on your initial approach to make it better?\n",
    "* Why are these choices appropriate given the data and the business problem?\n",
    "***\n",
    "# here you run your code to model the data\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "Provide your conclusions about the work you've done, including any limitations or next steps.\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* What would you recommend the business do as a result of this work?\n",
    "* What are some reasons why your analysis might not fully solve the business problem?\n",
    "* What else could you do in the future to improve this project?\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "# data_knife_dir = 'knife_images'\n",
    "# data_profit_dir = 'data/profit'\n",
    "# new_dir = 'split'\n",
    "\n",
    "# os.mkdir(new_dir)\n",
    "\n",
    "# train_folder = os.path.join(new_dir, 'train')\n",
    "# train_profit = os.path.join(train_folder, 'profit')\n",
    "# os.mkdir(train_folder)\n",
    "# os.mkdir(train_profit)\n",
    "\n",
    "# test_folder = os.path.join(new_dir, 'test')\n",
    "# test_profit = os.path.join(test_folder, 'profit')\n",
    "# os.mkdir(test_folder)\n",
    "# os.mkdir(test_profit)\n",
    "\n",
    "\n",
    "# val_folder = os.path.join(new_dir, 'validation')\n",
    "# val_profit = os.path.join(val_folder, 'profit')\n",
    "# os.mkdir(val_folder)\n",
    "# os.mkdir(val_profit)\n",
    "\n",
    "# val_profit\n",
    "\n",
    "# # train knife regression images\n",
    "# #80% of data\n",
    "# imgs = knife_images[:5620]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(train_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "# # test knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[5620:6322]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(test_profit, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "    \n",
    "    \n",
    "# # validation knife regression images\n",
    "# #10% of data\n",
    "# imgs = knife_images[6322:]\n",
    "# for img in imgs:\n",
    "#     origin = os.path.join(data_knife_dir, img)\n",
    "#     destination = os.path.join(val, img)\n",
    "#     shutil.copyfile(origin, destination)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Dense, Flatten, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "img_array = cv2.imread('knife_images/918.jpg')  # convert to array\n",
    "\n",
    "img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "plt.imshow(img_rgb)  # graph it\n",
    "plt.show();\n",
    "\n",
    "\n",
    "def image_checker(index,):\n",
    "    img_array = cv2.imread('knife_images/'+str(index)+'.jpg')  \n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)\n",
    "    plt.imshow(img_rgb)  # graph it\n",
    "    plt.show();\n",
    "\n",
    "top_benchmade_index[:50]\n",
    "\n",
    "image_checker(6158)\n",
    "\n",
    "image_checker(2286)\n",
    "\n",
    "image_checker(1879)\n",
    "\n",
    "image_checker(4326)\n",
    "\n",
    "image_checker(6094)\n",
    "\n",
    "#final processing steps for images\n",
    "\n",
    "image_list = []\n",
    "for x in range(len(df_CNN_regression)):\n",
    "    \n",
    "    img_array = cv2.imread('knife_images/'+str(x)+'.jpg')  # convert to array\n",
    "    img_rgb = cv2.resize(img_array,(256,256),3)  # resize\n",
    "    img_rgb = np.array(img_rgb).astype(np.float64)/255.0  # scaling\n",
    "    image_list.append(img_rgb)\n",
    "   \n",
    "    # img_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\n",
    "\n",
    "\n",
    "\n",
    "df_CNN_regression['mean_profit']= (df_CNN_regression['profit']/df_CNN_regression['profit'].mean())\n",
    "\n",
    "df_CNN_regression['mean_profit'].describe()\n",
    "\n",
    "X = np.array(image_list)\n",
    "\n",
    "y=  df_CNN_regression['mean_profit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, test_size=0.4, random_state=32)# Create the Test and Final Training Datasets\n",
    "\n",
    "X.shape\n",
    "\n",
    "y.shape\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Xtrain:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"X_val:\", X_test.shape)\n",
    "print(\"y_val:\", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#small batch\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256, 3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=30,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "df_scrub['profit'].mean()\n",
    "\n",
    "2.2164 * 41.374303202846974\n",
    "\n",
    "df_scrub.head()\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#The model learned patterns wells until epoch 20\n",
    "#after that the loss spikes signifcantly before dropping again\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();\n",
    "\n",
    "model.save('my_model_batch32.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#a train set of 60% and a val and test size of 20% each \n",
    "\n",
    "#this model showed a lot of indication that it was overfit\n",
    "#need to retry how I split the data \n",
    "#Instead of manul indexing, will use \n",
    "# from sklearn model_selection train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X[:4918]\n",
    "# y_train = y[:4918]\n",
    "\n",
    "# X_train = X[4918:5971]\n",
    "# y_train = y[4918:5971]\n",
    "\n",
    "# X_test = X[5971:]\n",
    "# y_test = y[5971:]\n",
    "\n",
    "\n",
    "# display(len(X_val)/len(X))\n",
    "# display(len(X_train)/len(X))\n",
    "# len(X_test)/len(X)\n",
    "\n",
    "\n",
    "\n",
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(224 ,224,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mse'])\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     epochs=32,\n",
    "#                     batch_size=300,\n",
    "#                     validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')\n",
    "\n",
    "results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# model.save('my_model_batch500.h5')\n",
    "\n",
    "history.history.keys()\n",
    "\n",
    "#The model is showing a lot of signs of overfitting \n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss( mean square error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_mse', 'val_mse'], loc='upper right')\n",
    "plt.show();\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "# results_train = model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "# model.save('my_model_batch500.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
