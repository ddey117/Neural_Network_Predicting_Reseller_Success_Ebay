{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd \n",
    "import  json\n",
    "import requests\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_iqr_filter(df):\n",
    "    \n",
    "    price_Q1 = df['converted_price'].quantile(0.25)\n",
    "    price_Q3 = df['converted_price'].quantile(0.75)\n",
    "    price_iqr = price_Q3 - price_Q1\n",
    "\n",
    "    profit_Q1 = df['profit'].quantile(0.25)\n",
    "    profit_Q3 = df['profit'].quantile(0.75)\n",
    "    profit_iqr = profit_Q3 - profit_Q1\n",
    "\n",
    "    ROI_Q1 = df['ROI'].quantile(0.25)\n",
    "    ROI_Q3 = df['ROI'].quantile(0.75)\n",
    "    ROI_iqr = ROI_Q3 - ROI_Q1\n",
    "\n",
    "    price_upper_limit = price_Q3 + (1.5 * price_iqr)\n",
    "    price_lower_limit = price_Q1 - (1.5 * price_iqr)\n",
    "\n",
    "    profit_upper_limit = profit_Q3 + (1.5 * profit_iqr)\n",
    "    profit_lower_limit = profit_Q1 - (1.5 * profit_iqr)\n",
    "\n",
    "    ROI_upper_limit = ROI_Q3 + (1.5 * ROI_iqr)\n",
    "    ROI_lower_limit = ROI_Q1 - (1.5 * ROI_iqr)\n",
    "    \n",
    "#     print(f'Brand: {df.brand[0]}')\n",
    "#     print(f'price upper limit: ${np.round(price_upper_limit,2)}')\n",
    "#     print(f'price lower limit: ${np.round(price_lower_limit,2)}')\n",
    "#     print('-----------------------------------')\n",
    "#     print(f'profit upper limit: ${np.round(profit_upper_limit,2)}')\n",
    "#     print(f'profit lower limit: ${np.round(profit_lower_limit,2)}')\n",
    "#     print('-----------------------------------')\n",
    "#     print(f'ROI upper limit: {np.round(ROI_upper_limit,2)}%')\n",
    "#     print(f'ROI lower limit: {np.round(ROI_lower_limit,2)}%')\n",
    "#     print('-----------------------------------')\n",
    "\n",
    "    \n",
    "    new_df = df[(df['converted_price'] <= price_upper_limit) &\n",
    "                (df['converted_price'] >= price_lower_limit) &\n",
    "                (df['profit'] <= profit_upper_limit) &\n",
    "                (df['ROI'] <= ROI_upper_limit) &\n",
    "                (df['profit'] <= profit_upper_limit) &\n",
    "                (df['ROI'] >= ROI_lower_limit)]\n",
    "    \n",
    "    return new_df\n",
    "#download jpg urls from dataFrame\n",
    "def download(row):\n",
    "    filename = os.path.join(root_folder, str(row.name) + im_extension)\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.Image\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    except:\n",
    "        print(f'{filename} error')\n",
    "\n",
    "\n",
    "def cardinality_threshold(column,threshold=0.75,return_categories_list=True):\n",
    "    #calculate the threshold value using\n",
    "    #the frequency of instances in column\n",
    "    threshold_value=int(threshold*len(column))\n",
    "    #initialize a new list for lower cardinality column\n",
    "    categories_list=[]\n",
    "    #initialize a variable to calculate sum of frequencies\n",
    "    s=0\n",
    "    #Create a dictionary (unique_category: frequency)\n",
    "    counts=Counter(column)\n",
    "\n",
    "    #Iterate through category names and corresponding frequencies after sorting the categories\n",
    "    #by descending order of frequency\n",
    "    for i,j in counts.most_common():\n",
    "        #Add the frequency to the total sum\n",
    "        s += dict(counts)[i]\n",
    "        #append the category name to the categories list\n",
    "        categories_list.append(i)\n",
    "        #Check if the global sum has reached the threshold value, if so break the loop\n",
    "        if s >= threshold_value:\n",
    "            break\n",
    "      #append the new 'Other' category to list\n",
    "    categories_list.append('Other')\n",
    "\n",
    "    #Take all instances not in categories below threshold  \n",
    "    #that were kept and lump them into the\n",
    "    #new 'Other' category.\n",
    "    new_column = column.apply(lambda x: x if x in categories_list else 'Other')\n",
    "\n",
    "    #Return the transformed column and\n",
    "    #unique categories if return_categories = True\n",
    "    if(return_categories_list):\n",
    "        return new_column,categories_list\n",
    "    #Return only the transformed column if return_categories=False\n",
    "    else:\n",
    "        return new_column\n",
    "    \n",
    "    \n",
    "    \n",
    "def year_wrangler(row):\n",
    "    if row['Year'] >= 1960 and row['Year'] < 1970:\n",
    "        return '60s'\n",
    "    elif row['Year'] >= 1970 and row['Year'] < 1980:\n",
    "        return '70s'\n",
    "    elif row['Year'] >= 1980 and row['Year'] < 1990:\n",
    "        return '80s'\n",
    "    elif row['Year'] >= 1990 and row['Year'] < 2000:\n",
    "        return '90s'\n",
    "    elif row['Year'] >= 2000 and row['Year'] < 2010:\n",
    "        return '00s'\n",
    "    elif row['Year'] >= 2010 and row['Year'] < 2020:\n",
    "        return '10s'\n",
    "    elif row['Year'] >= 2020 and row['Year'] < 2030:\n",
    "        return '20s'\n",
    "    \n",
    "    else:\n",
    "        return 'unknown'\n",
    "    \n",
    "def bin_knife_year(knife_df):\n",
    "    knife_df['construction_year'] = knife_df.apply(lambda row: year_wrangler(row), axis=1)\n",
    "    return knife_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drop_cols(water_pump_df):\n",
    "#     to_drop_final = ['id', 'recorded_by', 'num_private',\n",
    "#           'waterpoint_type_group', 'source',\n",
    "#           'source_class', 'extraction_type',\n",
    "#           'extraction_type_group', 'payment_type',\n",
    "#           'management_group', 'scheme_name',\n",
    "#           'water_quality', 'quantity_group',\n",
    "#           'scheme_management', 'longitude',\n",
    "#           'latitude', 'date_recorded',\n",
    "#           'amount_tsh', 'gps_height',\n",
    "#           'region_code', 'district_code']\n",
    "#           #'population'\n",
    "    \n",
    "#     return water_pump_df.drop(columns=to_drop_final, axis=1)\n",
    "\n",
    "#helper function to bin construction year\n",
    "\n",
    "\n",
    "# def apply_cardinality_reduct(water_pump_df, reduct_dict):\n",
    "#     for col, categories_list in reduct_dict.items():\n",
    "#         water_pump_df[col] = water_pump_df[col].apply(lambda x: x if x in categories_list else 'Other')\n",
    "#     return water_pump_df\n",
    "        \n",
    "\n",
    "\n",
    "# #one_hot_incode categorical data\n",
    "# def one_hot(water_pump_df):\n",
    "#     final_cat = ['funder', 'installer', 'wpt_name', 'basin', 'subvillage', 'region',\n",
    "#        'lga', 'ward', 'public_meeting', 'permit', 'construction_year',\n",
    "#        'extraction_type_class', 'management', 'payment', 'quality_group',\n",
    "#        'quantity', 'source_type', 'waterpoint_type']\n",
    "    \n",
    "#     water_pump_df = pd.get_dummies(water_pump_df[final_cat], drop_first=True)\n",
    "    \n",
    "#     return water_pump_df\n",
    "\n",
    "    \n",
    "    \n",
    "# #master function for cleaning dataFrame\n",
    "# def clean_dataFrame(water_pump_df, reduct_dict):\n",
    "#     water_pump_df = drop_cols(water_pump_df)\n",
    "#     water_pump_df = bin_construction_year(water_pump_df)\n",
    "#     water_pump_df = fill_unknowns(water_pump_df)\n",
    "#     water_pump_df = fill_col_normal_data(water_pump_df)\n",
    "#     water_pump_df = apply_cardinality_reduct(water_pump_df, reduct_dict)\n",
    "#     water_pump_df = one_hot(water_pump_df)\n",
    "    \n",
    "#     return water_pump_df\n",
    "\n",
    "# ###############################################\n",
    "# # The rest of the functions in this section\n",
    "# #define functions that reduce cardinality\n",
    "# #by mapping infrequent values ot other\n",
    "# #the dictionary derived from these functions\n",
    "# #will be used by my_funk in my master \n",
    "# #clean_dataFrame function\n",
    "\n",
    "# #helper function for reducing cardinality    \n",
    "# def cardinality_threshold(column,threshold=0.65):\n",
    "#     #calculate the threshold value using\n",
    "#     #the frequency of instances in column\n",
    "#     threshold_value=int(threshold*len(column))\n",
    "#     #initialize a new list for lower cardinality column\n",
    "#     categories_list=[]\n",
    "#     #initialize a variable to calculate sum of frequencies\n",
    "#     s=0\n",
    "#     #Create a dictionary (unique_category: frequency)\n",
    "#     counts=Counter(column)\n",
    "\n",
    "#     #Iterate through category names and corresponding frequencies after sorting the categories\n",
    "#     #by descending order of frequency\n",
    "#     for i,j in counts.most_common():\n",
    "#         #Add the frequency to the total sum\n",
    "#         s += dict(counts)[i]\n",
    "#         #append the category name to the categories list\n",
    "#         categories_list.append(i)\n",
    "#         #Check if the global sum has reached the threshold value, if so break the loop\n",
    "#         if s >= threshold_value:\n",
    "#             break\n",
    "#         #append the new 'Other' category to list\n",
    "#         categories_list.append('Other')\n",
    "\n",
    "#     #Take all instances not in categories below threshold  \n",
    "#     #that were kept and lump them into the\n",
    "#     #new 'Other' category.\n",
    "#     new_column = column.apply(lambda x: x if x in categories_list else 'Other')\n",
    "# #     return new_column\n",
    "#     return categories_list\n",
    "     \n",
    "#  #reduces the cardinality of appropriate categories   \n",
    "# def get_col_val_mapping(water_pump_df):\n",
    "#     col_threshold_list = [\n",
    "#         ('funder',0.65), \n",
    "#         ('installer', 0.65),\n",
    "#         ('wpt_name', 0.15),\n",
    "#         ('subvillage', 0.07),\n",
    "#         ('lga', 0.6),\n",
    "#         ('ward', 0.05)\n",
    "#     ]\n",
    "    \n",
    "#     reduct_dict = {}\n",
    "    \n",
    "#     for col, thresh in col_threshold_list:\n",
    "#         reduct_dict[col] = cardinality_threshold(water_pump_df[col],\n",
    "#                                                    threshold= thresh)\n",
    "        \n",
    "#     return reduct_dict\n",
    "\n",
    "# reduct_dict is a key value mapper that will\n",
    "# be used for both training and testing sets\n",
    "# in order to reduce cardinality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "listed_df = pd.read_csv('listed_data/listed_knives_df.csv', \n",
    "                        dtype={'UPC': str, \n",
    "                               'Year': str,\n",
    "                               'MPN': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listed.drop(['shipping_cost', \n",
    "#                 'price_in_US','cost',\n",
    "#                 'Original/Reproduction', \n",
    "#                 'specBrand', 'Type'],\n",
    "#                axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# str_columns = ['Location', 'Country', 'Model',\n",
    "#                'Country/Region of Manufacture', \n",
    "#                'Blade Material', 'Blade Type',\n",
    "#                'Blade Edge', 'Dexterity', \n",
    "#                'Color', 'Number of Blades',\n",
    "#                'Opening Mechanism', 'Handle Material', \n",
    "#                'Lock Type', 'Blade Range']\n",
    "\n",
    "# for col in str_columns:\n",
    "#     df_listed[col] = df_listed[col].str.lower()\n",
    "    \n",
    "# pattern = \".*,\\s*([^\\d,]+?)(?:\\s*\\d+)?$\"\n",
    "# df_listed['State_or_Province'] = df_listed['Location'].str.extract(pattern)\n",
    "\n",
    "# pattern = re.compile(\"(\\d{4}$)\")\n",
    "# df_listed['Year'] = df_listed['Year'].str.extract(pattern)\n",
    "\n",
    "# df_listed['Year'] = df_listed['Year'].fillna(0)\n",
    "\n",
    "# df_listed['Year'] = df_listed['Year'].astype(int)\n",
    "\n",
    "# df_listed = bin_knife_year(df_listed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_df = pd.read_csv('terapeak_data/sold_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case          17337\n",
       "buck          11384\n",
       "kershaw       10766\n",
       "victorinox     9486\n",
       "spyderco       6165\n",
       "benchmade      5792\n",
       "crkt           4292\n",
       "sog            3018\n",
       "Name: brand, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sold_df.brand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_df.drop(['price_in_US', \n",
    "              'shipping_cost'],\n",
    "             axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sold = apply_iqr_filter(sold_df).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_listed = listed_df.loc[listed_df['condition'] != 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_listed.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['title','pictureURLLarge','converted_price','brand','profit','ROI']\n",
    "used_listed2 = used_listed[cols].copy()\n",
    "df1 = pd.concat([sold_df, used_listed2]).copy()\n",
    "df1['Image'].fillna(df1['pictureURLLarge'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apply_iqr_filter(df1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network with \"title\" column as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = df.drop(['Image', 'url', \n",
    "                    'date_sold', 'profit',\n",
    "                    'ROI', 'brand', 'cost'],\n",
    "                     axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title.rename({'title': 'data',\n",
    "                 'converted_price': 'labels'},\n",
    "                axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.62917081729681"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_price = df_title['labels'].mean()\n",
    "mean_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title['labels'] = (df_title['labels']/mean_price)\n",
    "Y = df_title['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, Ytrain, Ytest = train_test_split(df_title['data'], Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to sequences\n",
    "MAX_VOCAB_SIZE = 40000\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(df_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30734 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "V = len(word2idx)\n",
    "print('Found %s unique tokens.' % V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data train tensor: (56537, 43)\n"
     ]
    }
   ],
   "source": [
    "# pad sequences so that we get a N x T matrix\n",
    "data_train = pad_sequences(sequences_train)\n",
    "print('Shape of data train tensor:', data_train.shape)\n",
    "\n",
    "# get sequence length\n",
    "T = data_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data test tensor: (14135, 43)\n"
     ]
    }
   ],
   "source": [
    "data_test = pad_sequences(sequences_test, maxlen=T)\n",
    "print('Shape of data test tensor:', data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RNN model\n",
    "\n",
    "# We get to choose embedding dimensionality\n",
    "D = 20\n",
    "\n",
    "# Hidden state dimensionality\n",
    "M = 15\n",
    "\n",
    "\n",
    "i = Input(shape=(T,))\n",
    "x = Embedding(V + 1, D)(i)\n",
    "x = LSTM(M, return_sequences=True)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "\n",
    "model = Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/5\n",
      "1767/1767 [==============================] - 21s 12ms/step - loss: 0.2472 - mae: 0.3507 - val_loss: 0.1740 - val_mae: 0.2939\n",
      "Epoch 2/5\n",
      "1767/1767 [==============================] - 21s 12ms/step - loss: 0.1417 - mae: 0.2603 - val_loss: 0.1649 - val_mae: 0.2824\n",
      "Epoch 3/5\n",
      "1767/1767 [==============================] - 21s 12ms/step - loss: 0.1144 - mae: 0.2305 - val_loss: 0.1621 - val_mae: 0.2775\n",
      "Epoch 4/5\n",
      "1767/1767 [==============================] - 20s 11ms/step - loss: 0.0969 - mae: 0.2098 - val_loss: 0.1666 - val_mae: 0.2745\n",
      "Epoch 5/5\n",
      "1767/1767 [==============================] - 19s 11ms/step - loss: 0.0858 - mae: 0.1965 - val_loss: 0.1666 - val_mae: 0.2750\n"
     ]
    }
   ],
   "source": [
    "# Compile and fit\n",
    "model.compile(\n",
    "  loss='MSE',\n",
    "  optimizer='adam',\n",
    "  metrics=['mae']\n",
    ")\n",
    "\n",
    "\n",
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  data_train,\n",
    "  Ytrain,\n",
    "  epochs=5,\n",
    "  validation_data=(data_test, Ytest)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 43)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 43, 20)            614700    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 43, 15)            2160      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 616,876\n",
      "Trainable params: 616,876\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12,8))\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.title(\"Loss vs val Loss for RNN model on titles (MSE)\", fontsize=15)\n",
    "plt.xlabel(\"epochs\", fontsize=15)\n",
    "plt.ylabel(\"loss (mean squared error)\", fontsize=15)\n",
    "plt.legend();\n",
    "plt.savefig('images/RNN_titles_MSE1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12,8))\n",
    "plt.plot(r.history['mae'], label='mae')\n",
    "plt.plot(r.history['val_mae'], label='val_mae')\n",
    "plt.title(\"Loss vs val Loss for RNN model on titles (MAE)\", fontsize=15)\n",
    "plt.xlabel(\"epochs\", fontsize=15)\n",
    "plt.ylabel(\"loss (mean absolute error)\", fontsize=15)\n",
    "plt.legend();\n",
    "plt.savefig('images/RNN_titles_MAE1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.276 * mean_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CNN model\n",
    "\n",
    "# We get to choose embedding dimensionality\n",
    "D = 20\n",
    "\n",
    "\n",
    "\n",
    "i = Input(shape=(T,))\n",
    "x = Embedding(V + 1, D)(i)\n",
    "x = Conv1D(32, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(64, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "\n",
    "model = Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "model.compile(\n",
    "  loss='MSE',\n",
    "  optimizer='adam',\n",
    "  metrics=['MSE']\n",
    ")\n",
    "\n",
    "\n",
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  data_train,\n",
    "  Ytrain,\n",
    "  epochs=5,\n",
    "  validation_data=(data_test, Ytest)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss per iteration\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy per iteration\n",
    "plt.plot(r.history['MSE'], label='MSE')\n",
    "plt.plot(r.history['val_MSE'], label='val_MSE')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN using images as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs = df.drop(['title', 'url', \n",
    "                   'date_sold', 'profit',\n",
    "                   'ROI', 'brand', 'cost',\n",
    "                   'pictureURLLarge'],\n",
    "                     axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs.dropna(subset=['Image'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs['file_index'] = df_imgs.index.values\n",
    "df_imgs['file_index'] = df_imgs['file_index'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs['filename'] = df_imgs['file_index'] + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Image Resolutions\n",
    "\n",
    "# # Import Packages\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot  as plt\n",
    "# from PIL import Image\n",
    "# from pathlib import Path\n",
    "# import imagesize\n",
    "# import numpy as np\n",
    "\n",
    "# # Get the Image Resolutions\n",
    "# imgs = [img.name for img in Path(root).iterdir() if img.suffix == \".jpg\"]\n",
    "# img_meta = {}\n",
    "# for f in imgs: img_meta[str(f)] = imagesize.get(root+f)\n",
    "\n",
    "# # Convert it to Dataframe and compute aspect ratio\n",
    "# img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "# img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "# img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "\n",
    "# print(f'Total Nr of Images in the dataset: {len(img_meta_df)}')\n",
    "# img_meta_df.head()\n",
    "\n",
    "\n",
    "\n",
    "# # Visualize Image Resolutions\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "# points = ax.scatter(img_meta_df.Width, img_meta_df.Height, color='blue', alpha=0.5, s=img_meta_df[\"Aspect Ratio\"]*100, picker=True)\n",
    "# ax.set_title(\"Image Resolution\")\n",
    "# ax.set_xlabel(\"Width\", size=14)\n",
    "# ax.set_ylabel(\"Height\", size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(row):\n",
    "    filename = row.filepath\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "#     os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.Image\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    except:\n",
    "        print(f'{filename} error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images/'\n",
    "df_imgs['filepath'] = root_folder + df_imgs['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs['filepath'].sample(2).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_imgs.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_files = []\n",
    "pathway = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images/'\n",
    "for filename in os.listdir(pathway):\n",
    "    if filename.endswith('.jpg'):\n",
    "        try:\n",
    "            img = Image.open(pathway + filename)  # open the image file\n",
    "            img.verify()  # verify that it is, in fact an image\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(filename)\n",
    "            removed_files.append(filename)\n",
    "            os.remove(pathway + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = df_imgs.loc[df_imgs['filename'].isin(removed_files)].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " img_list = os.listdir('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df = df_imgs.loc[df_imgs['filename'].isin(img_list)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df.rename({'Image': 'data',\n",
    "               'converted_price': 'labels'},\n",
    "                axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_price = img_df['labels'].median()\n",
    "median_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df['labels'] = (img_df['labels']/median_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = img_df['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, Ytrain, Ytest = train_test_split(img_df, Y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train,\n",
    "directory= None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"labels\",\n",
    "subset=\"training\",\n",
    "batch_size=100,\n",
    "seed=55,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\")\n",
    "    \n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"labels\",\n",
    "subset=\"validation\",\n",
    "batch_size=100,\n",
    "seed=55,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\")\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=df_test,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"labels\",\n",
    "batch_size=100,\n",
    "seed=55,\n",
    "shuffle=False,\n",
    "class_mode=\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(16, (3, 3), padding='same', activation='relu',\n",
    "                        input_shape=(256 ,256,  3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "                        input_shape=(256 ,256,  3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='MSE',\n",
    "              optimizer='Adam',\n",
    "               metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model.fit(train_generator, epochs=3, validation_data=valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict(test_generator,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(summary.history['loss'])\n",
    "plt.plot(summary.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss(mean absolute error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define two sets of inputs\n",
    "# inputA = Input(shape=(32,))\n",
    "# inputB = Input(shape=(128,))\n",
    "# # the first branch operates on the first input\n",
    "# x = Dense(8, activation=\"relu\")(inputA)\n",
    "# x = Dense(4, activation=\"relu\")(x)\n",
    "# x = Model(inputs=inputA, outputs=x)\n",
    "# # the second branch opreates on the second input\n",
    "# y = Dense(64, activation=\"relu\")(inputB)\n",
    "# y = Dense(32, activation=\"relu\")(y)\n",
    "# y = Dense(4, activation=\"relu\")(y)\n",
    "# y = Model(inputs=inputB, outputs=y)\n",
    "# # combine the output of the two branches\n",
    "# combined = concatenate([x.output, y.output])\n",
    "# # apply a FC layer and then a regression prediction on the\n",
    "# # combined outputs\n",
    "# z = Dense(2, activation=\"relu\")(combined)\n",
    "# z = Dense(1, activation=\"linear\")(z)\n",
    "# # our model will accept the inputs of the two branches and\n",
    "# # then output a single value\n",
    "# model = Model(inputs=[x.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
