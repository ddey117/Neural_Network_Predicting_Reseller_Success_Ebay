{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Resale Value of Knives from a Texas Government Surplus Store\n",
    "\n",
    "## Using Machine Learning to Support an Ebay Store's Financial Success\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Model and Intepret Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Author:** Dylan Dey\n",
    "***\n",
    "\n",
    "# Overview\n",
    "[Texas State Surplus Store](https://www.tfc.texas.gov/divisions/supportserv/prog/statesurplus/)\n",
    "\n",
    "[What happens to all those items that get confiscated by the TSA? Some end up in a Texas store.](https://www.wfaa.com/article/news/local/what-happens-to-all-those-items-that-get-confiscated-by-the-tsa-some-end-up-in-a-texas-store/287-ba80dac3-d91a-4b28-952a-0aaf4f69ff95)\n",
    "\n",
    "[Texas Surplus Store PDF](https://www.tfc.texas.gov/divisions/supportserv/prog/statesurplus/State%20Surplus%20Brochure-one%20bar_rev%201-10-2022.pdf)\n",
    "\n",
    "![Texas State Surplus Store](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRYkwyu20VBuQ52PrXdVRaGRIIg9OPXJg86lA&usqp=CAU)\n",
    "\n",
    "\n",
    "\n",
    "Thousands of people make a living selling pre-owned items on sites like EBay. A good place to locate items for sale is the Texas Facilities Commission collects left behind possessions, salvage, and surplus from Texas state agencies such as DPS, TXDOT, TCEQ, and Texas Parks & Wildlife. Examples of commonly available items include vehicles, furniture, office equipment and supplies, small electronics, and heavy equipment. The goal of this project is to create a predictive model in order to determine the resale value of knives from the Texas State Surplus Store on eBay. Descriptive analysis of over 70K sold knives on eBay in the last 2 years will also be used to examine the profitability of investing in knives from the surplus store. \n",
    "\n",
    "\n",
    "# BUSINESS PROBLEM\n",
    "\n",
    "\n",
    "![Father's Ebay Account Since 1999](attachment:texas_dave.jpg)\n",
    "\n",
    "[Texas Dave's Knives](https://www.ebay.com/str/texasdave3/Knives/_i.html?store_cat=3393246519)\n",
    "\n",
    " My family has been running a resale shop and selling on Ebay and other sites for years and lately the business has picked up. We are interested in exploring if the most common item sold at the Texas Surplus Store, pocket knives, would be a safe investment. On the surface they seem great for reselling, as they are oftentimes collectible and small enough to be easily shipped. \n",
    "\n",
    "I have been experimenting with low cost used knives for resale but have not risked a large capital investment in the higher end items. Analyzing past listings on eBay for the top brands available at the Surplus Store could prove useful for gaining insight on whether a larger investment would pay off. Understanding the risks involved in investing capital into different brands of knives and their potential returns will help narrow down what brands to invest in and help reduce excess inventory.\n",
    "\n",
    "It has been very time consuming and inaccurate trying to find the correct value to list an item for on eBay. Currently when listing we try to identify the specific knife by Google search, and then try to find the same or similar items sold on Ebay or other sites. This “guess and check” method often results in inventory not moving due to overpricing or being sold at a price lower than its true potential profit. Building a model that predicts the value of a pocket knife on eBay could help to easily determine the correct value of the item before a listing is live on the website.\n",
    "\n",
    "\n",
    "\n",
    "# Data Understanding\n",
    "\n",
    "> There are <mark>eight buckets of presorted brand knives</mark> that I was interested in exploring from the Texas Surplus Store. The Eight Pocketknife brands and their associated cost at the Texas Surplus Store:\n",
    "\n",
    "<ul>\n",
    "  <li>Benchmade: \\$45.00</li>\n",
    "  <li>Buck: \\$20.00</li>\n",
    "  <li>Case/Casexx: \\$20.00</li>\n",
    "  <li>CRKT: \\$15.00</li>\n",
    "  <li>Kershaw: \\$15.00</li>\n",
    "  <li>SOG: \\$15.00</li>\n",
    "  <li>Spyderco: \\$30.00</li>\n",
    "  <li>Victorinox: \\$20.00</li>\n",
    "</ul>\n",
    "\n",
    "### Domain Understading: Cost Breakdown\n",
    "- padded envelopes: \\$0.50 per knife\n",
    "- flatrate shipping: \\$4.45 per knife\n",
    "- brand knife at surplus store: 15, 20, 30, or 45 dollars per knife\n",
    "- overhead expenses (gas, cleaning suplies, sharpening supplies, etc): \\$3.00\n",
    "- Ebay's comission, with 13\\% being a reasonable approximation\n",
    "\n",
    ">A majority of the data was scraped from eBays proprietary Terapeak webapp, as this data goes back 2 years as compared to the API listed data that only goes back 90 days. It is assumed a large enough amount of listed data should approximate sold data well enough to prove useful for this project. \n",
    "\n",
    "> The target feature for the model to predict is the total price (shipping included) that a knife should be listed on eBay. One model will be using titles and images in order to find potential listings that are undervalued and could be worth investing in. Another model will accept only images as input, as this is an input that can easily be obtained in person at the store. This model will use past sold data of knives on eBay in order to determine within an acceptable amount of error the price it will resale for on eBay (shipping included) using only an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd \n",
    "import  json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Flatten, GRU\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helps see plots in readme\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition\n",
    "\n",
    "Define functions to import and clean data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_iqr_filter(df):\n",
    "    \n",
    "    price_Q1 = df['converted_price'].quantile(0.25)\n",
    "    price_Q3 = df['converted_price'].quantile(0.75)\n",
    "    price_iqr = price_Q3 - price_Q1\n",
    "\n",
    "    profit_Q1 = df['profit'].quantile(0.25)\n",
    "    profit_Q3 = df['profit'].quantile(0.75)\n",
    "    profit_iqr = profit_Q3 - profit_Q1\n",
    "\n",
    "    ROI_Q1 = df['ROI'].quantile(0.25)\n",
    "    ROI_Q3 = df['ROI'].quantile(0.75)\n",
    "    ROI_iqr = ROI_Q3 - ROI_Q1\n",
    "\n",
    "    price_upper_limit = price_Q3 + (1.5 * price_iqr)\n",
    "    price_lower_limit = price_Q1 - (1.5 * price_iqr)\n",
    "\n",
    "    profit_upper_limit = profit_Q3 + (1.5 * profit_iqr)\n",
    "    profit_lower_limit = profit_Q1 - (1.5 * profit_iqr)\n",
    "\n",
    "    ROI_upper_limit = ROI_Q3 + (1.5 * ROI_iqr)\n",
    "    ROI_lower_limit = ROI_Q1 - (1.5 * ROI_iqr)\n",
    "    \n",
    "#     print(f'Brand: {df.brand[0]}')\n",
    "#     print(f'price upper limit: ${np.round(price_upper_limit,2)}')\n",
    "#     print(f'price lower limit: ${np.round(price_lower_limit,2)}')\n",
    "#     print('-----------------------------------')\n",
    "#     print(f'profit upper limit: ${np.round(profit_upper_limit,2)}')\n",
    "#     print(f'profit lower limit: ${np.round(profit_lower_limit,2)}')\n",
    "#     print('-----------------------------------')\n",
    "#     print(f'ROI upper limit: {np.round(ROI_upper_limit,2)}%')\n",
    "#     print(f'ROI lower limit: {np.round(ROI_lower_limit,2)}%')\n",
    "#     print('-----------------------------------')\n",
    "\n",
    "    \n",
    "    new_df = df[(df['converted_price'] < price_upper_limit) &\n",
    "                (df['converted_price'] > price_lower_limit) &\n",
    "                (df['profit'] < profit_upper_limit) &\n",
    "                (df['ROI'] > profit_lower_limit) &\n",
    "                (df['profit'] < ROI_upper_limit) &\n",
    "                (df['ROI'] > ROI_lower_limit)]\n",
    "    \n",
    "    return new_df\n",
    "#download jpg urls from dataFrame\n",
    "def download(row):\n",
    "    filename = os.path.join(root_folder, str(row.name) + im_extension)\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.Image\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    except:\n",
    "        print(f'{filename} error')\n",
    "\n",
    "\n",
    "\n",
    "# This function removes noisy data\n",
    "#lots/sets/groups of knives can\n",
    "#confuse the model from predicting\n",
    "#the appropriate value of individual knives\n",
    "def data_cleaner(df):\n",
    "    lot = re.compile('(?<!-\\S)lot(?![^\\s.,:?!])')\n",
    "    group = re.compile('(group)')\n",
    "    is_set = re.compile('(?<!-\\S)set(?![^\\s.,?!])')\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    trim_list = [lot,group,is_set]\n",
    "    for item in trim_list:\n",
    "        df.loc[df['title'].apply(lambda x: re.search(item, x)).notnull(), 'trim'] = 1 \n",
    "    to_drop = df.loc[df['trim'] == 1].index\n",
    "    df.drop(to_drop, inplace=True)\n",
    "    df.drop('trim', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#take raw data and prepare it for modeling\n",
    "def prepare_listed(listed_data_df):\n",
    "    listed_used_knives = listed_data_df.loc[listed_data_df['condition'] != 1000.0]\n",
    "    listed_used_knives = data_cleaner(listed_used_knives.copy())\n",
    "    listed_used_knives.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return listed_used_knives\n",
    "\n",
    "#take raw data and prepare it for modeling\n",
    "def prepare_tera_df(df, x, overhead_cost=3):\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\"$\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].str.replace(\",\", \"\")\n",
    "    df['price_in_US'] = df['price_in_US'].apply(float)\n",
    "    \n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\"$\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].str.replace(\",\", \"\")\n",
    "    df['shipping_cost'] = df['shipping_cost'].apply(float)\n",
    "    \n",
    "    df['brand'] = list(bucket_dict.keys())[x]\n",
    "    df['converted_price'] = (df['price_in_US'] + df['shipping_cost'])\n",
    "    df['cost'] = list(bucket_dict.values())[x] + overhead_cost + 4.95\n",
    "    df['profit'] = ((df['converted_price']*.87) -  df['cost'])\n",
    "    df['ROI'] = (df['profit']/ df['cost'])*100.0\n",
    "    \n",
    "    return df   \n",
    "\n",
    "\n",
    "def avg_word_len(x):\n",
    "    words = x.split()\n",
    "    word_len = 0\n",
    "    for word in words:\n",
    "        word_len += len(word)\n",
    "        \n",
    "    return word_len / len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dylandey/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Finding API data\n",
    "df_bench = pd.read_csv(\"listed_data/df_bench.csv\")\n",
    "df_buck = pd.read_csv(\"listed_data/df_buck.csv\")\n",
    "df_case = pd.read_csv(\"listed_data/df_case.csv\")\n",
    "df_caseXX = pd.read_csv(\"listed_data/df_CaseXX.csv\")\n",
    "df_crkt = pd.read_csv(\"listed_data/df_crkt.csv\")\n",
    "df_kersh = pd.read_csv(\"listed_data/df_kershaw.csv\")\n",
    "df_sog = pd.read_csv(\"listed_data/df_sog.csv\")\n",
    "df_spyd = pd.read_csv(\"listed_data/df_spyderco.csv\")\n",
    "df_vict = pd.read_csv(\"listed_data/df_victorinox.csv\")\n",
    "\n",
    "\n",
    "#Load scraped terapeak sold data\n",
    "sold_bench = pd.read_csv(\"terapeak_data/bench_scraped2.csv\")\n",
    "sold_buck1 = pd.read_csv(\"terapeak_data/buck_scraped2.csv\")\n",
    "sold_buck2 = pd.read_csv(\"terapeak_data/buck_scraped2_reversed.csv\")\n",
    "sold_case = pd.read_csv(\"terapeak_data/case_scraped2.csv\")\n",
    "sold_caseXX1 = pd.read_csv(\"terapeak_data/caseXX_scraped2.csv\")\n",
    "sold_caseXX2 = pd.read_csv(\"terapeak_data/caseXX2_reversed.csv\")\n",
    "sold_crkt = pd.read_csv(\"terapeak_data/crkt_scraped.csv\")\n",
    "sold_kershaw1 = pd.read_csv(\"terapeak_data/kershaw_scraped2.csv\")\n",
    "sold_kershaw2 = pd.read_csv(\"terapeak_data/kershaw_scraped2_reversed.csv\")\n",
    "sold_sog = pd.read_csv(\"terapeak_data/SOG_scraped2.csv\")\n",
    "sold_spyd = pd.read_csv(\"terapeak_data/spyd_scraped2.csv\")\n",
    "sold_vict1 = pd.read_csv(\"terapeak_data/vict_scraped.csv\")\n",
    "sold_vict2 = pd.read_csv(\"terapeak_data/vict_reversed.csv\")\n",
    "\n",
    "sold_list = [sold_bench,sold_buck1,\n",
    "             sold_buck2,sold_case,\n",
    "             sold_caseXX1,sold_caseXX2,\n",
    "             sold_crkt,sold_kershaw1,\n",
    "             sold_kershaw2,sold_sog, \n",
    "             sold_spyd, sold_vict1,\n",
    "             sold_vict2]\n",
    "\n",
    "\n",
    "listed_df = pd.concat([df_bench,df_buck,\n",
    "                       df_case,df_caseXX,\n",
    "                       df_crkt,df_kersh,\n",
    "                       df_sog,df_spyd,\n",
    "                       df_vict])\n",
    "\n",
    "used_listed = prepare_listed(listed_df)\n",
    "\n",
    "bucket_dict = {'benchmade': 45.0,\n",
    "               'buck': 20.0,\n",
    "               'case': 20.0,\n",
    "               'crkt': 15.0,\n",
    "               'kershaw': 15.0,\n",
    "               'sog': 15.0,\n",
    "               'spyderco': 30.0,\n",
    "               'victorinox': 20.0\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12159 entries, 0 to 12158\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   itemId                 12159 non-null  int64  \n",
      " 1   title                  12159 non-null  object \n",
      " 2   galleryURL             12158 non-null  object \n",
      " 3   viewItemURL            12159 non-null  object \n",
      " 4   autoPay                12159 non-null  bool   \n",
      " 5   postalCode             11833 non-null  object \n",
      " 6   sellingStatus          12159 non-null  object \n",
      " 7   shippingInfo           12159 non-null  object \n",
      " 8   listingInfo            12159 non-null  object \n",
      " 9   returnsAccepted        12159 non-null  bool   \n",
      " 10  condition              12158 non-null  float64\n",
      " 11  topRatedListing        12159 non-null  bool   \n",
      " 12  galleryPlusPictureURL  1011 non-null   object \n",
      " 13  pictureURLLarge        11541 non-null  object \n",
      " 14  pictureURLSuperSize    11491 non-null  object \n",
      " 15  shipping_cost          12159 non-null  float64\n",
      " 16  price_in_US            12159 non-null  float64\n",
      " 17  converted_price        12159 non-null  float64\n",
      " 18  brand                  12159 non-null  object \n",
      " 19  cost                   12159 non-null  float64\n",
      " 20  profit                 12159 non-null  float64\n",
      " 21  ROI                    12159 non-null  float64\n",
      "dtypes: bool(3), float64(7), int64(1), object(11)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "used_listed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in sold_list:\n",
    "    dataframe.rename({'Text': 'title',\n",
    "                      'shipping_': 'shipping_cost'},\n",
    "                     axis=1, inplace=True)\n",
    "\n",
    "    dataframe['date_sold'] = pd.to_datetime(dataframe['date_sold'])\n",
    "\n",
    "#limited out at 10K columns while scraping. Combine dataframes that went over 10K.\n",
    "sold_buck = pd.concat([sold_buck1,sold_buck2])\n",
    "sold_caseXX = pd.concat([sold_caseXX1,sold_caseXX2])\n",
    "sold_kershaw = pd.concat([sold_kershaw1,sold_kershaw2])\n",
    "sold_vict = pd.concat([sold_vict1,sold_vict2])\n",
    "\n",
    "#apply function to remove characters from price\n",
    "#and create profit/ROI features\n",
    "sold_bench = prepare_tera_df(sold_bench, 0)\n",
    "sold_buck = prepare_tera_df(sold_buck, 1)\n",
    "sold_case = prepare_tera_df(sold_case, 2)\n",
    "sold_caseXX = prepare_tera_df(sold_caseXX, 2)\n",
    "sold_crkt = prepare_tera_df(sold_crkt, 3)\n",
    "sold_kershaw = prepare_tera_df(sold_kershaw, 4)\n",
    "sold_sog = prepare_tera_df(sold_sog, 5)\n",
    "sold_spyd = prepare_tera_df(sold_spyd, 6)\n",
    "sold_vict = prepare_tera_df(sold_vict, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase and strip titles and remove duplicates\n",
    "for dataframe in sold_list:\n",
    "    dataframe['title'] = dataframe['title'].str.lower()\n",
    "    dataframe['title'] = dataframe['title'].str.strip()\n",
    "    dataframe.drop_duplicates(\n",
    "        subset = ['date_sold','price_in_US', \n",
    "                  'shipping_cost'],\n",
    "        keep = 'last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_df = pd.concat([sold_bench, sold_buck,\n",
    "                     sold_case, sold_caseXX, \n",
    "                     sold_crkt, sold_kershaw,\n",
    "                     sold_sog, sold_spyd,\n",
    "                     sold_vict]) \n",
    "#remove lots\n",
    "sold_knives = data_cleaner(sold_df).copy()\n",
    "\n",
    "#combine data\n",
    "df = pd.concat([sold_knives,used_listed]).copy()\n",
    "df['Image'].fillna(df['pictureURLLarge'], inplace=True)\n",
    "\n",
    "#apply IQR filtering\n",
    "df = apply_iqr_filter(df).copy()\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "#remove any special characters\n",
    "def remove_special_char(x):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', x)\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(x):\n",
    "    x.translate(str.maketrans('', '', string.punctuation))\n",
    "    return x\n",
    "\n",
    "def apply_text_prep(df):\n",
    "\n",
    "    df['title'] = df['title'].apply(remove_punctuations)\n",
    "    df['title'] = df['title'].apply(remove_special_char)\n",
    "    #A lot of the strings had duplicate phrases\n",
    "    #create a set on split strings in order to\n",
    "    #only get unique words in each title\n",
    "    df['title'] = df['title'].apply(lambda s: ' '.join(list(set(s.split()))))\n",
    "\n",
    "\n",
    "    df['title_len'] = df['title'].apply(lambda x: len(x))\n",
    "    df['word_count'] = df['title'].apply(lambda x: len(x.split()))\n",
    "    df['avg_word_len'] = df['title'].apply(lambda x: avg_word_len(x))\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    df['title_nostop'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apply_text_prep(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Neural network with \"title\" column as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'].plot(kind = 'hist', title = 'Word Count Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_word_len'].plot(kind='hist', bins = 50, title = 'Avg_Word_len Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_len'].plot(kind='hist', bins= 100,title = 'Title Length Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network with \"title\" column as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = df.loc[:, ['title_nostop', 'converted_price']]\n",
    "\n",
    "\n",
    "df_title.rename({'title_nostop': 'data',\n",
    "                 'converted_price': 'labels'},\n",
    "                axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_title['labels'] = (df_title['labels']/mean_price)\n",
    "Y = df_title['labels'].values\n",
    "df_train, df_test, Ytrain, Ytest = train_test_split(df_title['data'],\n",
    "                                                    Y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, Y_val, Y_test = train_test_split(df_test, \n",
    "                                                Ytest, \n",
    "                                                test_size=0.5, \n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize vocab \n",
    "voc_size = 30000\n",
    "max_len = 11\n",
    "embedding_features = 100\n",
    "tokenizer = Tokenizer(num_words=voc_size, oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(df_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(df_train) \n",
    "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add padding to ensure all inputs are the same size\n",
    "data_train = pad_sequences(sequences_train, maxlen=max_len, padding= 'post', truncating = 'post')\n",
    "data_val = pad_sequences(sequences_val, maxlen=max_len, padding= 'post', truncating = 'post')\n",
    "data_test = pad_sequences(sequences_test, maxlen=max_len, padding= 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(Embedding(voc_size, embedding_features, input_length = max_len)) \n",
    "model.add(GRU(300, dropout=0.5))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "model.compile(\n",
    "  loss='MSE',\n",
    "  optimizer='adam',\n",
    "  metrics=['mae']\n",
    ")\n",
    "\n",
    "\n",
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  data_train,\n",
    "  Ytrain,\n",
    "  epochs=5,\n",
    "  validation_data=(data_val, Y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Spyderco Mantra 3 Liner Lock Knife Black Carbon Fiber & G-10 S30V Steel C233CFP\"\n",
    "s1_p = 136.1\n",
    "s2 = \"Benchmade 556 Green 154cm Combo Blade Pardue Design\"\n",
    "s2_p = 71.95\n",
    "s3 = \"Case XX 6207 SS Mini Trapper Brown Peachseed Bone Pocket Knife Made in Usa\"\n",
    "s3_p = 51.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_string(s):\n",
    "    s = remove_special_char(s.lower())\n",
    "    s = remove_punctuations(s)\n",
    "    s = ' '.join(list(set(s.split())))\n",
    "    test = tokenizer.texts_to_sequences([s])\n",
    "    test2 = pad_sequences(test, maxlen=max_len, padding= 'post', truncating = 'post')\n",
    "    pred=model.predict(test2)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = test_single_string(s1)[0][0]\n",
    "pred2 = test_single_string(s2)[0][0]\n",
    "pred3 = test_single_string(s3)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sample1](images/RNN/randomSpyd.jpeg)\n",
    "![sample2](images/RNN/randomBench.jpeg)\n",
    "![sample3](images/RNN/randomCase.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'True value: ${s1_p}, Predicted Value: ${pred1:.2f}, difference: ${pred1 - s1_p:.2f}')\n",
    "print(f'True value: ${s2_p}, Predicted Value: ${pred2:.2f} difference: ${pred2 - s2_p:.2f}')\n",
    "print(f'True value: ${s3_p}, Predicted Value: ${pred3:.2f} difference: ${pred3 - s3_p:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds =model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.reshape(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.evaluate(data_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12,8))\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.title(\"Loss vs val Loss for RNN model on titles (MSE)\", fontsize=15)\n",
    "plt.xlabel(\"epochs\", fontsize=15)\n",
    "plt.ylabel(\"loss (mean squared error)\", fontsize=15)\n",
    "plt.legend();\n",
    "plt.savefig('images/RNN_GRU_MSE1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12,8))\n",
    "plt.plot(r.history['mae'], label='mae')\n",
    "plt.plot(r.history['val_mae'], label='val_mae')\n",
    "plt.title(\"Loss vs val Loss for RNN model on titles (MAE)\", fontsize=15)\n",
    "plt.xlabel(\"epochs\", fontsize=15)\n",
    "plt.ylabel(\"loss (mean absolute error)\", fontsize=15)\n",
    "plt.legend();\n",
    "plt.savefig('images/RNN_GRU_MAE1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,show_shapes=True, to_file='images/RNN_GRU1_arc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae = mean_absolute_error(Y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(test_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "string_score = f'\\nMAE on training set: ${test_mae:.2f}'\n",
    "string_score += f'\\nRMSE on training set: ${RMSE:.2f}'\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plt.scatter(Y_test, preds)\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\"red\")\n",
    "plt.text(3, 150, string_score)\n",
    "plt.title('RNN Model for Predicting Resale Value')\n",
    "plt.ylabel('Model predictions for Resale Value($US)')\n",
    "plt.xlabel('True Values for Resale Value($US)')\n",
    "plt.savefig('images/regression_GRU_relu1.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title['labels'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = df.loc[:, ['title', 'converted_price']]\n",
    "\n",
    "\n",
    "df_title.rename({'title': 'data',\n",
    "                 'converted_price': 'labels'},\n",
    "                axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_title['labels'] = (df_title['labels']/mean_price)\n",
    "Y = df_title['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, Ytrain, Ytest = train_test_split(df_title['data'],\n",
    "                                                    Y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, Y_val, Y_test = train_test_split(df_test, \n",
    "                                                Ytest, \n",
    "                                                test_size=0.5, \n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to sequences\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE) \n",
    "tokenizer.fit_on_texts(df_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(df_train) \n",
    "sequences_val = tokenizer.texts_to_sequences(X_val) \n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "V = len(word2idx)\n",
    "print('Found %s unique tokens.' % V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences so that we get a N x T matrix\n",
    "data_train = pad_sequences(sequences_train)\n",
    "print('Shape of data train tensor:', data_train.shape)\n",
    "\n",
    "# get sequence length\n",
    "T = data_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = pad_sequences(sequences_val, maxlen=T)\n",
    "print('Shape of data test tensor:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pad_sequences(sequences_test, maxlen=T)\n",
    "print('Shape of data test tensor:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RNN model\n",
    "# We get to choose embedding dimensionality\n",
    "D = 12\n",
    "# Hidden state dimensionality\n",
    "M = 100\n",
    "i = Input(shape=(T,))\n",
    "x = Embedding(V + 1, D)(i)\n",
    "x = LSTM(M, return_sequences=True)(x) \n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(62, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x) \n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(1)(x)\n",
    "model = Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "model.compile(\n",
    "  loss='MSE',\n",
    "  optimizer='adam',\n",
    "  metrics=['mae']\n",
    ")\n",
    "\n",
    "\n",
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  data_train,\n",
    "  Ytrain,\n",
    "  epochs=5,\n",
    "  validation_data=(data_val, Y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,show_shapes=True, to_file='images/RNN_LSTM_arc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pred.reshape(11750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.evaluate(data_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(test_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12,8))\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.title(\"Loss vs val Loss for RNN model on titles (MSE)\", fontsize=15)\n",
    "plt.xlabel(\"epochs\", fontsize=15)\n",
    "plt.ylabel(\"loss (mean squared error)\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('images/MSE_LSTM_relu.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12,8))\n",
    "plt.plot(r.history['mae'], label='mae')\n",
    "plt.plot(r.history['val_mae'], label='val_mae')\n",
    "plt.title(\"Loss vs val Loss for RNN model on titles (MAE)\", fontsize=15)\n",
    "plt.xlabel(\"epochs\", fontsize=15)\n",
    "plt.ylabel(\"loss (mean absolute error)\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('images/MAE_LSTM_relu.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae = mean_absolute_error(Y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "string_score = f'\\nMAE on training set: ${test_mae:.2f}'\n",
    "string_score += f'\\nMAE on training set: ${RMSE:.2f}'\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plt.scatter(Y_test, preds)\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\"red\")\n",
    "plt.text(5, 135, string_score)\n",
    "plt.title('Regression Model for Predicting Resale Value')\n",
    "plt.ylabel('Model predictions for Resale Value($US)')\n",
    "plt.xlabel('True Values for Resale Value($US)')\n",
    "plt.savefig(\"images/regression_LSTM_relu.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CNN model\n",
    "\n",
    "# We get to choose embedding dimensionality\n",
    "D = 256\n",
    "\n",
    "\n",
    "\n",
    "i = Input(shape=(T,))\n",
    "x = Embedding(V + 1, D)(i)\n",
    "x = Conv1D(32, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(64, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(1, activation='relu')(x)\n",
    "\n",
    "model = Model(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "model.compile(\n",
    "  loss='MSE',\n",
    "  optimizer='adam',\n",
    "  metrics=['mae']\n",
    ")\n",
    "\n",
    "\n",
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  data_train,\n",
    "  Ytrain,\n",
    "  epochs=5,\n",
    "  validation_data=(data_val, Y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss per iteration\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy per iteration\n",
    "plt.plot(r.history['loss'], label='MSE')\n",
    "plt.plot(r.history['val_loss'], label='val_MSE')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN using images as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs = df.drop(['title', 'url', \n",
    "                   'date_sold', 'profit',\n",
    "                   'ROI', 'brand', 'cost',\n",
    "                   'pictureURLLarge'],\n",
    "                     axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs.dropna(subset=['Image'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs['file_index'] = df_imgs.index.values\n",
    "df_imgs['file_index'] = df_imgs['file_index'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs['filename'] = df_imgs['file_index'] + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(row):\n",
    "    filename = row.filepath\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "#     os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    url = row.Image\n",
    "#     print(f\"Downloading {url} to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    except:\n",
    "        print(f'{filename} error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images/'\n",
    "df_imgs['filepath'] = root_folder + df_imgs['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs['filepath'].sample(2).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_imgs.apply(download, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All image files are stored locally for this project. The below markdown code is for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "img_list = os.listdir('C:/Users/12108/Documents/GitHub/Neural_Network_Predicting_Reseller_Success_Ebay/nn_images/')\n",
    "\n",
    "img_df = df_imgs.loc[df_imgs['filename'].isin(img_list)].copy()\n",
    "\n",
    "img_df.reset_index(drop=True, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "img_df.rename({'Image': 'data',\n",
    "               'converted_price': 'labels'},\n",
    "                axis=1, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df_train, df_test, Ytrain, Ytest = train_test_split(img_df, Y, test_size=0.20)\n",
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.20)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train,\n",
    "directory= None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"labels\",\n",
    "subset=\"training\",\n",
    "batch_size=100,\n",
    "seed=55,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\")\n",
    "    \n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"labels\",\n",
    "subset=\"validation\",\n",
    "batch_size=100,\n",
    "seed=55,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\")\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=df_test,\n",
    "directory=None,\n",
    "x_col=\"filepath\",\n",
    "y_col=\"labels\",\n",
    "batch_size=100,\n",
    "seed=55,\n",
    "shuffle=False,\n",
    "class_mode=\"raw\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(16, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "#                         input_shape=(256 ,256,  3)))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='MSE',\n",
    "#               optimizer='Adam',\n",
    "#                metrics=['mae', 'mse'])\n",
    "\n",
    "# summary = model.fit(train_generator, epochs=3, validation_data=valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('cnn_grayscale_relu1.h5',  compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, to_file=\"images/CNN_architecture.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict(test_generator,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(summary.history['loss'])\n",
    "plt.plot(summary.history['val_loss'])\n",
    "plt.plot\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss(mean absolute error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "### Recurrent Neural Network (Long Short Term Memory)\n",
    "\n",
    "![RNN LSTM Arc](images/RNN_LSTM_arc.png)\n",
    "![RNN CNN MAE](images/MAE_LSTM_relu.png)\n",
    "![regression_plot](images/regression_LSTM_relu.png)\n",
    "\n",
    "- The mean price of the 8 brands of knives sold on ebay is around \\\\$50.00. \n",
    "- A mean absolute error of about plus or minus \\\\$13.80 is acceptable. \n",
    "\n",
    "### Convoluted Neural Network on Grayscale Images\n",
    "\n",
    "![CNN_Architecture](images/CNN_architecture.png)\n",
    "![CNN Regression Plot](images/Regression_CNN_relu1.png)\n",
    "![CNN_MSE](images/CNN_MAE_relu1.png)\n",
    "\n",
    "- The MAE when testing the CNN was roughly \\\\$25.00. That is an error of plus or minus about 50\\% of the mean price of knives sold. Not acceptable yet as compared to the RNN with titles. Will address in future work.\n",
    "\n",
    "## Future Work\n",
    "- Expand data to include other products readily purchasable at the Surplus Store. \n",
    "\n",
    "- Attempt data augmentation on the CNN image network\n",
    "\n",
    "- Attempt to obtain more aspect data for sold knives. Some important aspect data is limited access to sellers who average a certain amount of money per month. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "### Random Forest with TFIDF vectorization and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_title['labels'] = (df_title['labels']/mean_price)\n",
    "Y = df_title['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title['data'].sample(10).apply(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, Ytrain, Ytest = train_test_split(df_title['data'],\n",
    "                                                    Y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=51)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val, X_test, Y_val, Y_test = train_test_split(df_test, \n",
    "#                                                 Ytest, \n",
    "#                                                 test_size=0.5, \n",
    "#                                                 random_state=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(df_train)\n",
    "X_train_vec = tfidf_vectorizer.transform(df_train)\n",
    "x_test_vec = tfidf_vectorizer.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(verbose=3, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train_vec,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_true = Ytest\n",
    "y_pred = rf_model.predict(x_test_vec)\n",
    "\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_true, y_pred))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_true, y_pred))\n",
    "print('Root Mean Squared Error (RMSE):', metrics.mean_squared_error(y_true, y_pred, squared=False))\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_true, y_pred))\n",
    "print('Max Error:', metrics.max_error(y_true, y_pred))\n",
    "print('Mean Squared Log Error:', metrics.mean_squared_log_error(y_true, y_pred))\n",
    "print('Median Absolute Error:', metrics.median_absolute_error(y_true, y_pred))\n",
    "print('R^2:', metrics.r2_score(y_true, y_pred))\n",
    "print('Mean Poisson Deviance:', metrics.mean_poisson_deviance(y_true, y_pred))\n",
    "print('Mean Gamma Deviance:', metrics.mean_gamma_deviance(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer.get_feature_names()\n",
    "fi = rf_model.feature_importances_\n",
    "importance = [(features[i], fi[i]) for i in range(0,2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_title['labels'] = (df_title['labels']/mean_price)\n",
    "Y = df_title['labels'].values\n",
    "\n",
    "df_title['data'].sample(10).apply(print)\n",
    "\n",
    "df_train, df_test, Ytrain, Ytest = train_test_split(df_title['data'],\n",
    "                                                    Y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=51)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_val, X_test, Y_val, Y_test = train_test_split(df_test, \n",
    "#                                                 Ytest, \n",
    "#                                                 test_size=0.5, \n",
    "#                                                 random_state=51)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(df_train)\n",
    "X_train_vec = tfidf_vectorizer.transform(df_train)\n",
    "x_test_vec = tfidf_vectorizer.transform(df_test)\n",
    "\n",
    "X_train_vec.get_shape()\n",
    "\n",
    "tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(verbose=3, n_jobs=-1, random_state=42)\n",
    "\n",
    "rf_model.fit(X_train_vec,Ytrain)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "y_true = Ytest\n",
    "y_pred = rf_model.predict(x_test_vec)\n",
    "\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_true, y_pred))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_true, y_pred))\n",
    "print('Root Mean Squared Error (RMSE):', metrics.mean_squared_error(y_true, y_pred, squared=False))\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_true, y_pred))\n",
    "print('Max Error:', metrics.max_error(y_true, y_pred))\n",
    "print('Mean Squared Log Error:', metrics.mean_squared_log_error(y_true, y_pred))\n",
    "print('Median Absolute Error:', metrics.median_absolute_error(y_true, y_pred))\n",
    "print('R^2:', metrics.r2_score(y_true, y_pred))\n",
    "print('Mean Poisson Deviance:', metrics.mean_poisson_deviance(y_true, y_pred))\n",
    "print('Mean Gamma Deviance:', metrics.mean_gamma_deviance(y_true, y_pred))\n",
    "\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "fi = rf_model.feature_importances_\n",
    "importance = [(features[i], fi[i]) for i in range(0,2000)]\n",
    "\n",
    "importance[:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
